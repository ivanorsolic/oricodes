[
{
	"uri": "http://localhost:1313/rc-car/",
	"title": "RC Cars: a primer",
	"tags": [],
	"description": "",
	"content": "Prerequisite knowledge RC Cars: a primer Through this chapter we\u0026rsquo;ll explore what kind of parts is a RC car comprised of, and what are the most important ones for us. We’ll use that knowledge while buying parts and assembling them for our hardware platform.\n"
},
{
	"uri": "http://localhost:1313/artificial-intelligence/camera-calibration/camera-distortions/",
	"title": "Camera calibration: Explaining camera distortions",
	"tags": [],
	"description": "",
	"content": "Since we\u0026rsquo;re using only cameras to obtain the entirety of data we\u0026rsquo;ll use to drive our car around the real world, we\u0026rsquo;re sure trusting them a lot. We\u0026rsquo;re trusting that they\u0026rsquo;ll provide us with accurate representations of real world 3D objects as 2D images we\u0026rsquo;ll feed into our neural network.\nBut we shouldn\u0026rsquo;t take that for granted. Cameras, albeit cheap and easy to use, come with all sorts of issues when it comes to mapping the 3D world onto a 2D sensor/image correctly. But luckily, we can do something about it.\nThe pinhole camera model The pinhole camera model is a model of an ideal camera, that describes the mathematical relationship between the real world 3D object\u0026rsquo;s coordinates and its 2D projection on the image plane. 1\nPinhole cameras were the very beginning of photography 2, and are used even today to explain basic photography to students.\nThey posses some advantages over our regular lens cameras:\nThey have near infinite depth of field; everything appears in focus. No lenses are used to focus light, so they have no lens distortion and wide-images remain absolutely rectilinear. Basically, the smaller the pinhole gets, the more the resolution increases, until we reach the diffraction limit, at which point the image just gets darker and blurrier. Also, the smaller the pinhole, less light comes in, so the exposure time needs to be increased. Which brings us to the big issue with them:\nTheir exposure times are really long, which causes significant motion blur around any moving objects or it causes their complete absence if they\u0026rsquo;ve moved too fast. How can we get a small pinhole that gets a lot of light? We can use a convex lens, for starters.\nWhy does this help: instead of a single ray of light illuminating a single image point, now pencils of light illuminate each image point. Even our eyes use lenses. :)\nBut of course, lenses bring the issues we\u0026rsquo;ve mentioned earlier:\nThey have finite aperture so blurring of unfocused objects appears. They contain geometric distortions due to lenses, which increase as we get closer to the edges of the lenses. Types of distortions The first, and most common type of camera lens distortion is called radial distortion.\nThere are two types of this distortion, the positive or barrel radial distortion, and the negative or pincushion radial distortion.\nIn the context of self driving RCs, you\u0026rsquo;ll most probably be dealing with the barrel distortion, that will most probably be caused by fisheye lenses, since we\u0026rsquo;d like to get as big a field of view as we can. Some action cams even have a FOV of 170 to 180 degrees, which causes a lot of positive radial distortion.\nThe other type of distortion you can come across is called tangential distortion, which occurs when the lens is not aligned perfectly parallel to the imaging plane (the sensor). It causes the image to look tilted, which is obviously bad for us since some objects look further away than they really are.\nThat being said, we have to expect that no camera is actually really perfect. Most, if not all have some amount of radial and tangential distortion, since the lenses are imperfect in real life, and the lens isn\u0026rsquo;t always perfectly in line with the imaging plane.\nGetting rid of distortions with OpenCV Luckily for us, the radial and tangential distortions can be described using a couple of coefficients:\n$k_n$ coefficients will describe radial distortion $p_n$ coefficients will describe tangential distortion The worse the distortion, the more coefficients we need to accurately describe it.\nOpenCV works with up to six ($k_1$, $k_2$, $k_3$, $k_4$, $k_5$ and $k_6$) radial distortion coefficients, which should be more than enough for us, and with two ($p_1$, $p_2$) tangential distortion coefficients.\nIf we have the barrel radial distortion type, $k_1$ will typically be larger than zero. If we have the pincushion distortion, $k_1$ will typically be smaller than zero.\nOpenCV uses a pinhole camera model to describe how an image is created by projecting 3D points into the image plane using a perspective transformation:\n++\rs \\begin{bmatrix}{u}\\\\{v}\\\\{1}\\end{bmatrix} = \\begin{bmatrix}{f_x}\u0026{0}\u0026{c_x}\\\\{0}\u0026{f_y}\u0026{c_y}\\\\{0}\u0026{0}\u0026{1}\\end{bmatrix} \\begin{bmatrix} r_{11} \u0026 r_{12} \u0026 r_{13} \u0026 t_1 \\\\ r_{21} \u0026 r_{22} \u0026 r_{23} \u0026 t_2 \\\\ r_{31} \u0026 r_{32} \u0026 r_{33} \u0026 t_3 \\end{bmatrix} \\begin{bmatrix} X \\\\ Y \\\\ Z \\\\ 1 \\end{bmatrix}\r++\r$(X, Y, Z)$ are the coordinates of a 3D point we\u0026rsquo;re imaging $(u,v)$ are the 2D coordinates of the projection point in pixels The first matrix after the equation is the camera matrix, containing intrinsic camera parameters $(c_x, c_y)$ defines the principle point which is usually the center of the image $f_x$ and $f_y$ are the focal lengths expressed in pixel units The matrix containing the $r_{mn}$ parameters is the joint rotation-translation matrix, a matrix of extrinsic parameters which describes camera motion around a static scene. It\u0026rsquo;s used to translate the 3D coordinates to a 2D coordinate system, fixed with respect to the camera. Since we\u0026rsquo;re imaging 2D images, we\u0026rsquo;d like to map the 3D coordinates to a coordinate system:\n++\r\\begin{bmatrix}{x}\\\\{y}\\\\{z}\\end{bmatrix} = R \\begin{bmatrix}{X}\\\\{Y}\\\\{Z}\\end{bmatrix} + t \\\\\rx' = x/z \\\\\ry' = y/z \\\\\ru = f_x*x' + c_x \\\\\rv = f_y*y' + c_y\r++\rAlso, since we\u0026rsquo;re not using a pinhole camera, we need to add the distortion coefficients to our model:\n++\rx'' = x' \\frac{1 + k_1 r^2 + k_2 r^4 + k_3 r^6}{1 + k_4 r^2 + k_5 r^4 + k_6 r^6} + 2 p_1 x' y' + p_2 \\\\ y'' = y' \\frac{1 + k_1 r^2 + k_2 r^4 + k_3 r^6}{1 + k_4 r^2 + k_5 r^4 + k_6 r^6} + p_1 (r^2 + 2 y'^2) + 2 p_2 x' y' \\\\ \\text{where} \\quad r^2 = x'^2 + y'^2 \\\\ u = f_x*x'' + c_x \\\\\rv = f_y*y'' + c_y\r++\rSince we\u0026rsquo;re primarily interested in efficiently removing the radial distortion, we\u0026rsquo;ll be using Fitzgibbon\u0026rsquo;s division model as opposed to Brown-Conrady\u0026rsquo;s even-order polynomial model, since it requires fewer terms in cases of severe distortion. It is also a bit easier to work with, since inverting the single parameter division model requires solving a one degree less polynomial than inverting the single-parameter polynomial model. 3\nFinding the camera\u0026rsquo;s intrinsic and extrinsic parameters Now that we\u0026rsquo;ve laid out all of the formulas we use to correct radial and tangential distortion, the only question that remains is how do we get the intrinsic and extrinsic parameters.\nFor those purposes, we\u0026rsquo;ll be using the OpenCV calibrateCamera function, along with its findChessboardCorners function.\nThe calibrateCamera function is based on Zhang\u0026rsquo;s A Flexible New Technique for Camera Calibration and Caltech\u0026rsquo;s Camera Calibration Toolbox.\nIt needs the coordinates of a 3D object we\u0026rsquo;re imaging and its corresponding 2D projected coordinates in order to detect the intrinsic and extrinsic parameters of the camera we\u0026rsquo;re using to image the object.\nTo easily get those coordinates, we\u0026rsquo;ll be using a chessboard. A chessboard is an object with a known geometry to us and it has easily detectable feature points. Such objects are called calibration rigs or patterns, and OpenCV has a built-in function that uses a chessboard as a calibration rig, the findChessboardCorners function.\nThe findChessboardCorners function attempts to determine whether the input image is a view of the chessboard pattern and automatically locate the internal chessboard corners.\nThe cool thing with this is that we can print out a 3D object (a chessboard) whose geometry is well known to us, and map its 3D coordinates to our 2D image projection. The 3D points of the chessboard from the real world are called object points and their 2D mappings on our image are called image points.\nSo, we print out a chessboard, take multiple pictures of it from different angles to better capture the camera distortions, and feed them to the findChessboardCorners function. It will give us the detected object points and corresponding image points back to us, which we can use to then calibrate the camera.\nThe calibrateCamera function, given the object points and image points by the findChessboardCorners function, performs the following:\nComputes the initial intrinsic parameters. The distortion coefficients are all set to zeros initially. Estimates the initial camera pose as if the intrinsic parameters have been already known. Runs the global Levenberg-Marquardt optimization algorithm to minimize the reprojection error, that is, the total sum of squared distances between the observed feature points imagePoints and the projected (using the current estimates for camera parameters and the poses) object points objectPoints. The function returns a matrix with the intrinsic camera parameters and a matrix with the distortion coefficients, which we can use to undistort our images.\nOther stuff if you like computer vision Optics is a pretty interesting field of physics, and you if you\u0026rsquo;re planning to do any computer vision work or research, there\u0026rsquo;s a bunch of stuff to learn and read to understand how cameras work, which would help you dive deeper into the field.\nIf you\u0026rsquo;re not interested in it, feel free to skip the small paragraph below.\nI\u0026rsquo;d recommend at least reading about the two most important parameters of optical lenses: the focal length and the maximum aperture of the camera.\nIt\u0026rsquo;s useful to know how different focal lengths affect the represented size of distant objects, for example:\nYou can also learn a lot by reading how the focal length determines the angle of view, how the focal ratio (or f-number) defines the maximum usable aperture of a lens and so on. It\u0026rsquo;s really interesting.\nWikipedia: Pinhole camera model\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nCamera Obscura\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAutomatic Radial Distortion Estimation from a Single Image\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n"
},
{
	"uri": "http://localhost:1313/artificial-intelligence/custom-architecture/",
	"title": "Creating our first custom network architecture",
	"tags": [],
	"description": "",
	"content": "Where do we even begin with the AI part of the project?\nWell, I think it\u0026rsquo;d be a good idea to get the details of plugging a custom network into Donkey out of the way first.\nIf you remember the first autopilot we trained for sanity checking purposes, you\u0026rsquo;ll recall we\u0026rsquo;ve used an architecture that came with Donkey, whose source can be found at donkeycar/parts/keras.py.\nAnd that\u0026rsquo;s pretty cool, we\u0026rsquo;ve already got a fair number of architectures to play around with out of the box. But we\u0026rsquo;ll want to do things our way, make some custom architectures and play around with all sorts of stuff. It\u0026rsquo;s also beneficial to be able to visualize how all of the pieces of your project come together before you dive all the way down into the details of it.\nFor now, I won\u0026rsquo;t get into the details of machine and deep learning. That\u0026rsquo;s a whole other bullet to bite, and a science of its own. One that I\u0026rsquo;m also in the process of getting to know. If you want to understand all of the details of the following content, you\u0026rsquo;ll need to have some working knowledge of ML/DL. It\u0026rsquo;s also okay if you don\u0026rsquo;t. You\u0026rsquo;ll be able to follow along and get your feet wet, you just probably won\u0026rsquo;t get some implied details and minutiae.\nEnough philosophising, let\u0026rsquo;s get to work!\nHow do the architectures that come with Donkey work? First things first, let\u0026rsquo;s see how the architectures we get out of the box with Donkey work.\nYou don\u0026rsquo;t have to, and usually probably won\u0026rsquo;t, do this the way I\u0026rsquo;m doing it, which is going through the source code files and seeing what happens. You can consult the documentation or ask for help on the official project channels, but I love this approach since I believe you\u0026rsquo;ll get a better sense of how something works if you look at the insides of it. I\u0026rsquo;m also used to hacking and taking things apart just to see how they work, (see ivanorsolic.github.io for examples) and sometimes you don\u0026rsquo;t have the luxury of good docs and people willing to help you, so it\u0026rsquo;s useful to be able to dive into the internals of something and make sense out of it. I also just think this way is much more fun 🤓, but YMMV.\nSKIP TO THE TL;DR When we wanted to train our first autopilot with Donkey, we used the train flag to tell the manage.py script to train a model.\nSo if we open up the manage.py script and look at the main method, we can see how the script handles the train flag:\nif args[\u0026#39;train\u0026#39;]: from train import multi_train, preprocessFileList tub = args[\u0026#39;--tub\u0026#39;] model = args[\u0026#39;--model\u0026#39;] transfer = args[\u0026#39;--transfer\u0026#39;] model_type = args[\u0026#39;--type\u0026#39;] continuous = args[\u0026#39;--continuous\u0026#39;] aug = args[\u0026#39;--aug\u0026#39;] dirs = preprocessFileList( args[\u0026#39;--file\u0026#39;] ) if tub is not None: tub_paths = [os.path.expanduser(n) for n in tub.split(\u0026#39;,\u0026#39;)] dirs.extend( tub_paths ) multi_train(cfg, dirs, model, transfer, model_type, continuous, aug) Most notably, it uses the multi_train function that it imports from the train.py script. Other than that, it parses the rest of the arguments it needs to call the multi_train function and does some preprocessing with the list of tubs we want to use.\nSo, we\u0026rsquo;ll open up the train.py script, and find the definition of the multi_train function:\ndef multi_train(cfg, tub, model, transfer, model_type, continuous, aug): # choose the right regime for the given model type train_fn = train if model_type in (\u0026#34;rnn\u0026#34;,\u0026#39;3d\u0026#39;,\u0026#39;look_ahead\u0026#39;): train_fn = sequence_train train_fn(cfg, tub, model, transfer, model_type, continuous, aug) We can see that it chooses the train function as the default function for training the models, but can also use the sequence_train function if the architecture is a sequential network. Cool!\nSince we won\u0026rsquo;t be implementing a custom sequential network, we\u0026rsquo;ll take a look at the default train function. There\u0026rsquo;s a lot going on inside the function, from managing the data and creating generators to split it into batches for training to handling different model filetypes, but we don\u0026rsquo;t have to, and won\u0026rsquo;t go through all the details. That\u0026rsquo;s one of the reasons we\u0026rsquo;re using Donkey, so we don\u0026rsquo;t have to everything by ourselves.\nOur goal is to use a custom architecture with Donkey, and we\u0026rsquo;re trying to find out how Donkey uses the pre-defined architectures, so this line is very much of interest to us:\nkl = get_model_by_type(train_type, cfg=cfg) The function above is imported from donkeycar/utils.py, so we\u0026rsquo;ll open that script up and find the definition of the get_model_by_type function:\ndef get_model_by_type(model_type, cfg): \u0026#39;\u0026#39;\u0026#39; given the string model_type and the configuration settings in cfg create a Keras model and return it. \u0026#39;\u0026#39;\u0026#39; from donkeycar.parts.keras import KerasRNN_LSTM, KerasBehavioral, \\ KerasCategorical, KerasIMU, KerasLinear, Keras3D_CNN, \\ KerasLocalizer, KerasLatent from donkeycar.parts.tflite import TFLitePilot if model_type is None: model_type = cfg.DEFAULT_MODEL_TYPE print(\u0026#34;\\\u0026#34;get_model_by_type\\\u0026#34; model Type is: {}\u0026#34;.format(model_type)) input_shape = (cfg.IMAGE_H, cfg.IMAGE_W, cfg.IMAGE_DEPTH) roi_crop = (cfg.ROI_CROP_TOP, cfg.ROI_CROP_BOTTOM) if model_type == \u0026#34;tflite_linear\u0026#34;: kl = TFLitePilot() elif model_type == \u0026#34;localizer\u0026#34; or cfg.TRAIN_LOCALIZER: kl = KerasLocalizer(num_locations=cfg.NUM_LOCATIONS, input_shape=input_shape) # And so on ... else: raise Exception(\u0026#34;unknown model type: %s\u0026#34; % model_type) return kl This is exactly what we were looking for. We can see that the function:\nTakes in the name of the wanted architecture as a string (passed to the manage.py script using the --type flag) Imports all of the architectures from donkeycar/parts/keras.py Creates a model using the appropriate architecture (based on the name) Returns the created model It also defines the image shape and the region of interest crop that some models use, and sets the model type to the default type (defined in myconfig.py) if the type isn\u0026rsquo;t explicitly passed through the type flag.\nOkay, so now we know how the manage.py script gets the Keras model it then trains.\nLet\u0026rsquo;s take a look at keras.py to see how to define a custom architecture/model.\nKerasPilot base class We can see that there is a base class already prepared for us, that implements functions that all models use, such as model and weight loading, training and setting the optimizer of the model:\nclass KerasPilot(object): \u0026#39;\u0026#39;\u0026#39; Base class for Keras models that will provide steering and throttle to guide a car. \u0026#39;\u0026#39;\u0026#39; def __init__(self): self.model = None self.optimizer = \u0026#34;adam\u0026#34; def load(self, model_path): self.model = keras.models.load_model(model_path) def load_weights(self, model_path, by_name=True): self.model.load_weights(model_path, by_name=by_name) def shutdown(self): pass def compile(self): pass def set_optimizer(self, optimizer_type, rate, decay): if optimizer_type == \u0026#34;adam\u0026#34;: self.model.optimizer = keras.optimizers.Adam(lr=rate, decay=decay) elif optimizer_type == \u0026#34;sgd\u0026#34;: self.model.optimizer = keras.optimizers.SGD(lr=rate, decay=decay) elif optimizer_type == \u0026#34;rmsprop\u0026#34;: self.model.optimizer = keras.optimizers.RMSprop(lr=rate, decay=decay) else: raise Exception(\u0026#34;unknown optimizer type: %s\u0026#34; % optimizer_type) def train(self, train_gen, val_gen, saved_model_path, epochs=100, steps=100, train_split=0.8, verbose=1, min_delta=.0005, patience=10, use_early_stop=True): # And so on ... The only three functions we need to implement ourselves in our custom class are: compile and run (and the constructor)\nLet\u0026rsquo;s look at the default architecture, the KerasLinear class:\nclass KerasLinear(KerasPilot): \u0026#39;\u0026#39;\u0026#39; The KerasLinear pilot uses one neuron to output a continous value via the Keras Dense layer with linear activation. One each for steering and throttle. The output is not bounded. \u0026#39;\u0026#39;\u0026#39; def __init__(self, num_outputs=2, input_shape=(120, 160, 3), roi_crop=(0, 0), *args, **kwargs): super(KerasLinear, self).__init__(*args, **kwargs) self.model = default_n_linear(num_outputs, input_shape, roi_crop) self.compile() def compile(self): self.model.compile(optimizer=self.optimizer, loss=\u0026#39;mse\u0026#39;) def run(self, img_arr): img_arr = img_arr.reshape((1,) + img_arr.shape) outputs = self.model.predict(img_arr) steering = outputs[0] throttle = outputs[1] return steering[0][0], throttle[0][0] We can see that this particular class uses the roi_crop variable along with the input_shape, that gets passed to it through the utils.py script.\nThe class inherits the base KerasPilot class, and sets the self.model using the default_n_linear function, which actually implements the architecture in Keras.\nIt also compiles itself using the default parent class optimizer (adam), and uses the mean squared error as the error function.\nLet\u0026rsquo;s look at the actual Keras implementation in the default_n_linear function:\ndef default_n_linear(num_outputs, input_shape=(120, 160, 3), roi_crop=(0, 0)): drop = 0.1 input_shape = adjust_input_shape(input_shape, roi_crop) img_in = Input(shape=input_shape, name=\u0026#39;img_in\u0026#39;) x = img_in x = Convolution2D(24, (5,5), strides=(2,2), activation=\u0026#39;relu\u0026#39;, name=\u0026#34;conv2d_1\u0026#34;)(x) x = Dropout(drop)(x) x = Convolution2D(32, (5,5), strides=(2,2), activation=\u0026#39;relu\u0026#39;, name=\u0026#34;conv2d_2\u0026#34;)(x) x = Dropout(drop)(x) x = Convolution2D(64, (5,5), strides=(2,2), activation=\u0026#39;relu\u0026#39;, name=\u0026#34;conv2d_3\u0026#34;)(x) x = Dropout(drop)(x) x = Convolution2D(64, (3,3), strides=(1,1), activation=\u0026#39;relu\u0026#39;, name=\u0026#34;conv2d_4\u0026#34;)(x) x = Dropout(drop)(x) x = Convolution2D(64, (3,3), strides=(1,1), activation=\u0026#39;relu\u0026#39;, name=\u0026#34;conv2d_5\u0026#34;)(x) x = Dropout(drop)(x) x = Flatten(name=\u0026#39;flattened\u0026#39;)(x) x = Dense(100, activation=\u0026#39;relu\u0026#39;)(x) x = Dropout(drop)(x) x = Dense(50, activation=\u0026#39;relu\u0026#39;)(x) x = Dropout(drop)(x) outputs = [] for i in range(num_outputs): outputs.append(Dense(1, activation=\u0026#39;linear\u0026#39;, name=\u0026#39;n_outputs\u0026#39; + str(i))(x)) model = Model(inputs=[img_in], outputs=outputs) return model And that\u0026rsquo;s the last piece of the puzzle. It takes the number of outputs, the input shape and the region of interest via parameters, and implements a Keras model.\nSo, in summary, what are the steps for making your own architecture?\nTL;DR: How to implement your own architecture Create a new Python script (or open up the keras.py script and work in there)\nImport the KerasPilot base model class:\nfrom donkeycar.parts.keras import KerasPilot Define a custom function that implements your architecture in Keras\nDefine a custom class that inherits the KerasPilot class, implements compile and run and initializes the model using the above mentioned function\nAdd your architecture to the get_model_by_type function in utils.py\nUse your architecture by passing the model_type you defined in utils.py as the type flag to the manage.py script.\nImplementing a custom architecture First, we have to come up with an architecture to implement.\nWe\u0026rsquo;ll use Nvidia\u0026rsquo;s CNN architecture from the End to End Learning for Self-Driving Cars paper by M. Bojarski et al.\nWhy this particular paper? Well, I think it\u0026rsquo;s fitting since it has a nice bit of self-driving RC history behind it.\nAs Nvidia states on their blog, explaining the project behind the paper:\nThe groundwork for this project was actually done over 10 years ago in a Defense Advanced Research Projects Agency (DARPA) seedling project known as DARPA Autonomous Vehicle (DAVE), in which a sub-scale radio control (RC) car drove through a junk-filled alley way. DAVE was trained on hours of human driving in similar, but not identical, environments. The training data included video from two cameras and the steering commands sent by a human operator.\nHey, would you look at that! Back in 2004, DARPA had a seedling project which had an RC car autonomously drive through a junk-filled alley way. It was around the time the first DARPA Grand Challenge took place, in which a full-sized car had to autonomously navigate a 240 km route in the Mojave desert.\nNo cars managed to finish the course that year, with the best competitor\u0026rsquo;s car (Carnegie Mellon) traveling a mere 11.78 kilometers.\nHere\u0026rsquo;s a picture of DAVE, taken from the DARPA-IPTO Autonomous Off-Road Vehicle Control using End-to-End learning report.\nI recommend reading about the original (and subsequent) DARPA Grand Challenges, and the DAVE project. It\u0026rsquo;s pretty fun to see how the history of autonomous vehicles began, and how the early researches found out about all of the challenges and their solutions on the way.\nThere\u0026rsquo;s also a documentary about the DARPA Grand Challenge called The Great Robot Race by Nova. You can easily find it online.\nThe CNN Architecture Here\u0026rsquo;s a picture of the architecture, taken from Nvidia\u0026rsquo;s blog:\nAs Nvidia says in their blog post, the architecture consists of 9 layers, including a normalization layer, 5 convolutional layers, and 3 fully connected layers.\nThe first layer of the network performs image normalization. The normalizer is hard-coded and is not adjusted in the learning process. Performing normalization in the network allows the normalization scheme to be altered with the network architecture, and to be accelerated via GPU processing.\nThe convolutional layers are designed to perform feature extraction, and are chosen empirically through a series of experiments that vary layer configurations. We then use strided convolutions in the first three convolutional layers with a 2×2 stride and a 5×5 kernel, and a non-strided convolution with a 3×3 kernel size in the final two convolutional layers.\nWe follow the five convolutional layers with three fully connected layers, leading to a final output control value which is the inverse-turning-radius. The fully connected layers are designed to function as a controller for steering, but we noted that by training the system end-to-end, it is not possible to make a clean break between which parts of the network function primarily as feature extractor, and which serve as controller.\nPretty cool. Let\u0026rsquo;s implement it in Keras!\nI\u0026rsquo;ve created a new Python source file in the donkeycar/parts folder, named nvidia.py.\nI used the KerasLinear class as a starting point, using its adjust_input_shape method to crop the image to the ROI passed to the model, since I think that\u0026rsquo;s a good way to get better performance.\nI\u0026rsquo;ve also made the following adjustments to the original architecture:\nI\u0026rsquo;ve omitted the normalization layer for now, which can be implemented using Keras\u0026rsquo; normalization layers. I\u0026rsquo;ve added a 25 unit fully connected layer between the 50 and 10 unit layers, and a 5 unit layer before the output layer. I\u0026rsquo;ve added dropout regularization, with a 90% keep probability. I used two separate output units for steering and throttle, using the KerasLinear model as a starting point. This is what the above described architecture looks like implemented in Keras:\ndef customArchitecture(num_outputs, input_shape, roi_crop): input_shape = adjust_input_shape(input_shape, roi_crop) img_in = Input(shape=input_shape, name=\u0026#39;img_in\u0026#39;) x = img_in # Dropout rate keep_prob = 0.9 rate = 1 - keep_prob # Convolutional Layer 1 x = Convolution2D(filters=24, kernel_size=5, strides=(2, 2), input_shape = input_shape)(x) x = Dropout(rate)(x) # Convolutional Layer 2 x = Convolution2D(filters=36, kernel_size=5, strides=(2, 2), activation=\u0026#39;relu\u0026#39;)(x) x = Dropout(rate)(x) # Convolutional Layer 3 x = Convolution2D(filters=48, kernel_size=5, strides=(2, 2), activation=\u0026#39;relu\u0026#39;)(x) x = Dropout(rate)(x) # Convolutional Layer 4 x = Convolution2D(filters=64, kernel_size=3, strides=(1, 1), activation=\u0026#39;relu\u0026#39;)(x) x = Dropout(rate)(x) # Convolutional Layer 5 x = Convolution2D(filters=64, kernel_size=3, strides=(1, 1), activation=\u0026#39;relu\u0026#39;)(x) x = Dropout(rate)(x) # Flatten Layers x = Flatten()(x) # Fully Connected Layer 1 x = Dense(100, activation=\u0026#39;relu\u0026#39;)(x) # Fully Connected Layer 2 x = Dense(50, activation=\u0026#39;relu\u0026#39;)(x) # Fully Connected Layer 3 x = Dense(25, activation=\u0026#39;relu\u0026#39;)(x) # Fully Connected Layer 4 x = Dense(10, activation=\u0026#39;relu\u0026#39;)(x) # Fully Connected Layer 5 x = Dense(5, activation=\u0026#39;relu\u0026#39;)(x) outputs = [] for i in range(num_outputs): # Output layer outputs.append(Dense(1, activation=\u0026#39;linear\u0026#39;, name=\u0026#39;n_outputs\u0026#39; + str(i))(x)) model = Model(inputs=[img_in], outputs=outputs) return model Now we need to implement our custom class, inheriting the base KerasPilot class:\nI used the adam optimizer and the mean squared error as the error function I\u0026rsquo;ve set the model using the above customArchitecture function, passing it the number of outputs, the input shape and the region of interest I\u0026rsquo;ve copied the KerasLinear run method, since it already implements everything needed to run class NvidiaModel(KerasPilot): def __init__(self, num_outputs=2, input_shape=(120, 160, 3), roi_crop=(0, 0), *args, **kwargs): super(NvidiaModel, self).__init__(*args, **kwargs) self.model = customArchitecture(num_outputs, input_shape, roi_crop) self.compile() def compile(self): self.model.compile(optimizer=\u0026#34;adam\u0026#34;, loss=\u0026#39;mse\u0026#39;) def run(self, img_arr): img_arr = img_arr.reshape((1,) + img_arr.shape) outputs = self.model.predict(img_arr) steering = outputs[0] throttle = outputs[1] return steering[0][0], throttle[0][0] Now we need to add our custom class to the utils.py script so we can use it through the manage.py script. I\u0026rsquo;ve appended the following lines to the get_model_by_type function, right after the rest of the pre-defined architectures:\nelif model_type == \u0026#34;nvidia\u0026#34;: from donkeycar.parts.nvidia import NvidiaModel kl = NvidiaModel(input_shape=input_shape,roi_crop=roi_crop) And that\u0026rsquo;s it! We can now train a model using the manage.py script. Here\u0026rsquo;s the result of training the architecture using this test dataset and running it in the simulator:\nNot bad, considering it\u0026rsquo;s trained on a really small dataset. Also, it\u0026rsquo;s not the best driving data around. You can see it on the sharp 90 degree turns. It first goes as near as it can to the middle line, to take a swing so it can enter the turn faster. If we\u0026rsquo;d trained it on a much bigger dataset, with better driving data, it\u0026rsquo;d work better.\nLet\u0026rsquo;s move on to the next chapter, how to train your model.\n"
},
{
	"uri": "http://localhost:1313/artificial-intelligence/simulator-mod/high-resolution-images/",
	"title": "Simulator mod: High resolution images",
	"tags": [],
	"description": "",
	"content": "If we want to use the simulator to gather training data that\u0026rsquo;s larger than the default 160x120 image size, we\u0026rsquo;ll need to create a modified version of it.\nSetting up the simulator locally First off, let\u0026rsquo;s clone (or fork) the original simulator Tawn Kramer made for Donkey from GitHub.\ngit clone --single-branch --branch donkey https://github.com/tawnkramer/sdsandbox If you\u0026rsquo;re wondering why aren\u0026rsquo;t I just using the git clone --branch command: it clones all branches, but checks out just the one you\u0026rsquo;ve passed to the flag. Since Unity projects can be quite large, and Tawn has several branches on the repo, we\u0026rsquo;d end up downloading a lot of stuff we don\u0026rsquo;t need.\nWe\u0026rsquo;ll also need to install Unity. First you need to download and install the Unity Hub:\nWindows download Mac OS X download Linux download After installing it, open it and select the Installs tab from the left sidebar, click the blue Add button on the right and install the latest official release.\nAfter installing the latest release of Unity, select the Projects tab on the left sidebar, click the silver Add button and select the sdsim folder from inside the folder where you\u0026rsquo;ve cloned/downloaded the simulator.\nYou can now click on the project you\u0026rsquo;ve added and you\u0026rsquo;re good to go.\nModifying the camera resolution After opening the project in Unity, open up the Prefabs folder (in the project view, lower left by default) and then open the donkey prefab, which is the default RC model prefab for the simulator.\nInside the prefab hierarchy (upper left by default) you can see the cameraSensorBase which contains the CameraSensor, which is the RC camera sensor that we use to generate our training data (take pictures from inside the simulator).\nAfter selecting the CameraSensor, we can see, in the inspector on the right side of the screen, that there is a CameraSensor script connected to it. You can open it by double clicking on it or finding it in the lower left project viewer inside Assets/Scripts/CameraSensor.cs.\nWe\u0026rsquo;ll make the width and height fields of the class static, so we can edit them from another script we\u0026rsquo;ll create:\n\\\\ You can put whatever resolution you want to be default here. public static int width = 640; public static int height = 480; And we\u0026rsquo;ll change the parameters of the ReadPixels function to use our width and height:\ntex.ReadPixels(new Rect(0, 0, width, height), 0, 0); We\u0026rsquo;ll also edit the CameraHelper script to use our static fields for the width and height:\nTexture2D texture2D = new Texture2D(CameraSensor.width, CameraSensor.height, TextureFormat.RGB24, false); texture2D.ReadPixels(new Rect(0, 0, CameraSensor.width, CameraSensor.height), 0, 0); Now go to the Scenes folder and open the menu scene and add a Dropdown element to the menu by right clicking on the Canvas and selecting UI \u0026gt; Dropdown.\nWe\u0026rsquo;ll be using this dropdown as our resolution picker, so go ahead and click on it, and inside the inspector panel add all of the resolutions you\u0026rsquo;d like to be able to use as options:\nThen resize and position the dropdown on a place you\u0026rsquo;d like it to be on the menu:\nNow go back to the Scripts folder and add a new class called ResolutionSetter. This class will set the resolution inside the CameraSensor class based on the selected option in the dropdown.\nHere\u0026rsquo;s the code for the ResolutionSetter class:\nusing System.Collections; using System.Collections.Generic; using UnityEngine; using UnityEngine.UI; public class ResolutionSetter : MonoBehaviour { //Attach this script to a Dropdown GameObject Dropdown m_Dropdown; //This is the string that stores the current selection m_Text of the Dropdown string m_Message; //This Text outputs the current selection to the screen public Text m_Text; //This is the index value of the Dropdown int m_DropdownValue; void Start() { //Fetch the Dropdown GameObject m_Dropdown = GetComponent\u0026lt;Dropdown\u0026gt;(); //Add listener for when the value of the Dropdown changes, to take action m_Dropdown.onValueChanged.AddListener(delegate { DropdownValueChanged(m_Dropdown); }); setCameraSensorRes(m_Dropdown.options[m_DropdownValue].text); } void DropdownValueChanged(Dropdown change) { setCameraSensorRes(m_Dropdown.options[change.value].text); } void setCameraSensorRes(string resolution){ CameraSensor.width = System.Convert.ToInt32(resolution.Split(\u0026#39;x\u0026#39;)[0]); CameraSensor.height = System.Convert.ToInt32(resolution.Split(\u0026#39;x\u0026#39;)[1]); } } Now go back to the Scenes folder and open the menu scene once again. Select the dropdown element we created and in the inspector panel, drag and drop the ResolutionSetter script on it:\nAnd that\u0026rsquo;s it. Now the output images will have the resolution that you\u0026rsquo;ve set in the main menu using the dropdown.\nUpdating the default donkey-gym port One thing we have to edit in donkey itself before we can use our modded simulator:\nGo to your donkeycar project folder\nGo to donkeycar/parts/ and open up dgym.py\nFind the following line and change the port number from 9090 to 9091:\ndef __init__(self, sim_path, port=9091, headless=0, env_name=\u0026#34;whatever-env-name-here\u0026#34;, sync=\u0026#34;asynchronous\u0026#34;): Now when you want to control your sim through the simulator manage.py script:\nStart it as you usually would, it should start the simulator automatically Open the simulator and select the track you\u0026rsquo;d like the car to drive on Click the NN Control over Network button Open up localhost:8887 in a browser and you should be good to go! If you\u0026rsquo;re compiling your model to a different directory than the default one, or you\u0026rsquo;ve changed the project/executable name, be sure to update the myconfig.py file, specifically the DONKEY_SIM_PATH variable.\nBuilding and using the modded simulator You can now go to File \u0026gt; Build Settings (or press CTRL + SHIFT + B), select Build at the bottom of the screen and select the folder where your simulator binary is. After building, you can use your modded version the same way you used the default Donkey simulator release.\nHere\u0026rsquo;s what a sample output image when the dropdown is set to 640x480:\n"
},
{
	"uri": "http://localhost:1313/hardware/",
	"title": "Hardware",
	"tags": [],
	"description": "",
	"content": "Assembling the real-world stuff Hardware Through this chapter we\u0026rsquo;ll go through assembling the hardware platform, all things hardware.\n"
},
{
	"uri": "http://localhost:1313/artificial-intelligence/visualization/",
	"title": "Visualizations",
	"tags": [],
	"description": "",
	"content": "Seeing what the network sees Visualization In this subchapter we\u0026rsquo;ll go through some methods that will allow us to easily debug our networks by actually seeing (in a way) what the network sees and how it thinks.\n"
},
{
	"uri": "http://localhost:1313/draft-and-todo/data-augmentation/",
	"title": "Augmenting the data with neural style transfer",
	"tags": [],
	"description": "",
	"content": "Augmenting the data by generating mirrored images to counter (counter)clockwise bias on tracks.\nUsing neural style transfers to make the model more robust to lighting changes and real life track reflectiveness!\nI\u0026rsquo;ve got this implemented, will think about writing it up! "
},
{
	"uri": "http://localhost:1313/artificial-intelligence/camera-calibration/implementing-camera-calibration/",
	"title": "Camera calibration: Implementing the calibration and undistortion",
	"tags": [],
	"description": "",
	"content": "First we\u0026rsquo;ll import the stuff we need and declare some variables:\nimport numpy as np import cv2, os, glob objectPoints = [] imagePoints = [] cameraIntrinsicValues = [] # Distortion coefficients cameraExtrinsicValues = [] Now we\u0026rsquo;ll implement the function that finds and returns the object and image points, given images of a chessboard:\ndef getObjectAndImagePoints(): global objectPoints, imagePoints # Number of inside corners per row and column cornersPerRow = 10 cornersPerColumn = 7 # Initializing the object points to zero chessboardObjectPoints = np.zeros((cornersPerColumn * cornersPerRow, 3), np.float32) # Prepare a meshgrid for object points # (0,0,0), (1,0,0), (2,0,0) ..., (cornersPerRow,cornersPerColumn,0) # We can do this since we know how many corners there are on our printed chessboard chessboardObjectPoints[:,:2] = np.mgrid[0:cornersPerRow, 0:cornersPerColumn].T.reshape(-1, 2) # List of calibration images images = [] # To make sure you can run the script on any image filetype extensions = [\u0026#39;*.gif\u0026#39;, \u0026#39;*.png\u0026#39;, \u0026#39;*.jpeg\u0026#39;, \u0026#39;*.jpg\u0026#39;, \u0026#39;*.tiff\u0026#39;] for extension in extensions: images.extend(glob.glob(\u0026#39;calibration_images/\u0026#39;+extension)) # Step through the list and search for chessboard corners for calibrationImageFileName in images: calibrationImage = cv2.imread(calibrationImageFileName) # The detector doesn\u0026#39;t work well with images larger than 1280x720 # So we\u0026#39;ll resize any until they\u0026#39;re 720p or smaller height, width = calibrationImage.shape[:2] while width \u0026gt; 1280: width //= 2 height //= 2 calibrationImage = cv2.resize(calibrationImage, (width, height)) # Convert it to grayscale grayCalibrationImage = cv2.cvtColor(calibrationImage, cv2.COLOR_BGR2GRAY) # Find the image points cornersFound, foundCorners = cv2.findChessboardCorners(grayCalibrationImage, (cornersPerRow, cornersPerColumn),None) # If corners were found on the images # Append the found image points and defined object points to global variables if cornersFound: objectPoints.append(chessboardObjectPoints) imagePoints.append(foundCorners) # If you want to visualize the found corners cv2.drawChessboardCorners(calibrationImage, (cornersPerRow, cornersPerColumn), foundCorners, cornersFound) cv2.imshow(\u0026#39;Preview\u0026#39;, calibrationImage) cv2.waitKey(500) Now we can call the calibrateCamera function to get our extrinsic and intrinsic values:\ndef calibrateCamera(imageSize): global cameraIntrinsicValues, cameraExtrinsicValues, objectPoints, imagePoints retVal, cameraIntrinsicValues, cameraExtrinsicValues, rotationVectors, translationVectors = cv2.calibrateCamera(objectPoints, imagePoints, imageSize, None, None) And finally, with all of the values, we can undistort our image:\ndef undistortImage(image): return cv2.undistort(image, cameraIntrinsicValues, cameraExtrinsicValues, None, cameraIntrinsicValues) You can download the above code as python script here.\rcameraCalibration.py\r(2 KB)\r"
},
{
	"uri": "http://localhost:1313/extras/how-is-this-website-made/",
	"title": "How is this website made?",
	"tags": [],
	"description": "",
	"content": "I made this site using:\nHugo - a static site generator that converts my markdown source files into this website. Learn - a theme for Hugo which provides this sweet design and other functions. Typora - for actually typing up all of the content you see here in markdown. Netlify - which hosts this site (and so many others) for free! And Visual Studio Code for some theme and Hugo source files editing. The process is pretty simple:\nFor every page I\u0026rsquo;d like, I write down a markdown file in Typora. Then I run the Hugo executable to compile the new markdown files into web pages. Then I just commit my static files to my GitHub repo, which Netlify automatically deploys to my website. Pretty sweet, huh? I think so too. And it\u0026rsquo;s free! And secure!\n"
},
{
	"uri": "http://localhost:1313/rc-car/parts_list/",
	"title": "Parts: an overview",
	"tags": [],
	"description": "",
	"content": "There’s a lot of, and I mean a lot of parts when it comes to building an RC car on your own. But in the context of building a self-driving RC car, here’s a brief overview of some of the stuff we’ll need:\nYou\u0026rsquo;ll want to have: A RC car (with some batteries) A PWM/Servo Driver (I2C + some jumper cables) A Jetson Nano A powerbank (+ some usb cables) A microSD card (and optionally an external SSD) A WiFi/BT m.2 card (key E) or some USB equivalent Some tools and materials for building the chassis (and optionally access to a 3D printer) An Xbox/PS/PC gamepad Let’s focus on the RC car (and the batteries) part for now.\nWhat RC car should I get? You can basically get any RC car, as long as it has an ESC controller, or you can hook up one to it.\nSome parts are more important than others, but here’s a overview of the parts we’ll be focusing on:\nScale Body type Electric motor Servo ESC Receiver Batteries "
},
{
	"uri": "http://localhost:1313/artificial-intelligence/simulator-mod/custom-rc-model/",
	"title": "Simulator mod: Custom RC model",
	"tags": [],
	"description": "",
	"content": "If you want to get better data for your RC, you should edit the default model that comes with the simulator. Most notably, you should edit the camera position, resolution and field of view. You can also add multiple cameras and edit the center of mass of your vehicle, as well as any other properties you think would make a difference if you wanted to use the simulator to pre-train the weights for your RC.\nLet\u0026rsquo;s see how we can do that.\nEditing the camera Open up the Unity project and open the donkey prefab, found in Assets/Prefabs/donkey.fbx.\nIn the prefab hierarchy, select the cameraSensorBase and then select the CameraSensor.\nOn the right, in the Inspector panel, under the Camera settings, you can edit a bunch of settings related to it. I\u0026rsquo;d suggest you edit the FOV value to match the FOV your real camera has, and I\u0026rsquo;d recommend disabling the Fisheye and Post Processing scripts.\nYou can also move the camera in different positions using the move and rotate tools and the editor on the center of the screen, and pivot/angle it to match your real RC.\nTesting it out You can open up any a scene, e.g. the warehouse scene, zoom up to the track and drag and drop your prefab onto it. After dropping it on the track, select the camera sensor to see what its output looks like:\nAdding additional parts and changing the center of mass If you\u0026rsquo;d like, you can always create a custom 3D model of the electronics that are on your RC and insert it into the default donkey model. You can use the default donkey.fbx that comes with the simulator and edit it, or you can make your own from scratch.\nAfter creating a 3D model, drag and drop it into the Models folder. Now open up the donkey prefab and drag your model wherever you\u0026rsquo;d like on the Hierarchy panel. I\u0026rsquo;d recommend putting it inside the donkey element. You can then position and resize your elements to fit them on the RC.\nIf you want to reorder any elements or edit them individually, but the element looks blue, like this power bank v1 element, you can right click on it and select unpack prefab:\nThen you\u0026rsquo;ll be able to move individual parts around.\nIf you\u0026rsquo;ve made significant changes to the model, and added a lot of parts, your RC\u0026rsquo;s center of mass has probably changed, and you should reflect that in the simulator by changing the defined center of mass, since you want it to realistically handle corners just as it would in real life, and if your center of mass is higher up because you\u0026rsquo;ve added a bunch of stuff on your RC, it could perform great in the simulator but tip over in real life.\nYou can find the centerOfMass element inside the donkey prefab, and move and pivot it around just like any other element:\nYou can now build your simulator and try your model out.\n"
},
{
	"uri": "http://localhost:1313/artificial-intelligence/visualization/cnn-activations/",
	"title": "Visualization: CNN Activations",
	"tags": [],
	"description": "",
	"content": "We already saw that the custom architecture/model works, but it\u0026rsquo;s surely not the best we could come up with. The question is, how can we improve its performance and get some intuition on how it works?\nSeeing what the network sees One useful visual tool we could use is Donkey\u0026rsquo;s built in cnnactivations command. Per the official docs, it shows us the feature maps for each convolutional layer in the model we pass to it. The feature maps show us how each filter in every conv layer activates, given the input image we pass to it.\nThe tool currently works with only 2D convolutional layers.\nYou can use the command as follows:\ndonkey cnnactivations --model pathTo/yourModel.h5 --image pathTo/anImage.jpg Here are the results I got while running this picture through the Nvidia model we\u0026rsquo;ve trained previously:\nInput resolution? Now, we can see a couple of things, but most notably, that it\u0026rsquo;s hard to actually see things on these images. And in case you\u0026rsquo;re wondering, it\u0026rsquo;s not because they\u0026rsquo;re resized. They\u0026rsquo;re pretty low res, since the input image is only 160x120 and due to the nature of convolutions it only gets smaller.\nThe formula for the output size of a convolutional layer is: ++ \\Big\\lfloor{\\frac{n+2p-f}{s}+1}\\Big\\rfloor ++ Where $n$ is an input dimension, $p$ is the padding, $f$ is the number of filters and $s$ is the stride size.\nLet\u0026rsquo;s calculate the output size of our first convolutional layer, which was:\nx = Convolution2D(filters=12, kernel_size=5, strides=(2, 2), input_shape = input_shape, activation=\u0026#39;relu\u0026#39;, name=\u0026#34;Conv2D_First_layer\u0026#34;)(x) The input dimensions of our image are 160 and 120, the other parameters are easily seen, except for the padding, which is valid (aka zero), since we didn\u0026rsquo;t specify it explicitly.\n++ width=\\Big\\lfloor{\\frac{160-5}{2}+1}\\Big\\rfloor = \\Big\\lfloor{78.5}\\Big\\rfloor = 78 ++\n++ height=\\Big\\lfloor{\\frac{120-5}{2}+1}\\Big\\rfloor = \\Big\\lfloor{58.5}\\Big\\rfloor = 58 ++\nSo after the first layer, we\u0026rsquo;ll have 12 filters (or images) with the size of 78x58. Let\u0026rsquo;s look at the other layers:\nSecond layer: 37x27, 24 filters Third layer: 17x12, 48 filters Fourth layer: 8x5, 64 filters Fifth layer: 3x2, 64 filters So yeah, those are some small resolutions. I mean, it will work, but when you think about it, would you be able to perfectly drive a car, if your eyes saw this:\nOr this:\rThe smaller video is made out of 160x120 images, and the higher res one is made out of 640x360 images. The smaller images are around 3.2KB each, and the bigger ones are around 22KB each, which is almost seven times as big, for a fourfold increase in resolution. So you can probably imagine, it\u0026rsquo;s more expensive computationally, but it also offers way more data to the network to work with.\nThe images are by default smaller because Donkey was originally made for the Raspberry Pi, which doesn\u0026rsquo;t have a discrete GPU, so it can handle a lot less computationally intensive tasks. But since we\u0026rsquo;re working with a Jetson, we should be able to increase the resolution a bit, and if necessary implement some feature engineering and preprocessing to speed things up a little on higher res inputs.\nSo we\u0026rsquo;ll be increasing the input resolution to be able to train more complex models.\nLet\u0026rsquo;s move on to another visualization technique: saliency maps!\n"
},
{
	"uri": "http://localhost:1313/artificial-intelligence/simulator-mod/",
	"title": "Creating a custom simulator",
	"tags": [],
	"description": "",
	"content": "Adding high res images and stuff Modding the simulator In this subchapter we\u0026rsquo;ll go through the modification of the simulator to allow custom RC models and higher res datasets.\n"
},
{
	"uri": "http://localhost:1313/software/",
	"title": "Software",
	"tags": [],
	"description": "",
	"content": "Making hardware do stuff Software In this chapter we’ll go deal with all things software we need before starting our ML/DL work.\n"
},
{
	"uri": "http://localhost:1313/artificial-intelligence/camera-calibration/calibrating-the-camera/",
	"title": "Camera calibration: Calibrating the camera and undistorting images",
	"tags": [],
	"description": "",
	"content": "Finally, we can actually calibrate our camera and undistort our images!\nWe\u0026rsquo;ll start with the real camera on our RC and then we\u0026rsquo;ll also calibrate our simulator camera!\nOnce calibrated on the calibration images, you can use the same undistortion matrix for any other image that the same camera takes (given the focal length hasn\u0026rsquo;t changed)!\nCalibrating your real camera I\u0026rsquo;ll be using my RC camera, the EleCam Explorer 4K, which has an advertised 170 degree FOV.\nFirst, we need to print a checkerboard pattern so we can take some calibration rig photos.\nThere\u0026rsquo;s a pretty cool checkerboard collection by Mark Hedley Jones which you can use to generate a checkerboard/chessboard pattern you like. I\u0026rsquo;ll be using this one:\nIf you use another pattern, make sure to update the number of inner corners for the rows and columns in the getObjectAndImagePoints function we made earlier.\nAfter printing it out on an A4 paper, you should take at least 10 photos of it in different angles, e.g.:\nAlso, most action cams with a FOV as large as this one (170°) will have some built-in distortion correction, probably named something like fisheye correction or adjust:\n![Fisheye adjust](/images/ai/fisheye adjust.png)\nI\u0026rsquo;ve intentionally left mine off, to show how distorted the images are by default at such a big FOV, and to show that they can be undistorted even for those whose camera doesn\u0026rsquo;t have a built-in option to do so.\nAfter copying the images to my calibration_images folder and calling the getObjectAndImagePoints script, here\u0026rsquo;s what the detected image points look like:\nAfter getting the image points, we can call the calibrateCamera function once, and then undistortImage to undistort our images for as many new images as we want, here\u0026rsquo;s what the previous two image look like undistorted:\nCalibrating the simulator camera First, we need to put a checkerboard object into the simulator so we can take photos of it with our RC:\nOpen up Unity and open the project Go to Scenes and open the small_looping_course (or any course you\u0026rsquo;d like) In the Hierarchy panel (upper left) expand the world element and double click the starting_line element The camera should\u0026rsquo;ve automatically zoomed into the starting position from which your RC begins the drive when you first open up the track in the simulator Click on the GameObject menu in the main menu bar: Click 3D object \u0026gt; Cube Convert your checkerboard PDF to a JPG using a tool online or however you\u0026rsquo;d like Drag and drop your JPG in the Unity Assets/Materials folder Drag and drop the JPG from the Unity editor onto the Cube If you want to make the cube proportions fit the original checkerboard size, e.g. an A4 paper, after clicking on the cube, you can edit the scale values in the Inspector panel and set the values to be e.g. 0.297 for X and 0.21 for Y, since an A4 is 29.7 cm x 21.0 cm.\nRun the simulator in full screen at a high resolution (720p/1080p) and open the track in which you\u0026rsquo;ve added the checkerboard Select Joystick/Keyboard No Rec Drive around the course and take multiple screenshots of your cube, under different angles, just like you\u0026rsquo;d do with your real camera Screenshot hotkeys: Windows: Win key + Shift + S Mac OS X: Shift + Cmd + 3 Linux: Print Screen key (somewhere between F12 and Scroll Lock) After you\u0026rsquo;ve got yourself some images, you can run them through the same procedure as you would if you used a real camera. Here\u0026rsquo;s what mine looked like:\nHow are we going to use it for our neural net? You can undistort every image like this before you input it to the neural network. Here\u0026rsquo;s what the code could look like:\n# At the beginning of run getObjectAndImagePoints() calibrateCamera(inputImage.shape[1::-1]) # For every input image to the NN undistortedImage = undistortImage(inputImage) # Pass it along to the NN "
},
{
	"uri": "http://localhost:1313/artificial-intelligence/visualization/saliency-maps/",
	"title": "Visualization: Saliency Maps",
	"tags": [],
	"description": "",
	"content": "Another useful visual tool to see how your network works is a saliency map. They were proposed back in 1998 by Itti, Koch and Niebur, a group of neuroscientists working on feature extraction in images, in a paper titled A Model of Saliency-based Visaul Attention for Rapid Scene Analysis.\nIn the context of Deep Learning and convolutional neural networks, they were first mentioned by the Visual Geometry Group at the University of Oxford, in a paper called Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps\nI won\u0026rsquo;t go into excessive detail on how it works, but in our case, a saliency map will show us which pixels on the input image make the most impact on the output inferred by the network. We will make a heat map that shows us which visual features the car uses the most, in order to steer and throttle correctly. Hopefully, it should be the lane lines.\nNow, just a day after I\u0026rsquo;ve implemented a script that generates a video with a saliency map, given a trained model and some input data, I\u0026rsquo;ve noticed that Tawn Kramer (of Donkey) has already implemented a saliency map visualization, which you can generate using the donkey makemovie command. But since I\u0026rsquo;ve already made my implementation, we\u0026rsquo;ll walk through it.\nAs a starting point, I\u0026rsquo;ve used this Jupyter Notebook made by ermolenkodev.\nI\u0026rsquo;ll be using the Nvidia model we made earlier throughout the implementation. Feel free to use your own stuff in here.\nImplementing the Saliency Map vizualization First off, we need to import all the Keras stuff we use in our model:\nfrom donkeycar.parts.keras import KerasPilot from tensorflow.python.keras.layers import Input, Dense from tensorflow.python.keras.models import Model, Sequential from tensorflow.python.keras.layers import Convolution2D, Convolution2D, MaxPooling2D, Reshape, BatchNormalization from tensorflow.python.keras.layers import Activation, Dropout, Flatten, Cropping2D, Lambda Then we can copy the function that implements our model:\n# The ROI crop helper function def adjust_input_shape(input_shape, roi_crop): height = input_shape[0] new_height = height - roi_crop[0] - roi_crop[1] return (new_height, input_shape[1], input_shape[2]) # The definition of our model # Also, be sure to name every convolutional layer you have as \u0026#34;convx\u0026#34; (x ∈ ℕ) def customModel(num_outputs=2, input_shape=(160,120,3), roi_crop=(0,0)): input_shape = adjust_input_shape(input_shape, roi_crop) img_in = Input(shape=input_shape, name=\u0026#39;img_in\u0026#39;) x = img_in # Dropout rate keep_prob = 0.5 rate = 1 - keep_prob # Convolutional Layer 1 x = Convolution2D(filters=12, kernel_size=5, strides=(2, 2), activation=\u0026#39;relu\u0026#39;, name=\u0026#34;conv1\u0026#34;)(x) x = Dropout(rate)(x) # Convolutional Layer 2 x = Convolution2D(filters=24, kernel_size=5, strides=(2, 2), activation=\u0026#39;relu\u0026#39;, name=\u0026#34;conv2\u0026#34;)(x) x = Dropout(rate)(x) # Convolutional Layer 3 x = Convolution2D(filters=48, kernel_size=5, strides=(2, 2), activation=\u0026#39;relu\u0026#39;, name=\u0026#34;conv3\u0026#34;)(x) x = Dropout(rate)(x) # Convolutional Layer 4 x = Convolution2D(filters=64, kernel_size=3, strides=(1, 1), activation=\u0026#39;relu\u0026#39;, name=\u0026#34;conv4\u0026#34;)(x) x = Dropout(rate)(x) # Convolutional Layer 5 x = Convolution2D(filters=64, kernel_size=3, strides=(1, 1), activation=\u0026#39;relu\u0026#39;, name=\u0026#34;conv5\u0026#34;)(x) x = Dropout(rate)(x) # Flatten Layers x = Flatten()(x) # Fully Connected Layer 1 x = Dense(100, activation=\u0026#39;relu\u0026#39;)(x) # Fully Connected Layer 2 x = Dense(50, activation=\u0026#39;relu\u0026#39;)(x) # Fully Connected Layer 3 x = Dense(25, activation=\u0026#39;relu\u0026#39;)(x) x = Dense(10, activation=\u0026#39;relu\u0026#39;)(x) x = Dense(5, activation=\u0026#39;relu\u0026#39;)(x) outputs = [] for i in range(num_outputs): outputs.append(Dense(1, activation=\u0026#39;linear\u0026#39;, name=\u0026#39;n_outputs\u0026#39; + str(i))(x)) model = Model(inputs=[img_in], outputs=outputs) return model We can then initialize the model and pass the weights we previously trained to it:\nmodel = customModel()\r# Pass the path to your trained .h5 file here\rmodel.load_weights(\u0026#39;nvidia.h5\u0026#39;) Now we can define just the convolutional layers we\u0026rsquo;re interested in visualizing:\nimg_in = Input(shape=(160,120,3), name=\u0026#39;img_in\u0026#39;) x = img_in x = Convolution2D(filters=12, kernel_size=5, strides=(2, 2), activation=\u0026#39;relu\u0026#39;, name=\u0026#34;conv1\u0026#34;)(x) x = Convolution2D(filters=24, kernel_size=5, strides=(2, 2), activation=\u0026#39;relu\u0026#39;, name=\u0026#34;conv2\u0026#34;)(x) x = Convolution2D(filters=48, kernel_size=5, strides=(2, 2), activation=\u0026#39;relu\u0026#39;, name=\u0026#34;conv3\u0026#34;)(x) x = Convolution2D(filters=64, kernel_size=3, strides=(1, 1), activation=\u0026#39;relu\u0026#39;, name=\u0026#34;conv4\u0026#34;)(x) lastConvLayer = Convolution2D(filters=64, kernel_size=3, strides=(1, 1), activation=\u0026#39;relu\u0026#39;, name=\u0026#34;conv5\u0026#34;)(x) convolution_part = Model(inputs=[img_in], outputs=[lastConvLayer]) We can now set the weights of the convolutional layers we just created to the actual weights our model contains:\n# If you have more than 5 layers, or less than 5 layers, edit the number here numberOfConvLayers = 5 for layer_num in range(1, numberOfConvLayers): convolution_part.get_layer(\u0026#39;conv\u0026#39; + str(layer_num)).set_weights(model.get_layer(\u0026#39;conv\u0026#39; + str(layer_num)).get_weights()) To get the activation values from each layer, we need to define functors for them:\nfrom tensorflow.python.keras import backend as K inp = convolution_part.input # input placeholder outputs = [layer.output for layer in convolution_part.layers[1:]] # all layer outputs functors = K.function([inp], outputs) We\u0026rsquo;ll also define some helper variables that we\u0026rsquo;ll use for the strides, padding and the kernels while computing the visualization masks:\nimport tensorflow as tf import numpy as np import pdb # 3x3 kernel with all ones kernel_3x3 = tf.constant(np.array([ [[[1]], [[1]], [[1]]], [[[1]], [[1]], [[1]]], [[[1]], [[1]], [[1]]] ]), tf.float32) # 5x5 kernel with all ones kernel_5x5 = tf.constant(np.array([ [[[1]], [[1]], [[1]], [[1]], [[1]]], [[[1]], [[1]], [[1]], [[1]], [[1]]], [[[1]], [[1]], [[1]], [[1]], [[1]]], [[[1]], [[1]], [[1]], [[1]], [[1]]], [[[1]], [[1]], [[1]], [[1]], [[1]]] ]), tf.float32) # Based on the layers in your model, you should assign the kernel sizes you\u0026#39;re using at each layer here. # E.g. I\u0026#39;m using a 3x3 kernel in my last two layers, and a 3x3 in my first three layers layers_kernels = {5: kernel_3x3, 4: kernel_3x3, 3: kernel_5x5, 2: kernel_5x5, 1: kernel_5x5} # Same goes here for the strides you\u0026#39;re using in your layers layers_strides = {5: [1, 1, 1, 1], 4: [1, 1, 1, 1], 3: [1, 2, 2, 1], 2: [1, 2, 2, 1], 1: [1, 2, 2, 1]} And we can finally compute the visualization masks using ermolenkodev\u0026rsquo;s function:\ndef compute_visualisation_mask(input_image): activations = functors([np.array([input_image])]) activations = [np.reshape(input_image, (1, input_image.shape[0], input_image.shape[1], input_image.shape[2]))] + activations upscaled_activation = np.ones((53, 73)) for layer in [5, 4, 3, 2, 1]: # Edit if you have a different # of layers averaged_activation = np.mean(activations[layer], axis=3).squeeze(axis=0) * upscaled_activation output_shape = (activations[layer - 1].shape[1], activations[layer - 1].shape[2]) x = tf.constant( np.reshape(averaged_activation, (1,averaged_activation.shape[0],averaged_activation.shape[1],1)), tf.float32 ) conv = tf.nn.conv2d_transpose( x, layers_kernels[layer], output_shape=(1,output_shape[0],output_shape[1], 1), strides=layers_strides[layer], padding=\u0026#39;VALID\u0026#39; ) with tf.Session() as session: result = session.run(conv) upscaled_activation = np.reshape(result, output_shape) final_visualisation_mask = upscaled_activation return (final_visualisation_mask - np.min(final_visualisation_mask))/(np.max(final_visualisation_mask) - np.min(final_visualisation_mask)) And the only thing we need to implement is the code to animate the results and save them as a video:\nimport cv2 import numpy as np import matplotlib.pyplot as plt from matplotlib import animation from IPython.display import display, HTML def save_movie_mp4(image_array, filename=\u0026#39;output.gif\u0026#39;, fps=30): dpi = 72.0 xpixels, ypixels = image_array[0].shape[0], image_array[0].shape[1] fig = plt.figure(figsize=(ypixels/dpi, xpixels/dpi), dpi=dpi) im = plt.figimage(image_array[0]) def animate(i): im.set_array(image_array[i]) return (im,) writer = animation.PillowWriter(fps=fps) anim = animation.FuncAnimation(fig, animate, frames=len(image_array)) anim.save(filename, writer=writer) We can now finally pass our model and our input data to the program and let it create the Saliency Map video for us:\nimport glob, re, time, datetime # The path to your dataset pathToData = \u0026#39;data/tub/\u0026#39; # Output video name output = \u0026#34;saliency.gif\u0026#34; # Output FPS fps = 60 # Number of frames you want to use numberOfFrames = 600 def sort_human(l): convert = lambda text: float(text) if text.isdigit() else text alphanum = lambda key: [convert(c) for c in re.split(\u0026#39;([-+]?[0-9]*\\.?[0-9]*)\u0026#39;, key)] l.sort(key=alphanum) return l inputImages = [] alpha = 0.004 beta = 1.0 - alpha counter = 0 print(\u0026#34;Generating %ds of video.\u0026#34; % (numberOfFrames/fps)) accumulatedTime = 0 start = time.time() for path in sort_human(glob.glob(pathToData + \u0026#39;*.jpg\u0026#39;)): inputImage = cv2.imread(path) salient_mask = compute_visualisation_mask(inputImage) salient_mask_stacked = np.dstack((salient_mask,salient_mask)) salient_mask_stacked = np.dstack((salient_mask_stacked,salient_mask)) blend = cv2.addWeighted(inputImage.astype(\u0026#39;float32\u0026#39;), alpha, salient_mask_stacked, beta, 0.0) inputImages.append(blend) counter += 1 if counter \u0026gt;= numberOfFrames: break elif counter % 100 == 0: end = time.time() accumulatedTime += end - start remainingSeconds = (accumulatedTime/counter)*(numberOfFrames-counter) print(\u0026#34;Generated %d/%d frames.\u0026#34; % (counter, numberOfFrames)) print(\u0026#34;Estimated time left: %dm:%ds.\u0026#34; % divmod(remainingSeconds, 60)) print(\u0026#34;Runtime so far: %dm:%ds.\u0026#34; % divmod(accumulatedTime, 60)) start = time.time() And save the gif:\nsave_movie_mp4(inputImages, output, fps) You can download the above code as an ipynb here.\rSaliencyMap.ipynb\r(11 KB)\rThe results The output video I got using the Nvidia model we made earlier and the dataset we downloaded:\nWe can see that the car indeed uses the lane lines, but it also uses the horizon as a feature a lot. That\u0026rsquo;s quite interesting. We can get rid of that problem using a ROI crop or by implementing some computer vision feature extraction/engineering, which we\u0026rsquo;ll do right after we make a high res version of the simulator. :)\nWe can also see that it\u0026rsquo;s far more interested in the right line, than the middle (left) one. That\u0026rsquo;s because, in general, the car tends to go to the right, since we\u0026rsquo;re driving around the circuit clockwise. We need to do some data augmentation to solve this issue, which we\u0026rsquo;ll also do a bit later.\n"
},
{
	"uri": "http://localhost:1313/artificial-intelligence/",
	"title": "Artificial Intelligence",
	"tags": [],
	"description": "",
	"content": "Making the RC intelligent(ish) Artificial Intelligence In this chapter we\u0026rsquo;ll finally begin working on the machine learning/deep learning part. Some would even call it AI. :)\n"
},
{
	"uri": "http://localhost:1313/artificial-intelligence/camera-calibration/",
	"title": "Camera distortions and calibration",
	"tags": [],
	"description": "",
	"content": "Removing distortions Camera calibration In this subchapter we\u0026rsquo;ll go through some basic types of camera distortions and how to fix them.\n"
},
{
	"uri": "http://localhost:1313/rc-car/scale/",
	"title": "Scale",
	"tags": [],
	"description": "",
	"content": "\rMost RC cars are scaled down versions of their real-life equivalent, so they\u0026rsquo;re expressed in ratios, the most common ones being (real-life size : RC model size):\n1:18 1:16 1:10 1:8 there are also all sorts of scales in between those (and above/below) Of course, the question is: why do we care, and what\u0026rsquo;s better for a self-driving RC car?\nIt\u0026rsquo;s pretty simple:\na bigger RC car equals more real estate and more power to carry all of our gadgets on top of it, without damaging the motors while struggling with all of the weight, but a bigger car needs a bigger race track and road size we want to drive it on "
},
{
	"uri": "http://localhost:1313/extras/static-websites/",
	"title": "Static websites",
	"tags": [],
	"description": "",
	"content": " A static web page is delivered to the user exactly as stored on the server. By contrast, a dynamic web page first has to be generated using a web application. Which of course means that static websites display the same web pages for every user, as opposed to dynamic websites which can dynamically generate sites tailored to their users (at a significant cost and overhead). Static sites can still dynamically do stuff, but they have to do it at the user\u0026rsquo;s end (e.g. using JavaScript). I\u0026rsquo;ll briefly explain how some of it works in the background, but if you\u0026rsquo;re not interested and just want a guide on how to make a website like this, feel free to skip to the link below.\nCLICK HERE TO SKIP What is a static site generator? Static site generators (SSG) build static websites using a source directory of files and templates. Or in layman\u0026rsquo;s terms, an SSG takes a file like a markdown document, and generates a static website/web page from it. So why should you use a static site and a static site generator? Netlify has a pretty neat list that I\u0026rsquo;ll try to abbreviate below:\nSecurity: By having only static HTML/CSS/JS files you\u0026rsquo;re very much minimizing your attack surface compared to running a bunch of software like a CMS (e.g. WordPress, Drupal) with its own database being run in a virtual machine running some OS\u0026hellip;\nThat doesn\u0026rsquo;t mean static sites don\u0026rsquo;t have an attack surface at all though. They can, and do, depending on you. But by default, especially if you\u0026rsquo;re a beginner, you have much better security out of the box, and I\u0026rsquo;ll go through what to do to make sure it stays secure.\nSpeed: Whenever someone visits your site, especially if you\u0026rsquo;re using a CDN (more on that later), the load speeds (e.g. time-to-first-byte) will be much faster (or at worst the same) than a CMS.\nThe reason behind this is that you\u0026rsquo;re giving the browser exactly what it wants (which is static HTML/CSS/JS) as soon as it requests the site, since that is actually the only stuff you have on your server and there is no need to render it into static files before serving it to the viewer.\nSmaller footprint (Speed + Security + Price?): A static site can be hosted on any server that can just return HTML files. Which means, you don\u0026rsquo;t have to find hosting that provides machines running software specific to your site (e.g. PHP, MySQL, WordPress, Nginx/Apache, whatever Linux distro you\u0026rsquo;d like). Which means you don\u0026rsquo;t have to worry about the hosting company (or you yourself) keeping all of the software and dependencies up to date. Which means that once you\u0026rsquo;ve set up your site, you can just focus on actually doing stuff you\u0026rsquo;re interested with it (e.g. writing). It also means you\u0026rsquo;ll get a lot more for a lot, lot cheaper (even free!). Scalability If you ever get to the sweet trouble of having much more visitors than you\u0026rsquo;d\u0026rsquo;ve expected, that won\u0026rsquo;t be an issue at all if you\u0026rsquo;re using a static site. Scaling it up just means increasing your bandwidth with your provider, so more requests can be served, and it usually just means clicking a few buttons and paying a few bucks extra. What static site generator to use? There sure is a bunch of them. And there are quite a few good ones. You can see a list of them here, but I\u0026rsquo;d recommend going with Hugo. It has a friendly, active community that will help you out and it is really well developed, fast and as secure as it gets. But you can use any other generator you\u0026rsquo;d like, the process is mostly the same and is just as straightforward for other SSGs.)\n"
},
{
	"uri": "http://localhost:1313/artificial-intelligence/visualization/donkey-makemovie/",
	"title": "Visualization: donkey makemovie",
	"tags": [],
	"description": "",
	"content": "The makemovie command is a great tool to visually inspect and debug your model. Here are some example uses:\nTo create just a video of the training data, with an overlay that shows steering: donkey makemovie --tub=pathToYour/data/ --out=outputVideo.mp4\nTo create a video with an overlay of your model steering and the training data steering: donkey makemovie --tub=pathToYour/data/ --out=outputVideo.mp4 --model=yourModel.h5 --type=modelType\nTo create a video with a saliency map and both overlays: donkey makemovie --tub=pathToYour/data/ --out=outputVideo.mp4 --model=yourModel.h5 --type=modelType --salient\n"
},
{
	"uri": "http://localhost:1313/extras/",
	"title": "Extras",
	"tags": [],
	"description": "",
	"content": "Additional stuff Extras Some other stuff, like how I created this website and possibly other tooling/pipeline stuff in the future.\n"
},
{
	"uri": "http://localhost:1313/rc-car/body_type/",
	"title": "RC Car body types",
	"tags": [],
	"description": "",
	"content": "\rThe best body type for on road self-driving purposes is the standard race body type.\nBut to be thorough, we could roughly group all of the RC cars in 4 distinct categories:\nRACE/STREET Probably the first thing that comes to mind when thinking of an RC car, a standard race car. This body type is the fastest and the best on paved, flat surfaces and is meant for on road use only.\nThere is also a specialized group of RC cars of this body type that are meant for drifting on racing tracks. The main difference are the tires, which are much slicker than regular ones.\nBUGGY Buggies are the golden middle between an off-road and on road RC car. That being said, they can go off-road, unlike a regular racing car, so that makes them the best bet for people that aren\u0026rsquo;t sure if they want to go off-road or on road. They are the second best thing on the road but the slowest off-road.\nTRUGGY Truggies are also an in-between body type, but they lean more to the off-road role, as opposed to the buggies. Consider them a buggy with monster truck tires.\nThey\u0026rsquo;re the second fastest body type off-road, and the third fastest on road.\nTRUCKS These are your regular monster trucks. For a self-driving application, they\u0026rsquo;d be pretty bad, since they tend to flip over while making on road turns, so it\u0026rsquo;s best to stick with them only if you\u0026rsquo;re planning to go off road in the woods, grass or dirt.\nThey are the best for going off-road but the slowest for going on road.\n"
},
{
	"uri": "http://localhost:1313/draft-and-todo/",
	"title": "Drafts and #TODO&#39;s",
	"tags": [],
	"description": "",
	"content": "To be published soon Drafts and #TODO\u0026rsquo;s I\u0026rsquo;ll keep some drafts and TODO placeholders here that I\u0026rsquo;m intending to publish through a week\u0026rsquo;s time.\n"
},
{
	"uri": "http://localhost:1313/rc-car/electric_motors/",
	"title": "Electric motors",
	"tags": [],
	"description": "",
	"content": "The main question concerning electric motors is: brushed or brushless? Brushed pros: cheaper, simpler, better for off-road.\nBrushed cons: heavier, bigger, worse power efficiency (75-80%), they wear out in time.\nBrushless pros: long lifespan, much better speed and handling, better power efficiency (85-90%).\nBrushless cons: much more expensive, worse for off-road.\nSo what should we get? It depends on your budget, but brushed motors work just fine, and besides, for self-driving purposes, you don’t need a RC car that drives 100 KPH. It’s always possible to swap out a brushed motor for a brushless later on.\n"
},
{
	"uri": "http://localhost:1313/artificial-intelligence/computer-vision-lane-finding/",
	"title": "Finding lane lines easier",
	"tags": [],
	"description": "",
	"content": "Remember we\u0026rsquo;ve showed before that our CNN is taking the horizon as the input feature, and that we\u0026rsquo;ll be addressing it after making a simulator mod that\u0026rsquo;ll allow us to take high res images. Well, here we are.\nWhat we\u0026rsquo;re going to do To solve the horizon problem and simultaneously help the car recognize lane lines better, we\u0026rsquo;ll do the following:\nPerform a perspective transform on every input image to get a birds-eye view of the road Convert the image to HLS color space, extract only the S channel and perform some thresholding on it to extract only the lanes from the input image Perspective transform There\u0026rsquo;s one really useful thing we can do with our input images, and we kinda already did it while we were calibrating our camera; a perspective transform. So, what\u0026rsquo;s a perspective transform? Let\u0026rsquo;s say that this is our input image:\nWe talked a bit about how cameras map the 3D world onto a 2D plane, which certainly has some downsides. One of them would be this:\nWe know that the lane lines in the real world are parallel, but if the only input information we had was the image above, we could easily assume that they intersect somewhere in the distance. Maybe it\u0026rsquo;s easier to see on a photo of a real road:\nWe could easily assume that the road gets narrower and narrower further on, until the outer lines intersect somewhere in the distance.\nNow, if we had a birds-eye perspective, like this:\nWe could easily see that the lane lines are parallel throughout the road, and that the width of a lane should stay the same.\nLuckily for us, we don\u0026rsquo;t have to train an autonomous drone to fly above our car and send images of the road to it, we can just do a perspective transform, and go from this:\nTo this:\nImplementing the perspective transform As always, we\u0026rsquo;ll be importing the stuff we need first, we\u0026rsquo;ll be using numpy and OpenCV (and matplotlib for plotting the images in our Jupyter notebook, which you can download at the bottom of this page):\nimport numpy as np import cv2 import matplotlib.pyplot as plt %matplotlib inline Since we\u0026rsquo;re working in an Jupyter notebook and we\u0026rsquo;re mixing OpenCV with matplotlib, we\u0026rsquo;ll define some helper functions for showing our images, to make our code shorter and cleaner:\n# So we don\u0026#39;t have to use cv2.cvtColor everytime def showImage(image): plt.figure() plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB)) # So we can show titled images too def showTitledImage(title, image): plt.figure() plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB)) plt.title(title) plt.axis(\u0026#39;off\u0026#39;) Now we can open our image:\nimage = cv2.imread(\u0026#39;path/toYour/image.jpg\u0026#39;) showImage(image) Next, we\u0026rsquo;ll need to define the region of interest on our input image that we\u0026rsquo;d like to transform into our birds-eye perspective. You can get these coordinates by opening your image in an image editor and manually selecting the pixels that denote four edges of your ROI.\nTo help you better understand, here\u0026rsquo;s an image of my ROI:\nNext, you should write down the pixel coordinates of each corner of your image into an numpy array:\n# Define the region of the image we\u0026#39;re interested in transforming regionOfInterest = np.float32( [[0, 180], # Bottom left [112.5, 87.5], # Top left [200, 87.5], # Top right [307.5, 180]]) # Bottom right To clarify, my bottom left coordinate is the pixel where the red and the blue line intersect, whose position is (0, 180). The same goes for the three remaining corners.\nI\u0026rsquo;d like that the selected ROI looks like a square after the transform, where the left and right lane lines are parallel to each other, so I\u0026rsquo;ve defined my target perspective coordinates to look like:\nThe numpy array containing those coordinates looks like:\n# Define the destination coordinates for the perspective transform newPerspective = np.float32( [[80, 180], # Bottom left [80, 0.25], # Top left [230, 0.25], # Top right [230, 180]]) # Bottom right So both arrays look like:\n# Define the region of the image we\u0026#39;re interested in transforming regionOfInterest = np.float32( [[0, 180], # Bottom left [112.5, 87.5], # Top left [200, 87.5], # Top right [307.5, 180]]) # Bottom right # Define the destination coordinates for the perspective transform newPerspective = np.float32( [[80, 180], # Bottom left [80, 0.25], # Top left [230, 0.25], # Top right [230, 180]]) # Bottom right If you\u0026rsquo;d like to visualize your coordinate arrays by drawing lines over the original image like I did above, you can use the following function:\n# Function that draws coordinates on an image, connecting them by lines def drawLinesFromCoordinates(coordinates): # Pair first point with second, second with third, etc. points = zip(coordinates, np.roll(coordinates, -1, axis=0)) # Connect point1 with point2: plt.plot([x1, x2], [y1, y2]) for point1, point2 in points: plt.plot([point1[0], point2[0]], [point1[1], point2[1]], linewidth=5) And call it like:\n# Draw the image to visualize the selected region of interest showTitledImage(\u0026#34;My ROI\u0026#34;, image) drawLinesFromCoordinates(regionOfInterest) Once you have your coordinates set, that\u0026rsquo;s actually it. The rest of the implementation just calls a couple OpenCV functions, the first of which is cv2.getPerspectiveTransform, which takes in the starting coordinates we\u0026rsquo;ve defined (our ROI) and the target coordinates and gives us back a matrix with which we can warp our image to the target perspective.\nThis actually isn\u0026rsquo;t that complicated, it\u0026rsquo;s actually pretty simple math which generates a transformation matrix with which tells us how to map some input to a defined output. Also, if you\u0026rsquo;d like to unwarp your image back into its original perspective, you can get an inverse transformation matrix by calling the cv2.getPerspectiveTransform and switching the coordinate parameters (see code below).\nSo all of the above, in code, looks like this:\n# Compute the matrix that transforms the perspective transformMatrix = cv2.getPerspectiveTransform(regionOfInterest, newPerspective) # Compute the inverse matrix for reversing the perspective transform inverseTransformMatrix = cv2.getPerspectiveTransform(newPerspective, regionOfInterest) Once we have our transformation matrix, we warp our image using cv2.warpPerspective. So when we implement all of the above, we get something like this:\n# Warp the perspective # image.shape[:2] takes the height, width, # [::-1] inverses it to width, height warpedImage = cv2.warpPerspective(image, transformMatrix, image.shape[:2][::-1], flags=cv2.INTER_LINEAR) # If you\u0026#39;d like to see the warped image showImage(warpedImage) And that\u0026rsquo;s it for the perspective transform! If you\u0026rsquo;d like, you can just plug in the warped image into a convnet and let it find the lane lines from it. But you can also make its job a lot easier by using just a few simple and computationally inexpensive tricks.\nMaking the lane lines easier to see There are many different color spaces. We\u0026rsquo;re all most used to the RGB color space. OpenCV uses the BGR color space by default. One color space that\u0026rsquo;s shown to be pretty good for our type of task is called HSL, and stands for hue, saturation and lightness.\nWe can convert our warped image from BGR (since we\u0026rsquo;ve opened our image using OpenCV and by default it uses BGR) to HSL with the cv2.cvtColor function:\n# Convert the image to HLS colorspace # Annoyingly enough, OpenCV calls it HLS, not HSL hlsImage = cv2.cvtColor(warpedImage, cv2.COLOR_BGR2HLS) # Didn\u0026#39;t use showImage since it tries to convert BGR to RGB by default plt.imshow(hlsImage) So we transform our warped image from RGB:\nTo HSL:\nIt certainly doesn\u0026rsquo;t seem any easier to distinguish the lane lines from the HSL image than the RGB image, it actually seems a bit harder, until we split the image into three separate channels:\n# Split the image into three variables by the channels hChannel, lChannel, sChannel = cv2.split(hslImage) showTitledImage(\u0026#34;H channel\u0026#34;, hChannel) showTitledImage(\u0026#34;L channel\u0026#34;, lChannel) showTitledImage(\u0026#34;S channel\u0026#34;, sChannel) The hue channel doesn\u0026rsquo;t look very helpful:\nNeither does the lightness channel:\nBut the saturation channel certainly does:\nThe reason for this is that the lane lanes are always saturated with color, be it white, yellow or any other color, which is what makes them stand out in the saturation channel, compared to the asphalt or the sand in the background.\nWe could\u0026rsquo;ve tried to find our lane lines by searching for pixels of a specific color, but that method would be far more susceptible to errors when the lane color changes due to light or weather conditions, or if we come across a lane that\u0026rsquo;s just a bit differently colored than what we\u0026rsquo;ve expected, e.g. during construction.\nThis method is more robust since it doesn\u0026rsquo;t care what color the lane line is, it just cares that it\u0026rsquo;s really saturated with color.\nThere\u0026rsquo;s one more thing we could do to make the lane lines pop out, as opposed to the lightly activated pixels on the asphalt, the sand on the right and the RC shadow in the middle of the lane. We could threshold the values of the saturation channel image, which basically means we\u0026rsquo;d only keep the pixels that are above a certain level of saturation, keeping only the most saturated pixels of the image.\nThat\u0026rsquo;s pretty easy to do using OpenCV, and you can (and should) experiment with the threshold values by using a lot of different pictures of the road taken in different lighting conditions and of different lane line colors.\nThis is what the code looks like:\n# Threshold the S channel image to select only the lines lowerThreshold = 65 higherThreshold = 255 # Threshold the image, keeping only the pixels/values that are between lower and higher threshold returnValue, binaryThresholdedImage = cv2.threshold(sChannel,lowerThreshold,higherThreshold,cv2.THRESH_BINARY) # Since this is a binary image, we\u0026#39;ll convert it to a 3-channel image so OpenCV can use it # Doesn\u0026#39;t really matter if we use RGB/BGR/anything else thresholdedImage = cv2.cvtColor(binaryThresholdedImage, cv2.COLOR_GRAY2RGB) showTitledImage(\u0026#34;Thresholded S channel\u0026#34;, thresholdedImage) Note that the thresholded image will be a binary image, which means that any pixel that was above (or below) our threshold value (i.e. the pixels that we kept) will have a value of 255, which is the maximum value for the color channel (we\u0026rsquo;re using the 0-255 range).\nSo the output image will look like this:\nThe white pixels are the ones that had a value larger than 65 in the original saturation channel image, and their value is 255 on the above image. The rest of the pixels which didn\u0026rsquo;t cross the lower threshold were discarded and their value was set to 0, which is black in the image above.\nWe can now feed this image into our network and it\u0026rsquo;ll be much easier for it to find the lane lines.\nThe entire process visualized This is what the entire process would look like, in a video:\n## Alternative approaches, and why not use them\rWe also could\u0026rsquo;ve used a number of different approaches as opposed to our use of the S channel. We also could\u0026rsquo;ve combined multiple of them. A lot of people do. Let\u0026rsquo;s go through some of them and I\u0026rsquo;ll explain why I didn\u0026rsquo;t use them.\nFitting lanes using a polynomial approximation One thing I\u0026rsquo;ve seen a lot of people do, most of them during their Udacity Self-Driving Car Nanodegree program, is using a sliding window to find the lanes in an image processed in a way similar to ours, and fitting a polynomial to it.\nFirst, they would take the processed binary image and compute a histogram of the lower half of the image, to try and determine where the lane lines begin. This is what such a histogram would look like for our (entire, not just the bottom half) processed image:\nYou can get the histogram by simply adding up the pixel values on the y axis, for each X-axis value. Here\u0026rsquo;s the code:\n# Sum the pixels on the x axis # This will sum all pixel values for a given x axis (column) histogram = np.sum(binaryThresholdedImage, axis=0) plt.plot(histogram) This is what it looks like overlaid on our processed image:\nThe method then splits the histogram into two parts, using the two X-axis that have the largest peak in the histogram as the starting positions for the two lane lines, and implements a sliding window search, going from the bottom up, that identifies the positions of all non-zero pixels on the image, and saves them into arrays.\nAfter obtaining the four arrays, two for the left line (x and y) and two for the right one, it uses the numpy polyfit method to fit a polynomial to the line, and thus gets two polynomials that each define the lane.\nOnce the lanes are found in the first frame, the sliding windows don\u0026rsquo;t go through the entire process again for the next frame. They use the previous frame as a starting position and use a margin at the top of the lane line to detect just the change from the first to the second frame, since it can\u0026rsquo;t change that drastically from one frame to another.\nThe fitted polynomials can then be easily visualized, and since I\u0026rsquo;ve actually implemented this, this is what the result would look like in our simulator:\nThat looks pretty cool and works relatively well, so why not use it?\nWell, I\u0026rsquo;ve tried to use it by passing on the fitted polynomials to the network, by passing the warped image with the detected lane overlaid on it:\nand by passing the original image with the detected lane overlaid on it\nThe two main issues with the method are:\nIt\u0026rsquo;s computationally expensive to do all of the processing and fitting for every single frame It can go haywire if your car moves the slightest out of the predefined ROI, and even if you quickly correct it, since it uses the previous frames as a start for the next one, to be more computationally efficient, it would take some time for it to correct itself. I\u0026rsquo;ve also tried not using the previous frame and doing all of the work for every single frame, and it takes a lot of computational power to do so. Also, I wouldn\u0026rsquo;t like my model to rely so much on feature engineering and extraction, I\u0026rsquo;d like it to be more robust, even if it means it will be harder to train or take longer to train. By simply passing our processed image to a small convolutional network, it should be able to get all the information it needs from it, and it should be way less computationally expensive in the long run, even if we\u0026rsquo;ve added a couple of additional convolutional layers to our model. Even if we passed the fitted polynomials to the network, we\u0026rsquo;d still have to train the network to learn how to interpret them, and taking into consideration the short tests I did, I believe it would be less robust and even more computationally expensive to do so.\nUsing additional preprocessing methods Apart from transforming the perspective of our image and getting the thresholded S channel image, we could\u0026rsquo;ve also used a number of different techniques to make the lane lines more visible. One example would be using the Sobel operator, which allows us to take the derivative of an image in the x or the y direction.\nI\u0026rsquo;ve tried a number of them, including Sobel, which I\u0026rsquo;ll show below, and I\u0026rsquo;ve found that they wouldn\u0026rsquo;t really help much, at least on my test data.\nHere\u0026rsquo;s an example of Sobel on both axes using these two input pictures:\nHere\u0026rsquo;s the images after applying the Sobel operator over both axes:\nHere are the above images after np.absolute:\nAnd here are the above images after applying a binary threshold on them, with the lower threshold being 200 and the upper 255:\nHere\u0026rsquo;s the code if you want to try it out for yourself:\ndef showSobelImages(images): f, axarr = plt.subplots(2,2) axarr[0,0].imshow(images[0], cmap=\u0026#39;gray\u0026#39;) axarr[0,0].axis(\u0026#39;off\u0026#39;) axarr[0,1].imshow(images[1], cmap=\u0026#39;gray\u0026#39;) axarr[0,1].axis(\u0026#39;off\u0026#39;) axarr[1,0].imshow(images[2], cmap=\u0026#39;gray\u0026#39;) axarr[1,0].axis(\u0026#39;off\u0026#39;) axarr[1,1].imshow(images[3], cmap=\u0026#39;gray\u0026#39;) axarr[1,1].axis(\u0026#39;off\u0026#39;) grayWarpedImage = cv2.cvtColor(warpedImage, cv2.COLOR_BGR2GRAY) plt.imsave(\u0026#39;graywarped.jpg\u0026#39;, grayWarpedImage, cmap=\u0026#39;gray\u0026#39;) plt.imsave(\u0026#39;schannel.jpg\u0026#39;, sChannel, cmap=\u0026#39;gray\u0026#39;) # Taking the derivative on the X-axis xAxisSobelWarped = cv2.Sobel(grayWarpedImage, cv2.CV_64F, 1, 0, ksize=5) xAxisSobelSChannel = cv2.Sobel(grayWarpedImage, cv2.CV_64F, 1, 0, ksize=5) # Taking the derivative on the Y-axis yAxisSobelWarped = cv2.Sobel(grayWarpedImage, cv2.CV_64F, 0, 1, ksize=5) yAxisSobelSChannel = cv2.Sobel(grayWarpedImage, cv2.CV_64F, 0, 1, ksize=5) images = [xAxisSobelWarped, xAxisSobelSChannel, yAxisSobelWarped, yAxisSobelSChannel] showSobelImages(images) # Absolute values showSobelImages([np.absolute(x) for x in images]) lowerThreshold = 200 higherThreshold = 255 thresholdedImages = [] for image in images: returnValue, image = cv2.threshold(np.absolute(image), lowerThreshold,higherThreshold, cv2.THRESH_BINARY) thresholdedImages += [image] showSobelImages(thresholdedImages) And you can download the entire Jupyter notebook below: Jupyter notebook\rlaneFinding.ipynb\r(1 MB)\r"
},
{
	"uri": "http://localhost:1313/draft-and-todo/reinforcement-learning/",
	"title": "Reinforcement learning: letting the car learn to drive on its own",
	"tags": [],
	"description": "",
	"content": "Using reinforcement learning in the real world for cars is pretty dangerous and expensive, since they\u0026rsquo;ll be crashing into everything they can at the very beginning. To counter this, we\u0026rsquo;ll be creating an OpenAI environment in which the car learns to drive by itself in order to pretrain weights for real life use.\nTried this, worked but wasn\u0026rsquo;t anything extraordinary, will think about writing it up! Take a look at the donkey-gym for examples if you really want to try it. Good luck.\n"
},
{
	"uri": "http://localhost:1313/extras/prerequisites/",
	"title": "You&#39;ll need:",
	"tags": [],
	"description": "",
	"content": " Git\nWindows installer here. Linux instruction here (but it should come with Linux out of the box). Mac OS X installer here. A Markdown editor\nI very much recommend Typora, I\u0026rsquo;m writing this using it. But you can use anything you like, even VS Code. A Hugo theme you like\nYou can see a list of themes, along with demos here. You can also search GitHub or the Web and find any other theme you\u0026rsquo;d like. I\u0026rsquo;m going to be showing two themes: Minimal for the test website, which I\u0026rsquo;m using at my InfoSec/Hardware blog (hosted on GitHub Pages). Learn, which is the theme of the website you\u0026rsquo;re reading this on (hosted on Netlify). If you\u0026rsquo;re using GitHub Pages and Jekyll or any other combination, they all come with a bunch of themes you can choose from. Or you can create something of your own. :) Hugo\nHugo has great official docs on how to do this, but as always, TL;DR: Any platform: Download the latest release from GitHub (v69.0 at the time of writing). Just copy the binary file to a folder where you want your websites to be. Platform specific alternatives here (e.g. Homebrew, Chocolatey). "
},
{
	"uri": "http://localhost:1313/extras/creating-a-demo-website/",
	"title": "Creating a website with Hugo",
	"tags": [],
	"description": "",
	"content": "Go to the folder where your Hugo binary is and run:\nhugo new site quickstart Add the theme you\u0026rsquo;d like to your site\u0026rsquo;s themes directory:\ncd quickstart git init # Replace the URL below with your theme git submodule add https://github.com/calintat/minimal.git themes/minimal If you\u0026rsquo;re wondering how to find the URL for your theme:\nAssuming you\u0026rsquo;re on themes.gohugo.io and on your theme\u0026rsquo;s page, just copy the link from the download button and you\u0026rsquo;re good! Alternatively, if the theme comes only in a .zip file, just extract the zip into the themes directory. Customizing the theme and adding content The main configuration file for each Hugo website is called config.toml found in the root directory of the website. There, you can edit and config a bunch of stuff.\nEvery theme also comes with its own specific settings, so what I like to do right after I install a theme is to go into the theme folder and copy the contents of the exampleSite folder to the root of my website. This way we get some demo content that comes with the theme and we also copy the config.toml of the theme to our website, so we can see how the author configured their own demo site.\nIn my specific case, I need to go to quickstart/themes/minimal/exampleSite and copy all of the folder contents to quickstart.\nconfig.toml Opening up the config.toml file we just copied to the quickstart directory, we can see a bunch of settings. Let\u0026rsquo;s go through them:\nbaseURL = \u0026#34;http://example.com/\u0026#34; # Your website URL will go here languageCode = \u0026#34;en-us\u0026#34; # The language of the site title = \u0026#34;Minimal\u0026#34; # The title of the site theme = \u0026#34;minimal\u0026#34; # The theme we\u0026#39;re using disqusShortname = \u0026#34;username\u0026#34; # Delete this, they show ads on your site. googleAnalytics = \u0026#34;\u0026#34; # Delete this too. :) The baseURL is used by Hugo to generate links inside your website, so make sure to keep it up to date when you publish your website. The rest is pretty self-explanatory.\nNext up, params. The values from this part of the config file can be used through the .Site.Params variable in templates (e.g. {{ $.Site.Params.description }} in HTML).\nDon\u0026rsquo;t worry if you don\u0026rsquo;t understand it, it\u0026rsquo;s used when you want to create your own templates or toy around with more advanced stuff. The theme uses it throughout the website to enable some functionalities, such as fonts, syntax highlighters, background colors, etc.\n[params] author = \u0026#34;Your Name\u0026#34; description = \u0026#34;What\u0026#39;s your site about\u0026#34; githubUsername = \u0026#34;yourUsername\u0026#34; accent = \u0026#34;red\u0026#34; # Accent color for the theme showBorder = true # Theme setting for the border backgroundColor = \u0026#34;white\u0026#34; # Background color for the theme font = \u0026#34;Raleway\u0026#34; # Theme font - can use any Google Font name highlight = true # Should the theme highlight syntax highlightStyle = \u0026#34;default\u0026#34; # In what style? highlightLanguages = [\u0026#34;go\u0026#34;, \u0026#34;haskell\u0026#34;, \u0026#34;kotlin\u0026#34;, \u0026#34;scala\u0026#34;, \u0026#34;swift\u0026#34;] # What languages? And the rest of the config file lists some menu items and icons, shouldn\u0026rsquo;t be a problem to figure out.\nThe content After updating the config.toml file with your info and any changes you\u0026rsquo;d like, you can navigate to the content folder of your website. In this case quickstart/content. This is where all of your actual source content is, which Hugo then takes and compiles to a static website.\nEvery content file begins with a header which tells Hugo what type of content it is and how to compile it. This varies from theme to theme, so you\u0026rsquo;ll have to look it up, but the author of every time I\u0026rsquo;ve ever used has always provided both explanations and demo content like this, so it should be easy to understand.\nThis specific theme has three content types:\nA regular page (e.g. About):\n# Note: I had to escape the dashes using # since Hugo was rendering them as markdown on my site # --- title: About description: What is Hugo? # --- A project page:\n# ---\rtitle: \u0026#34;Project 1\u0026#34;\rdescription: \u0026#34;Lorem ipsum dolor sit amet\u0026#34;\rrepo: \u0026#34;#\u0026#34; # delete this line if you want a blog-like page\rtags: [\u0026#34;go\u0026#34;, \u0026#34;golang\u0026#34;, \u0026#34;hugo\u0026#34;]\rweight: 1\rdraft: false\r# --- And a blog post type:\n# ---\rtitle: \u0026#34;Creating a New Theme\u0026#34;\rdate: 2014-09-28\rtags: [\u0026#34;go\u0026#34;, \u0026#34;golang\u0026#34;, \u0026#34;hugo\u0026#34;, \u0026#34;themes\u0026#34;]\rdraft: false\r# --- As you can see, the only difference between the blog post and project page types is the link to the GitHub repo. The regular page type has only a title and description.\nIf you\u0026rsquo;re using Typora, press ALT + CTRL + S to enter the Source Code Mode so you can see the header.\nThe content can also be hierarchically structured using folders, but I\u0026rsquo;ll get to that later on, through the second theme (the one you\u0026rsquo;re reading this on).\nCompiling and running the website Since this is just a quick demo website to check everything works, I won\u0026rsquo;t be adding or editing any content.\nTo compile your content and theme into a static website, just run Hugo inside your website root folder, e.g. on my Windows machine:\nPS C:\\Users\\ori\\Desktop\\demo\\quickstart\u0026gt; hugo.exe Building sites … | EN +------------------+----+ Pages | 50 Paginator pages | 2 Non-page files | 0 Static files | 1 Processed images | 0 Aliases | 19 Sitemaps | 1 Cleaned | 0 Total in 178 ms The static website can now be found inside the public folder, in this case quickstart/public.\nBut helpfully, Hugo includes a local server so you can preview your website locally before publishing it, you can run it as:\nhugo server Or in my case:\nPS C:\\Users\\ori\\Desktop\\demo\\quickstart\u0026gt; hugo.exe server Building sites … | EN +------------------+----+ Pages | 50 Paginator pages | 2 Non-page files | 0 Static files | 1 Processed images | 0 Aliases | 19 Sitemaps | 1 Cleaned | 0 Built in 32 ms Watching for changes in C:\\Users\\ori\\Desktop\\demo\\quickstart\\{archetypes,content,data,layouts,static,themes} Watching for config changes in C:\\Users\\ori\\Desktop\\demo\\quickstart\\config.toml Environment: \u0026#34;development\u0026#34; Serving pages from memory Running in Fast Render Mode. For full rebuilds on change: hugo server --disableFastRender Web Server is available at http://localhost:1313/ (bind address 127.0.0.1) Press Ctrl+C to stop The cool thing about this is that it will show any edits to your content in real-time in your browser. :)\nYou can also pass various flags to it, like -D to enable drafts or --disableFastRender to force full re-renders whenever you update a source content file.\nThis is what the demo website looks like for me after visiting localhost:1313\nAnd that\u0026rsquo;s actually it! You just add content to your content folder and Hugo does the rest for you.\n"
},
{
	"uri": "http://localhost:1313/rc-car/servo/",
	"title": "Steering servo",
	"tags": [],
	"description": "",
	"content": "\nAn RC servo is used for controlling the steering wheels of the car. It almost always comes with the RC car, so you shouldn’t worry about getting one.\nIt typically expects around 4.8V to 6V input on the power wire (varies by car) and a PWM control signal on the signal wire. Typically, the three wires are colored black-red-white, or brown-red-yellow, where:\nthe dark wire (black/brown) is ground, and the center wire (red) is power, and the light wire (white/yellow) is control. The control signal is RC-style PWM, where one pulse is sent 60 times a second, and the width of this pulse controls how left/right the servo turns. When this pulse is:\n1500 microseconds, the servo is centered; 1000 microseconds, the servo is turned all the way left (or right) 2000 microseconds, the servo is turned all the way in the other direction. This is NOT the same kind of PWM that you would use to control the duty cycle of a motor, or the brightness of a LED.\nThe power for the servo typically comes from the motor ESC, which has a BEC (Battery Eliminator Circuit) built in.\nSource: DonkeyCar docs.\n"
},
{
	"uri": "http://localhost:1313/rc-car/electronic_speed_controller/",
	"title": "Electronic Speed Controller",
	"tags": [],
	"description": "",
	"content": " The role of the ESC is to take a RC PWM control signal (pulse between 1000 and 2000 microseconds) in, and use that to control the power to the motor so the motor spins with different amounts of power in forward or reverse. Many RC car kits come with an ESC preinstalled, in which case you should be just fine.\nAgain, 1500 microseconds typically means \u0026ldquo;center\u0026rdquo; which for the motor means \u0026ldquo;dead stop.\u0026rdquo;\nThe battery typically connects straight to the ESC using thicker wiring than the simple control signals, because the motor draws many more amps than the control. The ESC then connects on to the motor with equally thick power wiring.\nStandard motors and ESCs have a peak current of about 12A; a 1/8th scale RC car with powerful brushless motor can have a peak draw up to 200A.\nAdditionally, the ESC typically contains a linear or switching voltage converter that outputs the power needed to control the steering servo; this is typically somewhere in the 4.8V to 6V range. Most BECs built into ESCs will not deliver more than about 1A of current, so it is not typically possible to power both the steering servo and the Jetson Nano from the BEC.\nThe main thing to look out for when getting an ESC on your own is to be sure to match your motor type to your ESC type (brushed/brushless).\nSource: DonkeyCar docs.\n"
},
{
	"uri": "http://localhost:1313/extras/hosting/",
	"title": "Where will we be hosting the site?",
	"tags": [],
	"description": "",
	"content": "I\u0026rsquo;d recommend using GitHub Pages or Netlify. The main difference is:\nGitHub Pages just hosts your website, without any additional features, but you can host unlimited project sites. And yes, you can use a custom domain for each one. It\u0026rsquo;s also dead simple to use. You can use either Hugo or Jekyll or whatever you\u0026rsquo;d like. Netlify provides only one free website, for one team member, with 100 GB of bandwidth per month. Unless you\u0026rsquo;re creating an open-source project or documentation, in which case you can get the Pro plan for free, which is pretty sweet. Netlify also has a ton of useful features that just work out of the box, the most useful being continuous deployment, which automatically gets any new markdown page files from your GitHub repository, compiles them to HTML using Hugo and serves them to your visitors immediately. If you\u0026rsquo;re creating an open-source project or docs, go with Netlify, they\u0026rsquo;re absolutely fantastic, and their team is a bunch of good people (e.g. Sarah and Cassidy, who you should follow on Twitter if you wanna see and learn cool stuff :)).\nIf you don\u0026rsquo;t care about continuous deployment (imho, you should) and some of the work being automatized for you (imho, you should), but you do care about being able to host a dozen of sites, go with GitHub Pages. One great thing about both, they\u0026rsquo;ll automatically deploy an SSL/TLS certificate for you, for free! (Thanks Let\u0026rsquo;s Encrypt ♥)\nOkay, we can get started!\nHosting the website Okay, now we can host our demo website on Netlify or GitHub Pages!\nYou\u0026rsquo;ll need a GitHub, GitLab or Bitbucket account to host your site on Netlify, or a GitHub account if you\u0026rsquo;re hosting on GitHub Pages.\nI\u0026rsquo;ll assume you have a GitHub account so I can show both Netlify and GitHub Pages.\nCreating a new repository First, you\u0026rsquo;ll need to create a repository for your website.\nOn GitHub, click on the plus icon in the upper right corner and select New repository: I\u0026rsquo;ll name my repository demo and set it to be private and click Create repository.\nCool, we now have a place to upload/publish our website.\nPublishing the site to a GitHub repository Open up a terminal/shell in your website folder and enter the following:\ngit init # Initialises a git repository in the current directory git add . # Adds all of the files in the directory to the repository git commit -m \u0026#34;First commit!\u0026#34; # Records the changes made in the directory to the repo Now go to your repository on GitHub and copy the quick setup URL:\nAnd go back to the terminal/shell and enter:\ngit remote add origin https://github.com/yourUsername/yourRepo.git git push -u origin master And your website should now be visible in your GitHub repository!\nHosting the site using GitHub Pages To host your site using GitHub pages, the static website files have to be either:\nIn a separate branch called gh-pages In a folder called docs (in the master branch) In the root of the master branch I think the easiest way is just to publish the public folder containing the static files to a new gh-pages branch, which you can do by opening up a terminal in your website folder and running:\ngit subtree push --prefix public origin gh-pages And that\u0026rsquo;s it really. Your website is now published at https://yourUsername.github.io/yourRepoName.\nIf you\u0026rsquo;re doing it this way, you\u0026rsquo;ll need to push to both master and gh-pages each time you want to publish your site. That way you\u0026rsquo;ll keep the original source content files and publish the finished site.\nOne last thing to do though. Remember we said we\u0026rsquo;d need to update our baseURL setting in config.toml when we actually publish the site somewhere so Hugo knows how to create links? Let\u0026rsquo;s do that now.\nFirst, open up your repository on GitHub and open the Settings tab in the upper right: Scroll down to GitHub pages and copy your website URL: Update the baseURL in the config.toml of your website:\nbaseURL = \u0026#34;https://ivanorsolic.github.io/demo/\u0026#34; Compile the site and push the changes to your repository. This is what you\u0026rsquo;ll be doing any time you\u0026rsquo;ve added new content or updated something on your website:\n# Terminal at the website root hugo git add . git commit -m \u0026#34;Updated config.toml: baseURL\u0026#34; git push git subtree push --prefix public origin gh-pages Now you can visit your website online at the URL you copied from your repo settings:\nPretty sweet, huh? You can even bundle the commands from above into a script and just run it anytime you want to publish your website.\nHosting the site using Netlify First, sign up for Netlify using your GitHub account and authorize Netlify to access your GitHub account when the prompt comes up.\nNext, click the New site from Git button in the upper right: Connect whatever Git provider you\u0026rsquo;re using, I\u0026rsquo;ll assume its GitHub:\nChoose the repo containing your website: If you\u0026rsquo;re using Hugo, enter hugo as the build command and public as the publish directory, and click deploy site: And that\u0026rsquo;s it! Your website should be published on the URL Netlify has shown you after you\u0026rsquo;ve clicked on the Deploy site button.\n"
},
{
	"uri": "http://localhost:1313/rc-car/receiver/",
	"title": "Receiver",
	"tags": [],
	"description": "",
	"content": "\rIf you buy a \u0026ldquo;kit car\u0026rdquo; that is listed as \u0026ldquo;needs a receiver,\u0026rdquo; then you don\u0026rsquo;t need to buy a receiver.\nThe Jetson Nano and the PWM/Servo driver will replace the receiver, outputting control signals to the car. If you’re buying a kit with a steering servo, motor, and ESC, you should actually try to not get a receiver, since the RC car could be specifically designed for that receivers PWM signals, and you’ll be taking it apart anyways.\n"
},
{
	"uri": "http://localhost:1313/extras/using-a-custom-domain/",
	"title": "Using a custom domain",
	"tags": [],
	"description": "",
	"content": "It\u0026rsquo;s very easy to use a custom domain with both GitHub Pages and Netlify.\nFirst, you need a custom domain. You can buy it from wherever you\u0026rsquo;d like, I\u0026rsquo;ll assume you\u0026rsquo;re using Namecheap.\nGo to Namecheap domain search, find a domain you like and buy it.\nGo to your hosting provider:\nGitHub: Go to your repository, open up Settings, scroll down to the GitHub pages section and enter your domain. Netlify: Go to your website, open up Domain settings and add your domain as a custom domain. After adding the domain, click on the options dropdown (three dots) to the right of the domain you\u0026rsquo;ve just added and select Set Up Netlify DNS. Click on until you get to the screen that says Update your domain\u0026rsquo;s nameservers. Copy the list of the nameservers somewhere. Tip: Always add your domain to Netlify or GitHub pages first. Configuring the domain with your DNS provider without adding it to the hosting provider first could result in someone else being able to host a site on one of your subdomains. Go to your Namecheap domain list and select manage: Netlify: Scroll down to the Nameservers section, change it to Custom DNS, enter the list of the nameservers you\u0026rsquo;ve copied and press the green check mark and you\u0026rsquo;re done:\nGitHub: Open the Advanced DNS tab in the upper right corner, delete all of the records from the Host records and add the following:\nCNAME Record - www - yourUsername.github.io A record - @ - 185.199.108.153 A record - @ - 185.199.109.153 A record - @ - 185.199.110.153 A record - @ - 185.199.111.153 And you\u0026rsquo;re done. It takes some time for the changes (DNS) to propagate, but you should be able to access your site through your new custom domain in a couple minutes to about a half an hour!\n"
},
{
	"uri": "http://localhost:1313/artificial-intelligence/integrating-the-lane-finding/",
	"title": "Advanced lane finding model",
	"tags": [],
	"description": "",
	"content": "\nThe model will consist of two parallel CNNs, each of which end with a dense 100-unit layer, which we will then concatenate and pass through three additional dense layers, and end with two linear activations. The model should have about 6.5 million parameters, which take up about 2GB of VRAM, so we should be able to run it on our Jetson Nano with half that much RAM to spare. :)\nThe idea of teaching a vehicle to drive autonomously Now I know that a whole separate (and rather big) CNN is an overkill for the thresholded binary image, but the main reason why I\u0026rsquo;m currently doing it like this is to test if its possible to have multiple specialized parts of the architecture all siphon in to a smaller final part (the last n dense layers) in order for the car to make a decision where to steer and how much to throttle, which is more or less how you do everything while you\u0026rsquo;re in a car, parking, driving, you just combine multiple inputs, e.g.:\nChecking your mirrors and dead angle to make sure you\u0026rsquo;re safe to do a maneuver - collision avoidance, object recognition and tracking Recognizing a sign that says what exit you should take in order to get to your destination Actually knowing where you are with respect to that exit and by what path can you get to it - path planning, localization Actually give the appropriate steering and throttle to actually perform the maneuver So no matter what we\u0026rsquo;re doing in a car, be it parking, changing lanes or driving to a destination, all we can really do to control the car is turn the steering wheel and control its speed (assuming no gear shifting, e.g. an electric car 😋), we\u0026rsquo;re just taking in the input from our surroundings, mostly using our eyes, which we analyze through a series of specialized procedures which ultimately lead us to control our car based on the decisions we\u0026rsquo;ve made, in order to perform a maneuver.\nSo what I wanted to do is to have a series of specialized parts in the net, which we could even call smaller subnets, which would take the input images and extract highly specific data from it, using (relatively) specialized procedures, which we would then plug into the final layer, along with the first convolutional network that uses the raw input image, which should give the final part of the network enough context about the world and enough information in order to appropriately control the RC.\nIt would look something like this:\nI was playing around with the idea in my mind when I saw Andrej Karpathy\u0026rsquo;s talk on PyTorch at Tesla, where he explained their use of HydraNets. In a nutshell, because they have a 1000 (!) distinct output tensors (predictions), and all of them have to know a ton of context and details about the scene, they use a shared backbone, like this (screenshot taken from YouTube: PyTorch at Tesla:\nThey actually have 48 networks that output a total of 1000 predictions, which is insane to do in real-time (on 1000x1000 images and 8 cameras) while being accurate enough to actually drive living humans on real roads. Though, they do have some pretty sweet custom hardware (FSD2), unlike our Jetson Nanos 😢.\nNow, it obviously makes much more sense to do what Tesla did, to have a shared backbone since a lot of the information that the backbone extracts from the input images can be applied to all of the specialized tasks, so you don\u0026rsquo;t have to learn them all over again for each and every one of them.\nBut I figured, what the heck, I\u0026rsquo;d try my idea out, which I did, since the main reason for doing this is to actually learn to apply ML/DL to something I could actually see drive around my backyard, and when we teach the car to do behaviours like lane changing in the next chapter, you\u0026rsquo;ll see that it actually works!\nThis is what it looks like in action:\nLet's implement it.\rCreating a Keras model First off, we\u0026rsquo;ll create a Keras model in the donkeycar/parts folder. I\u0026rsquo;ll be calling it OriModel.\nfrom donkeycar.parts.keras import KerasPilot from tensorflow.python.keras.models import Model, Sequential from tensorflow.python.keras.layers import Input, Dense, Activation, Dropout, Flatten, Conv2D from tensorflow.python.keras.layers.merge import Concatenate import cv2 import numpy as np We\u0026rsquo;ll be inheriting the base KerasPilot class for Donkey, the model will be a sequential one and among others it will use dense, 2D convolutional layers and the concatenation layer.\nWe\u0026rsquo;ll also need OpenCV and Numpy for our image preprocessing.\nWe\u0026rsquo;ll start by inheriting the base class and implementing the constructor and compile methods:\nclass OriModel(KerasPilot): \u0026#39;\u0026#39;\u0026#39; Custom model that takes an input image and feeds it and a preprocessed version of it to the model. The preprocessing converts the image to HSL color space, extracts the S channel and thresholds it. The thresholded S channel is passed to the model to help find lane lines easier. \u0026#39;\u0026#39;\u0026#39; def __init__(self, model=None, input_shape=(180, 320, 3), *args, **kwargs): super(OriModel, self).__init__(*args, **kwargs) self.model = oriModel(inputShape=input_shape) self.compile() def compile(self): self.model.compile(optimizer=self.optimizer, loss=\u0026#39;mse\u0026#39;) We\u0026rsquo;ll want to preprocess images at runtime, so we can use it during inference, so we\u0026rsquo;ll implement the run method accordingly:\ndef run(self, inputImage): # Preprocesses the input image for easier lane detection extractedLaneInput = self.processImage(inputImage) # Reshapes to (1, height, width, channels) extractedLaneInput = extractedLaneInput.reshape((1,) + extractedLaneInput.shape) inputImage = inputImage.reshape((1,) + inputImage.shape) # Predicts the output steering and throttle steering, throttle = self.model.predict([inputImage, extractedLaneInput]) print(\u0026#34;Throttle: %f, Steering: %f\u0026#34; % (throttle[0][0], steering[0][0])) return steering[0][0], throttle[0][0] We\u0026rsquo;ll use the code we wrote in the previous chapter and unify it in a couple helper methods:\ndef warpImage(self, image): # Define the region of the image we\u0026#39;re interested in transforming regionOfInterest = np.float32( [[0, 180], # Bottom left [112.5, 87.5], # Top left [200, 87.5], # Top right [307.5, 180]]) # Bottom right # Define the destination coordinates for the perspective transform newPerspective = np.float32( [[80, 180], # Bottom left [80, 0.25], # Top left [230, 0.25], # Top right [230, 180]]) # Bottom right # Compute the matrix that transforms the perspective transformMatrix = cv2.getPerspectiveTransform(regionOfInterest, newPerspective) # Warp the perspective - image.shape[:2] takes the height, width, [::-1] inverses it to width, height warpedImage = cv2.warpPerspective(image, transformMatrix, image.shape[:2][::-1], flags=cv2.INTER_LINEAR) return warpedImage def extractLaneLinesFromSChannel(self, warpedImage): # Convert to HSL hslImage = cv2.cvtColor(warpedImage, cv2.COLOR_BGR2HLS) # Split the image into three variables by the channels hChannel, lChannel, sChannel = cv2.split(hslImage) # Threshold the S channel image to select only the lines lowerThreshold = 65 higherThreshold = 255 # Threshold the image, keeping only the pixels/values that are between lower and higher threshold returnValue, binaryThresholdedImage = cv2.threshold(sChannel,lowerThreshold,higherThreshold,cv2.THRESH_BINARY) # Since this is a binary image, we\u0026#39;ll convert it to a 3-channel image so OpenCV can use it thresholdedImage = cv2.cvtColor(binaryThresholdedImage, cv2.COLOR_GRAY2RGB) return thresholdedImage def processImage(self, image): warpedImage = self.warpImage(image) # We\u0026#39;ll normalize it just to make sure it\u0026#39;s between 0-255 before thresholding warpedImage = cv2.normalize(warpedImage,None,0,255,cv2.NORM_MINMAX,cv2.CV_8U) thresholdedImage = self.extractLaneLinesFromSChannel(warpedImage) one_byte_scale = 1.0 / 255.0 # To make sure it\u0026#39;s between 0-1 for the model return np.array(thresholdedImage).astype(np.float32) * one_byte_scale And now to the architecture/model itself. First, let\u0026rsquo;s define the inputs and the dropout rate:\ndef oriModel(inputShape, numberOfBehaviourInputs): # Dropout rate keep_prob = 0.9 rate = 1 - keep_prob # Input layers imageInput = Input(shape=inputShape, name=\u0026#39;imageInput\u0026#39;) laneInput = Input(shape=inputShape, name=\u0026#39;laneInput\u0026#39;) behaviourInput = Input(shape=(numberOfBehaviourInputs,), name=\u0026#34;behaviourInput\u0026#34;) Now let\u0026rsquo;s define the upper CNN, which takes in the raw image as its input:\n# Input image convnet x = imageInput x = Conv2D(24, (5,5), strides=(2,2), name=\u0026#34;Conv2D_imageInput_1\u0026#34;)(x) x = LeakyReLU()(x) x = Dropout(rate)(x) x = Conv2D(32, (5,5), strides=(2,2), name=\u0026#34;Conv2D_imageInput_2\u0026#34;)(x) x = LeakyReLU()(x) x = Dropout(rate)(x) x = Conv2D(64, (5,5), strides=(2,2), name=\u0026#34;Conv2D_imageInput_3\u0026#34;)(x) x = LeakyReLU()(x) x = Dropout(rate)(x) x = Conv2D(64, (3,3), strides=(1,1), name=\u0026#34;Conv2D_imageInput_4\u0026#34;)(x) x = LeakyReLU()(x) x = Dropout(rate)(x) x = Conv2D(64, (3,3), strides=(1,1), name=\u0026#34;Conv2D_imageInput_5\u0026#34;)(x) x = LeakyReLU()(x) x = Dropout(rate)(x) x = Flatten(name=\u0026#34;flattenedx\u0026#34;)(x) x = Dense(100)(x) x = Dropout(rate)(x) I\u0026rsquo;ll explain why I\u0026rsquo;m using LeakyReLU in a moment. Let\u0026rsquo;s define the bottom CNN which takes in the thresholded lane image as its input:\n# Preprocessed lane image input convnet y = laneInput y = Conv2D(24, (5,5), strides=(2,2), name=\u0026#34;Conv2D_laneInput_1\u0026#34;)(y) y = LeakyReLU()(y) y = Dropout(rate)(y) y = Conv2D(32, (5,5), strides=(2,2), name=\u0026#34;Conv2D_laneInput_2\u0026#34;)(y) y = LeakyReLU()(y) y = Dropout(rate)(y) y = Conv2D(64, (5,5), strides=(2,2), name=\u0026#34;Conv2D_laneInput_3\u0026#34;)(y) y = LeakyReLU()(y) y = Dropout(rate)(y) y = Conv2D(64, (3,3), strides=(1,1), name=\u0026#34;Conv2D_laneInput_4\u0026#34;)(y) y = LeakyReLU()(y) y = Dropout(rate)(y) y = Conv2D(64, (3,3), strides=(1,1), name=\u0026#34;Conv2D_laneInput_5\u0026#34;)(y) y = LeakyReLU()(y) y = Flatten(name=\u0026#34;flattenedy\u0026#34;)(y) y = Dense(100)(y) y = Dropout(rate)(y) Now we have to concatenate the two networks and feed them into the last three dense layers:\n# Concatenated final convnet\rc = Concatenate(axis=1)([x, y])\rc = Dense(100, activation=\u0026#39;relu\u0026#39;)(c)\rc = Dense(50, activation=\u0026#39;relu\u0026#39;)(c) And finally, we\u0026rsquo;ll define the output and return the model:\n# Output layers steering_out = Dense(1, activation=\u0026#39;linear\u0026#39;, name=\u0026#39;steering_out\u0026#39;)(o) throttle_out = Dense(1, activation=\u0026#39;linear\u0026#39;, name=\u0026#39;throttle_out\u0026#39;)(o) model = Model(inputs=[imageInput, laneInput, behaviourInput], outputs=[steering_out, throttle_out]) return model Why LeakyReLU As every model, this one began as one thing and ended up a whole different thing in terms of hyperparameters and the architecture. I\u0026rsquo;ll explain that in much more detail in the next chapter. But one thing that happened to me while training the above model using ReLU as the activation function for the convolutional layers was dying ReLU(s).\nFirst, let\u0026rsquo;s remember how the ReLU (Rectified Linear Unit) function is defined: ++ f(x) \\begin{cases} x \u0026amp; \\text{when x=\u0026gt;0}\\\\ 0 \u0026amp; \\text{when x\u0026lt;0} \\end{cases} ++ This is what it looks like when plotted:\nYou can see that any negative input will result in a 0 activation for the ReLU function.\nNow imagine we get a large negative value that gets input to our unit that uses the ReLU activation function. It will cause the unit weights to update in a way that will prevent it to ever be activated again.\nThis is a known disadvantage of ReLU, and there are even papers written on this topic alone 1. Stanford\u0026rsquo;s course CS231n states that: you can find that as much as 40% of your network can be “dead” .\nIn my case, the neural network was just outputting the same throttle and steering values for every input. And since it had a lot of layers, I knew it should be learning at least something, and after removing the small (10%) dropout rate I\u0026rsquo;ve implemented, and after it still hasn\u0026rsquo;t changed, I realised that many of the convolutional units weren\u0026rsquo;t really outputting anything, which caused the rest of the network to just have the same output over and over again.\nThe fix was obviously to use either LeakyReLU or PReLU or any other activation function made specifically to overcome this advantage of ReLUs, while still preserving it\u0026rsquo;s linear non-saturating form.\nWhy not PReLU? Because I wanted to try something simple before having an additional parameter to train, which I\u0026rsquo;d get by using PReLUs, and after trying out LeakyReLU the model trained just fine, with the loss being close to 0.001 (using MSE as the loss function).\nHere\u0026rsquo;s the definition of LeakyReLU ++ f(x) \\begin{cases} x \u0026amp; \\text{when x=\u0026gt;0}\\\\ \\alpha \\cdot x \u0026amp; \\text{when x\u0026lt;0} \\end{cases} ++ The default alpha in Keras is 0.3. Here\u0026rsquo;s a plot of LeakyReLU with $\\alpha = 0.3$:\nAnd here\u0026rsquo;s a plot when $\\alpha = 0.03$:\nYou can choose whichever $\\alpha$ value you\u0026rsquo;d like. This will allow the unit to activate even if the value is negative.\nSpeaking of choosing $\\alpha$, the main idea behind PReLU is to make it a parameter which the network will learn. You can read about PReLUs in this paper they were first proposed by He et al. or you can take a look at the Keras implementation here.\nTraining the network I\u0026rsquo;ve trained the network using about 10k records made on the randomly generated track, and left the network to train for 12 epochs which took 21m 45s on my RTX 2060. The final validation loss was 0.003665.\nHere is a graph showing a plot of the training loss vs the validation loss:\nHere is the training loss plot with the two separate outputs plotted:\nHere is the validation loss plot with the two separate outputs plotted:\nhttps://arxiv.org/pdf/1903.06733.pdf\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n"
},
{
	"uri": "http://localhost:1313/rc-car/batteries/",
	"title": "Batteries",
	"tags": [],
	"description": "",
	"content": "\nThere are two types of batteries used for RC cars: Nickel Metal Hydride batteries (NiMH) and Lithium Polymer batteries (LiPo).\nTL;DR: LiPo batteries are much better, but also more expensive.\nLithium Polymer batteries generally have higher current capacity (the amount of Amps the battery can deliver at one point while driving) as well as energy storage (the number of Amp Hours the battery stores when fully charged) so it may also last longer.\nThe amount of charge a battery can hold (how long it runs) is measured in Ampere-hours. The amount of current a battery can instantaneously deliver while driving is measured simply in Amperes.\nAmperes are often re-calculated in terms of multiples of the energy content, divided by one hour which is often called \u0026ldquo;C.\u0026rdquo; Thus, a LiPo rated for 10C and 2000 mAh, can deliver 20 Amperes of current while driving. A NiHM rated for 5C and 1100 mAh can deliver 5.5 Amperes of current while driving.\nBatteries can deliver more than the C rating for very short amounts of time, but will heat up or build up internal resistance so you shouldn’t count on that as being their standard capability.\nFor your custom car, be aware of the voltages needed for the ESC and the motor of the car, and make sure to get a battery that matches them in voltage.\nSmaller RC cars will come with NiMH for affordability, or 2S LiPo for power. Larger RC cars will use 3S (11.1V) or 4S (14.8V) or even 6S (22.2V) Lithium batteries, and thus need to have ESC and motor combinations to match.\nBe sure to get a charger that matches your battery.\nIf you have a LiPo battery:\nget a good Lithium battery charger, with a balancing plug, never discharge a Lithium battery below 3.2V per cell If you discharge a LiPo battery completely, it won’t ever be charged up to its normal voltage again, and trying to do so will overheat the battery and set it on fire.\nTo prevent this, get a battery alarm and a LiPo charging bag:\n"
},
{
	"uri": "http://localhost:1313/hardware/inventory/",
	"title": "Hardware inventory",
	"tags": [],
	"description": "",
	"content": "Let’s start with a list of all of the hardware I’ll be using through this tutorial, and of course, with the money shot:\nRC Car kit The RC Car I went with was the Tamiya TT-02, which came prebuilt and even included an receiver and a remote, for just a bit over 100€. Very lucky!\nThe reason I went with this is that it was just such a good deal for such a car. It’s 1:10 scaled, so I’ll have plenty of real estate for hooking all sorts of electronics on it, and it’s a really good car for on road driving. There’s plenty of mods available for it and plenty of extra parts, which I even got with it, such as extra wheel bases or additional mounting arms. After working with it for a couple months now, I’d highly recommend it.\nBe sure to pay attention to the level of pre-assembly your kit comes with. There are kits that can take days to assemble if you’re doing all by yourself, and there are kits that are completely pre-built out of the box (commonly denoted RTR - ready to run). If you’re up for a challenge, get a Tamiya kit that isn’t prebuilt. They have great instructions and it’s fun to get to know your car inside out. The difficulty of assembly also varies by car, and is often denoted on the box or in the instructions.\nCharger and batteries I got a Carson Expert Charger LiPo Compact 3A set, with a Carson 3000mAh LiPo battery, for under 30€, which is also pretty sweet!\nI’d very much recommend getting a ‘smarter’ charger, with a balancing connector so you know exactly how much your battery is charged. You’ll want to store them at around 70% of their capacity if you’re not using them.\nI also went ahead and got an extra Fconegy 6000 mAh LiPo battery for around 25€.\nAnd of course, per my warning on the batteries section at the RC Car primer, I got a LiPo alarm and a charging bag to avoid any unnecessary explosions, housefires or general mayhem. It’s just an extra 20€ or so, but it’s very much so worth it.\n![](/images/hardware/lipo bag.jpg)\nJetson Nano Ah, the 🧠z of the project, the Jetson Nano Developer Kit. It’s pretty much the way to go for 100€ or so, it packs a pretty decent GPU, 4x USB 3.0 ports, a real gigabit ethernet port, and you can punch a bit more power into it with a jumper and a 4A@5V power source with a barrel jack cable.\nJetson Nano: MicroSD card You’ll be needing a microSD card to run your Jetson Nano OS, and you should try getting a faster on (at least Class 10, preferably UHS-II or faster :)).\nI got an Sandisk Extreme 64GB A2, UHS-3 card for about 25€.\nYou don’t have to get a huge (in terms of storage) microSD if you’re planning on using an external SSD with your Jetson. I’d say 32GB would be just enough, but the price difference isn’t that big, so you can go for the 64GB version just ‘cause.\nJetson Nano: Noctua Fan If you’re planning to run the Jetson at fully capacity (4A barrel jack mode), which you most definitely should, it’s gonna need some help stayin’ cool, so you should get a fan for it.\nYou can pretty much get any 5V, 4-pin PWM, 40x40mm size fan, but I’d very much recommend going for a Noctua NF-A4 fan. You can choose between the 10mm or 20mm version, just be sure to get the 5V version of the fan and not the regular 12V one.\nYou can get the 20mm version for about 10-15€.\nYou’ll also want a couple of self-tapping M3 screws or regular M2 or M2.5 screws with nuts for mounting the fan to the heatsink. I’d recommend going with the self-tapping M3 screws, it’s so much easier. Also watch out for some aluminum shavings when you mount the fan on it, you wouldn’t want them getting to the circuitry below and causing mayhem.\nAlso, if you’re going with the 20mm version, get 25mm long screws. If you’re going with the 10mm version, get 15mm long screws.\nJetson Nano: Wireless LAN and Bluetooth There are two options on getting your Nano (and your car) wirelessly connected:\nAn M.2 WiFi + BT card USB WiFi and BT adapters I’d highly recommend going for the M.2 option. I got the Intel 8265 card for about 10€.\nIf you’re getting the M.2 variant, you’ll need two antennas and an IPEX MHF4 to RP-SMA adapter cable. Pay attention to the connector types, it’s easy to get a wrong one, and vendors often misuse the labels, mixing IPEX, uFL, IPAX, IPX, MHF, and AM. Sometimes they’re right, sometimes they’re not.\nThe Jetson Nano has a M.2 key E slot, just to keep in mind when getting a card. It also doesn’t work with NVME SSDs, if you’re wondering.\nBeware of the model of the m.2 card you’re getting. At the time of writing, the Jetson Nano SD Card Image natively supports only the 8265. Technically, you should be able to use any Intel card older than the 8265 since it’s supported in the 4.9 kernel, but some people had issues running even the 7265. If you’re up to it, you can build the core14 branch on the backports git, but if you know how to do that, you probably don’t need me telling it about you.\nJetson Nano: Camera To enable the car to look around itself and navigate itself, you’ll need a camera. You can use pretty much any USB camera, or you can use a MIPI CSI camera with the Nano.\nThe cool thing about the Jetson Nano is that it’s compatible with almost all Raspberry Pi products, which includes camera.\nI went with the Raspberry Pi Camera v2, which uses the IMX219 sensor.\nI’d recommend getting an extra longer flat cable, so you don’t have to worry about the positioning of the camera relative to the Nano on the vehicle, since you only get a rather short one with the camera you buy.\nNow, about the almost all RPi products part mentioned above. If you’re buying a RPi Camera, make sure to get the newer version with the IMX219 sensor instead of the older (EOL) OV5647 sensor, since it’s not supported by the Jetson Nano image. Of course, you can build the drivers into the kernel yourself, but I’d go with the pragmatic solution on this one.\nI may also be using an Elephone EleCam Explorer Elite 4K cam I had lying around as an additional camera just for fun.\nPCA9685 Servo Driver To enable your Jetson Nano to interface with your steering servo and ESC, you’ll need a PWM/Servo Driver. I’ll be using the PCA9685 over I2C, which is pretty cheap and has lots of extra channels.\nYou might want to get some jumper cables if you don’t have any lying around, since you’ll have to connect this to the Jetson Nano’s GPIO pins somehow.\nJetson Nano: Power bank To power all of this for a reasonable amount of time, you’ll want to get a pretty good power bank. I went with the RAVPower 20100 mAh USB-C power bank with 45W PD for about 50€.\nYou can pretty much get any power bank you’d like, but I’d recommend getting something above 10Ah and more than 2A if you plan on using the barrel jack connection on the Nano.\nJetson Nano: (Optional) External SSD First of all, why:\nWith a microSD, on average you’ll get ~ 87 MB/s read speed with an access time of ~0.52 milliseconds. With a SSD, you’ll get an average read rate of ~367 MB/s and an access time of ~0.29 milliseconds. So go ahead and get an NVMe SSD and an external USB enclosure for it. I’ll be using a LITE-ON SSD I had lying around after replacing my Razer Blade Stealth OEM one with a Samsung 970 Evo.\nGetting an external NGFF/M.2 SSD enclosure can be a very much hit-or-miss business if you’re trying to save a couple of bucks. I got lucky and got an off-brand one working at very decent speeds for some 20€. Other people I know haven’t been that lucky on their first attempt, so beware when buying it, make sure there’s an option to return it if it doesn’t work. :)\nAlthough this gives you about 4x more speed with your Nano, it is fairly technical and complicated and includes manually patching and building a Linux kernel, so I can only assume that you’re very very interested, since it’s so much fun. You’re here to build a nothing less than a self-driving vehicle, what’s a little kernel hacking compared to it. :)\nJetson Nano: (Optional) Camera Lens Attachments If you’re getting the standard Raspberry Pi Camera, it won’t have much of a FOV by itself. One thing you can do, is get one of the clip-on attachment lenses for smartphones, which can greatly increase its FOV.\nI got something like this:\nThese lenses are what they are, cheap. With cheap lenses come all sorts of nasty stuff, like chromatic aberration and barrel distortion. Try to look at some of the customer reviews that contain photos and find one with a bit less distortion.\nThe Rest: Tools and stuff Of course, you’ll need some basic tools, such as:\nA good scalpel A cutting board A hand drill (depending on what you’re planning to use as your base) Screwdrivers with an assortment of M3 screws (various lengths, with nuts) But that’s pretty much it for the inventory. On to building it!\n"
},
{
	"uri": "http://localhost:1313/draft-and-todo/derivacije/",
	"title": "Raspisivanje math-a",
	"tags": [],
	"description": "",
	"content": "\rTHIS IS A DRAFT\nI\u0026rsquo;ll probably just delete this!\nNagib linije Recimo da imamo sljedeću funkciju: f(x) = x+1:\nŽelimo odrediti njezin nagib (slope). Nagib definiramo kao promjenu y u odnosu na promjenu x.\nNagib nam govori koliko brzo y raste ako promijenimo x.\nOdaberemo bilo koju točku (x1,y1) na grafu. Nakon toga x povećamo za proizvoljnu veličinu Δx, te pogledamo koliko iznosi promjena Δy. Tako dobivamo drugu točku, (x1+Δx, y1+Δy), odnosno (x2, y2). Nagib možemo izračunati tako što podijelimo promjenu na y osi sa promjenom na x osi.\n(x2, y2) = (x1 + Δx, y1+Δy)\nΔy / Δx = (y2 - y1) / (x2 - x1)\nPošto je ova funkcija linearna, vidimo da je nagib konstantan, kojegod dvije točke odabrali.\nKonkretno, uzmimo za primjer točku (0,1). Ako x koordinatu povećamo za 1 (Δx), y će iznositi 2 (Δy). x1 = 0, y1 = 1, x2 = 1, y2 = 2. Nagib ove funkcije, je stoga: Δy/Δx = (2 - 1) / (1 - 0) = 1. Vidimo da je promjena y u odnosu na x jednaka, odnosno funkcija raste točno onoliko koliko smo promijenili x, linearna je, rast je konstantan i iznosi 1.\nAko imamo nelinearnu funkciju, kao ispod:\nMožemo vidjeti da rast funkcije nije jednak u svim točkama. Primjerice, nagib (rast) na sljedeće dvije točke:\nNije jednak nagibu (rastu) na sljedeće dvije točke:\nNa gornje dvije točke, nagib je veći, odnosno rast funkcije je brži. Postavlja se pitanje, kakav je nagib funkcije u pojedinoj točki, budući da se slijeva nadesno mijenja, odnosno povećava, rast funkcije raste.\nBudući da se nagib funkcije konstantno mijenja, iz točke u točku, odnosno raste, konkretan nagib na jednoj točki možemo odrediti pomoću nagiba tangente koja dodiruje graf funkcije u točno toj točki:\nNagib smo na linearnoj funkciji odredili tako što smo x pomakli za neku vrijednost Δx, izmjerili pomak Δy, te izračunali njihov omjer, odnosno razliku između početne točke i novo dobivene točke:\nAko na ovoj, nelinearnoj funkciji, zamislimo da je Δx jako, jako malen (malo delta: δx=x+ε, epsilon, infinitesimal), te izmjerimo isto malen pomak Δy (malo delta δy), dobit ćemo nagib funkcije u toj točki:\nδy/δx - umjesto δ se često piše d, kao derivative (Leibnitzova notacija).\nAko je δx, jako mal (teži u nulu), dobijamo derivaciju (rast) funkcije u toj točki:\nDerivacije Uzmimo za primjer f(x) = x²:\nZnamo da za x=2, f(x) = x², f(2) iznosi 4. Ako x pomaknemo udesno za jako malen broj, 0.00000772, f(x) će se promijeniti na 4.00003086. Stoga:\nδx = 0.00000772\nδy = 0.00003086\nδy/δx = 2.0000077 ~= 2 (zbog zaokruživanja)\nStoga možemo reći da je derivacija δy/δx=f\u0026rsquo;(x) = 2x:\nŠto znači da u točki 2, derivacija f\u0026rsquo;(x) iznosi 2*x=4, odnosno y os će se promijeniti za dvostruku vrijednost promjene na x osi.\nAko se vratimo na prvu opisanu (linearnu)funkciju f(x) = x+1, vidjeli smo da je njezin rast konstantan, odnosno jedan, a derivacija iste te funkcije iznosi f\u0026rsquo;(x) = 1.\nParcijalne derivacije Ako je funkcija multivariate, odnosno ovisi o više varijabli, primjerice: f(x,y) = -x² - y²\nKada deriviramo z = f(x,y) po jednoj od varijabli, drugu tretiramo kao konstantu, pa dobivamo sliku:\nAko napravimo derivaciju po drugoj osi (y-osi), dobivamo rast funkcije u točki (x,y) za tu os.\nSvaka parcijalna derivacija je skalarna vrijednost, koja nam govori nagib tangente nad grafom funkcije u toj točki.\nAko sve parcijalne derivacije kombiniramo, dobivamo vektor [∂f/∂x, ∂f/∂y] (generalizira se na n-dimenzija) koji nam govori u kojem smjeru graf najbrže raste.\nDrugi naziv za vektor [∂f/∂x, ∂f/∂y] je gradijent funkcije f.\nGradijent funkcije Uzmemo li contour plot za gore prikazani konveksni paraboloid i neku točku (x,y,f(x,y)), te izračunamo njezin gradijent, dobit ćemo vektor [∂f/∂x, ∂f/y] koji pokazuje smjer najbržeg rasta funkcije, najstrmiji nagib orijentiran prema rastu.\nAko taj vektor pomnožimo sa minus 1, dobijamo suprotan vektor [-∂f/∂x, -∂f/∂y], koji pokazuje najbrži smjer pada funkcije:\nNote: vektori predstavljaju klase ekvivalencije, te je vektor sa gornje slike jednak vektorima sa sljedeće slike:\nSvaki vektor predstavlja smjer u kojemu se trebamo kretati iz točke u kojoj smo računali gradijent, kako bi najbrže došli do najbližeg lokalnog minimuma f-je, koji je na grafu ujedno i globalni minimum, pošto je paraboloid konveksan.\nWhy First things first, kako uopće dobit smjer gradijenta?\nUzmimo za primjer parcijalnu derivaciju po x, formula je: ∂f/∂x. Ova derivacija nam govori kako će se promijeniti f, ako x pomaknemo za neku malu vrijednost ∂x, govori nam rast/pad funkcije. Isto vrijedi i za ∂f/∂y, ako malo promijenimo y, za ∂y, govori nam za koliko će se promijeniti f.\nPrimjer koji možemo zamisliti da je gradijent [∂f/∂x, ∂f/y] = [2x, 3y-1]. Ako su ∂x i ∂y jako mali brojevi, blizu su nuli, pa unesemo (0,0) i dobivamo vektor [0, -1]. To je smjer gradijenta i ujedno i najbrži smjer rasta funkcije f.\nDa smo ubacili nekakav random vektor, primjerice (-1, -1), dobili bi vektor [-2, 4]:\nOva dva vektora su skroz različita, gdje prvi pokazuje smjer najbržeg rasta, odnosno smjer gradijenta (usmjerenih derivacija), a drugi nekakav skroz random smjer. Ako želimo ići u smjeru gradijenta, samo uzmemo njegovu normu, odnosno zamislimo da smo uvrstili.\nIsto tako, zamislimo da krećemo iz točke (x,y,f(x,y)), ako vrijednosti x i y promijenimo za ∂x i ∂y, doći ćemo u novu točku (crvena sa bijelim). Ako povežemo početnu točku sa novom točkom (x+∂x, y+∂y, f(x+∂x,y+∂y)), dobivamo vektor koji pokazuje smjer u kojemu se nalazi maksimum.\nSad zamislimo da imamo vektor a, smjer gradijenta, koji pokazuje točno u smjeru najbržeg rasta funkcije f. Ako uzmemo nekakav proizvoljan unit vektor b, koji pokazuje u random smjeru, zanima nas koliko smo \u0026ldquo;pogodili\u0026rdquo; smjer najbržeg rasta.\nDot produkt dva vektora nam u biti govori koliko su ta dva vektora slična, ako primjerice precrtamo a na b. Pitanje je kako odabrati vektor b koji je najsličniji vektoru a, koji pokazuje točno u smjeru rasta.\nNajsličniji vektor, vektoru a, je sam taj vektor, jer je dot produkt dva ista vektora jednak 1, odnosno 100% su slični/isti.\nChain derivative rule Ako imamo kompoziciju funkcija:\n++\rf(x)=A(B(C(x)))\r++\rI želimo lako izračunati derivaciju f'(x), chain rule nam kaže da je ona jednaka:\r++\rf′(x)=f′(A)⋅A′(B)⋅B′(C)⋅C′(x)\r++\rOvo vrijedi i za bilo koju drugu derivaciju, primjerice f'(B):\r++\r\\begin{align}\rf′(B)\u0026=f′(A)⋅A′(B)\r\\\\B(C(x)) \u0026= const\r\\end{align}\r++\rNaravno, B(C(x)) smatramo konstantom, pošto je ovo parcijalna derivacija f u odnosu na B.\rKonkretniji primjer Uzmimo za primjer sljedeće funkcije:\n++\r\\begin{align}\rf(x,y) = x^2*y\\\\\rx(t) = cos(t)\\\\\ry(t) = sin(t)\r\\end{align}\r++\rZamislimo da u funkciju f(x,y) kao parametre x i y želimo uvrstiti vrijednost funkcija x(t), y(t):\r++\r\\begin{align}\rKompozicija: \u0026f ∘ g = f(g(x))\r\\\\ \u0026f(x(t), y(t)) = (x(t))^2 \\cdot y(t)\r\\end{align}\r++\rPitanje je, kako možemo dobiti derivaciju funkcije f u odnosu na t:\r++\r\\frac {d}{dt}f(x(t),y(t))\r++\rJedan način je da jednostavno uvrstimo vrijednosti x(t) i y(t) i deriviramo ih:\r++\r\\begin{align}\rf(cos(t),sin(t))\u0026= (cos(t))^2\\cdot sin(t)\r\\\\\\frac{d}{dt}f(cos(t),sin(t)) \u0026= (cos(t)^2)'\\cdot sin(t)+sin(t)'\\cdot (cos(t))^2\r\\\\\\ \u0026= cos^2(t)cos(t)+2cos(t)sin(t)(-sin(t))\r\\end{align}\r++\rMeđutim, postoji i drugi način. Prvo deriviramo sve funkcije posebno. Za f(x,y) napravimo obje parcijalne derivacije:\n++\r\\begin{align}\rf(x,y) \u0026= x^2\\cdot y\\\\\r\\frac{\\partial f}{\\partial x}f(x,y) \u0026= 2xy\\\\\r\\frac{\\partial f}{\\partial y}f(x,y) \u0026= x^2\r\\end{align}\r++\rZa x(t) i y(t) napravimo obične derivacije:\r++\r\\begin{align}\rx(t) \u0026= cos(t)\\\\\r\\frac{d}{dt} x(t)\u0026=-sin(t)\\\\\r\\end{align}\r++\r++\r\\begin{align}\ry(t) = sin(t)\\\\\r\\frac{d}{dt}y(t)=cos(t)\r\\end{align}\r++\rChain derivative rule nam govori da možemo dobiti derivaciju funkcije f u odnosu na varijablu t:\n++\r\\frac{d}{dt}f(x(t),y(t))=\\frac{\\partial f}{\\partial x} \\cdot \\frac{dx}{dt} + \\frac{\\partial f}{\\partial y} \\cdot \\frac{dy}{dt}\r++\rAko imamo kompoziciju:\r++\rf ∘ g = f(g(x))\r++\rOnda je derivacija funkcije f, po varijabli x jednaka:\r++\r\\begin{align}\r(f∘g)'\u0026=(f'∘g)\\cdot g'\\\\\r(f∘g)'\u0026=f'(g(x))\\cdot g'(x)\\\\\r\\end{align}\r++\rNaša kompozicija je multivarijabilna funkcija:\r++\r(f ∘ (x,y)) = f(x(t),y(t))\r++\rNote: no idea piše li se actually tako kompozicija multivariate funkcija, pošto ih nema u literaturi.\rDerivacija te naše kompozicije, po chain ruleu je onda jednaka:\n++\r\\begin{align}\r\\frac{\\partial f}{\\partial x}f(x(t),y(t))\\cdot\\frac{dx}{dt}x(t)+\\frac{\\partial f}{\\partial y}f(x(t),y(t))\\cdot\\frac{dy}{dt}y(t)\r\\end{align}\r++\rIli ljepše napisano:\r++\r\\begin{align}\r\\frac{d}{dt}f(x(t),y(t))=\\frac{\\partial f}{\\partial x} \\cdot \\frac{dx}{dt} + \\frac{\\partial f}{\\partial y} \\cdot \\frac{dy}{dt}\r\\end{align}\r++\rAko to raspišemo i unutar parcijalnih derivacija uvrstimo vrijednost x, za lijevu stranu zbrajanja dobijemo:\r++\r\\begin{align}\r\\frac{\\partial f}{\\partial x} \u0026= 2xy = 2cos(t)sin(t)\\\\\r\\frac{dx}{dt} \u0026=-sin(t)\\\\ \\frac{\\partial f}{\\partial x} \\cdot \\frac{dx}{dt} \u0026= 2cos(t)sin(t)(-sin(t))\r\\end{align}\r++\rAko raspišemo desnu stranu zbrajanja, dobijemo:\r++\r\\begin{align}\r\\frac{\\partial f}{\\partial y} \u0026= x^2 = (cos(t))^2 = cos^2(t)\r\\\\\\frac{d}{dt}\u0026=cos(t)\r\\\\\\frac{\\partial f}{\\partial y} \\cdot \\frac{dy}{dt}\u0026 = cos^2(t)cos(t)\r\\end{align}\r++\rNa kraju, dobijemo istu jednadžbu kao što smo dobili i ručnom derivacijom:\r++\r\\frac{\\partial f}{\\partial x} \\cdot \\frac{dx}{dt} + \\frac{\\partial f}{\\partial y} \\cdot \\frac{dy}{dt} = cos^2(t)cos(t)+2cos(t)sin(t)(-sin(t))\r++\rChain rule u backpropagationu Recimo da imamo single hidden layer neural net:\nLoss funkciju definiramo kao:\n++\r\\ell(w,b) = [(w^Tx+b)-y]^2\r++\rOstali parametri su:\r++\r\\begin{align}\r(prediction): \\hat{y}\u0026 = \\sum_{j=1}^dw_jx_j+b\r\\\\(residual):r\u0026=y-\\hat{y}\r\\\\ (loss):l\u0026=r^2\r\\end{align}\r++\rZanima nas kako se mijenja output $\\ell$, ako mijenjamo parametre $w, b$. Odgovor na to nam daje parcijalna derivacija $\\ell$ po tim parametrima:\r++\r\\begin{align}\r\\\\\\frac{\\partial l}{\\partial r} \u0026= 2r\r\\\\\\frac{\\partial l}{\\partial \\hat y} \u0026= \\frac{\\partial l}{\\partial r} \\cdot \\frac{\\partial r}{\\partial \\hat y} = 2r \\cdot -1 = -2r\r\\\\\\frac{\\partial \\ell}{\\partial w} \u0026= \\frac{\\partial \\ell}{\\partial \\hat y} \\cdot \\frac{\\partial \\hat y}{\\partial w} = -2r \\cdot x_j\r\\end{align}\r++\rZamislimo da imamo sljedeći neural net:\nDefiniramo cost funkciju za ovaj net kao:\n++\rCost=C(R(Z(XW)))\r++\rGdje su funkcije i njihovi parametri definirani kao:\rPomoću chain rulea, možemo lako odrediti koliko točno koji weight utječe na grešku koju naš neural net radi. Kad parcijalno deriviramo cost function po jednom od parametara, sve ostale tretiramo kao konstante, pa možemo odrediti kako mijenjanje određenog parametar (dimenzija) utječe na rast i pad naše funkcije (cost funkcije).\n++\r\\begin{align}\rCost\u0026=C(R(Z(XW)))\r\\\\\r\\\\C′(W_o)\u0026=C′(\\hat y)⋅\\hat y′(Z_o)⋅Z′_o(W_o)\r\\\\\u0026=(\\hat y−y)⋅R′(Z_o)⋅H\r\\end{align}\r++\rPrimjenom chain rulea možemo odrediti točan cost primjerice weighta od hidden layera prema outputu. Isto tako, rekurzivno, možemo odrediti i derivaciju cost funkcije u odnosu na hidden layer cost:\r++\r\\begin{align}\r\\\\C′(W_h)\u0026=C′(y^)⋅O′(Zo)⋅Z′o(H)⋅H′(Z_h)⋅Z′h(W_h)\r\\\\\u0026=(y^−y)⋅R′(Z_o)⋅W_o⋅R′(Z_h)⋅X\r\\end{align}\r++\rŠto više layera unatrag idemo, broj derivacija koje trebamo izračunati se povećava, pa bi tako za izračun prvog weighta u mreži sa 10 hidden layera morali napraviti čak 23 derivacije:\rTakođer, kad računamo derivaciju za n-ti sloj, koristimo sve derivacije koje smo koristili za sloj n+1, samo dodajući par potrebnih derivacija za sloj iznad. Puno posla se ponavlja:\nKako to olakšat?\nLayer error označava derivaciju cost funkcije u odnosu na input pojedinog layera. Odgovara na pitanje: kako se rezultat cost funkcije mijenja kad malo promijenimo input u tom layeru.\nPrvo izračunamo output layer error, i taj rezultat proslijedimo hidden layeru prije njega.\nNakon što izračunamo layer error tog hidden layera, vratimo njegovu vrijednost hidden layeru prije njega, itd, sve dok ne dođemo do input layera.\nSlika za lakši reference:\nOutput layer error Zanima nas derivacija cost funkcije u odnosu na ulaz (input) output layera: $Z_o$.\nTa derivacija nam govori kako promjena weightova između zadnjeg hidden layera i output layera utječe na rezultat cost funkcije:\n++\r\\begin{align}\r\\\\\\frac{\\partial Cost}{\\partial Z_o} \u0026= \\frac{\\partial l}{\\partial r} \\cdot \\frac{\\partial r}{\\partial \\hat y} = 2r \\cdot -1 = -2r\r\\\\C′(Z_o)\u0026=(\\hat y−y)⋅R′(Z_o)\r\\end{align}\r++\rLjudi često čitav ovaj izraz zamijene sa $E_o$, kao error output layera. Isto se odnosi na bilo koji hidden layer, npr $E_1$ označava error prvog hidden layera.\r++\rE_o = (\\hat y−y)⋅R′(Z_o)\r++\rHidden layer error Zanima nas derivacija cost funkcije u odnosu na input koji hidden layer prima:\n++\rC′(Z_h)=(\\hat y−y)⋅R′(Z_o) \\cdot W_o \\cdot R′(Z_h)\r++\rAko uvrstimo output layer error ($E_o$) u formulu, dobijamo:\r++\rC'(Z_h) = E_o\\cdot W_o \\cdot R′(Z_h)\r++\rOdnosno:\r++\rE_h = E_o\\cdot W_o \\cdot R′(Z_h)\r++\r"
},
{
	"uri": "http://localhost:1313/software/kernel-hacking/",
	"title": "Running the OS from an external SSD using a custom kernel",
	"tags": [],
	"description": "",
	"content": "This is the technical, optional upgrade that will enable you to boot your OS from an external SSD.\nSyonyk has an awesome Jetson Nano guide with all of this stuff explained and was the primary source of info while researching how to do this.\nJetsonHacks also have a guide that should make this much easier to do, as they’ve prepared scripts that do all of the work for you, but I haven’t tried it so I can’t say it works for sure, but it should!\nThis will take some time. Taking kernels apart, patching them and building them takes a while even on your regular workstations or servers, but doing it on an embedded device is a whole new world of pain when it comes to waiting for stuff to build, compile or extract. Be prepared to be patient while you’re doing this.\nSo what is it we’re exactly planning to do here, and why? We want to use our external SSD as the root filesystem because of the huge performance boost it gives us. But the problem is this:\nThe USB 3 ports require the kernel to load some firmware on boot to enable them to work, which means the USB ports won’t work until the device boots up That firmware is stored on the root filesystem, which we want to, you know, put on our external SSD That means we’d need the firmware from our SSD in order to use our SSD… So, how can we work this out? Simple: patch the kernel and embed the firmware we need right into it, so it doesn’t need to read it from the root filesystem, which we can then freely put on our external SSD.\nWhile we’re at it, there’s another thing that would give us a noticeable performance boost which we could build into the kernel, same-filled page merging support for zswap.\nOkay, hack all the kernels All of the steps written below are meant to be run directly on your Jetson Nano.\nFirst of all, since the Nano doesn’t run the stock Linux kernel, we’ll need the sources for its custom kernel. Miraculously Nvidia actually provides it.\nDownload the BSP Sources from the right side, under Jetson Nano on the L4T download page. Unpack the archive Unpack the kernel source from the public sources folder to your home directory Open the kernel folder Steps shown below:\ncd Downloads tar -xf public_sources.tbz2 cd ~ tar -xf ~/Downloads/public_sources/kernel_src.tbz2 cd ~/kernel/kernel-4.9 Once we’re in the kernel directory, we’ll save the current kernel configuration from the Nano to it:\nzcat /proc/config.gz \u0026gt; .config Now we need to copy the USB firmware from the root filesystem into the kernel directory, since we want to pack it together with the kernel:\ncp /lib/firmware/tegra21x_xusb_firmware ./firmware/ Now we need to change the kernel config, and since we’re using a GUI desktop environment on the Nano, we can just use menuconfig:\nsudo apt-get install libncurses5-dev make menuconfig You should see something like this:\nWhen selecting kernel features in menuconfig, * denotes a built-in feature and M denotes a module. You’ll be wanting stars for the features you want to build in.\nGo to Kernel features: Select: Enable frontswap to cache swap pages if tmem is present Select: Compressed cache for swap pages (EXPERIMENTAL) (NEW) Select: Low (Up to 2x) density storage for compressed pages We didn’t select the 3x allocator (z3fold) because isn\u0026rsquo;t reliable prior to mid-4.20 kernel versions. With same-filled page merging, the results are about the same as the zbud 2x allocator.\nExit Go to Device Drivers Generic Driver Options External firmware blobs to build into the kernel binary Type: “tegra21x_xusb_firmware” Exit Save the new configuration Now we’ll update ZSwap to enable same-filled page compression. The kernel that the Jetson Nano is running at the time of writing is 4.9, which doesn’t include it, but it can easily be backported from a newer kernel.\nOne of the best things about Linux, what makes it what it is, is that it’s available for everyone, and we can find it on Linus Torvalds’ GitHub:\nGo to the Linux kernel source on GitHub Go to mm/zswap.c Take a look at the commit history by clicking on the History button Find the zswap: same-filled pages handling commit On GitHub, you can add .patch at the end of the URL to get the patch file for the commit you’re looking at. Just be sure to remove the #diff parameter from the URL if you’re looking at the diff from the commit.\nDownload the patch file for the same-filled pages commit Patch the kernel using patch -p1 and the downloaded patch There is a memset_l call in the patch we’ve just applied, and it doesn’t exist yet in the 4.9 kernel, so we’ll need to replace it with the regular memset. See instructions below:\nMake sure you’re in the kernel-4.9 directory before proceeding.\n# You should be in the Downloads directory wget https://github.com/torvalds/linux/commit/a85f878b443f8d2b91ba76f09da21ac0af22e07f.patch # Change to the kernel directory cd ~/kernel/kernel-4.9 patch -p1 \u0026lt; ~/Downloads/a85f878b443f8d2b91ba76f09da21ac0af22e07f.patch # Replace the nonexistent memset_l call with the regular memset sed -i \u0026#39;s/memset_l(page, value, PAGE_SIZE \\/ sizeof(unsigned long));/memset(page, value, PAGE_SIZE);/g\u0026#39; mm/zswap.c Building and installing the kernel Now that we’ve made all of the changes we wanted to the kernel, we need to build it and place it on the /boot partition.\nThis will take a while.\n# Make sure we\u0026#39;re in the kernel directory cd ~/kernel/kernel-4.9 make -j5 # -j denotes the number of threads sudo make modules_install sudo cp /boot/Image /boot/Image.dist sudo cp arch/arm64/boot/Image /boot After this is done, you can reboot. If the Nano boots successfully, it means that you’re running your new custom kernel. You can run uname -r to check it:\n# The stock kernel returns \u0026#39;4.9.140-tegra\u0026#39; # Your custom kernel should return only \u0026#39;4.9.140\u0026#39; uname -r If all is well, we can transfer our root partition to the external SSD.\nThis will also take a while when you get to the copying of the root filesystem to the SSD.\nPlug the SSD in Wipe the partition table Create a GPT partition table Create a new EXT4 volume 4 gigabytes smaller than the SSD Create a 4 gigabytes swap partition See steps below:\n# Wipe the partition table sudo dd if=/dev/zero of=/dev/sda bs=1M count=1 # Create a GPT partition table, then create a new EXT4 volume # Create a Linux swap partition (4GB) - arrow over to \u0026#34;Type\u0026#34; and select \u0026#34;Linux swap\u0026#34; # Go over to \u0026#34;Write\u0026#34; and type \u0026#34;yes\u0026#34; and then quit sudo cfdisk /dev/sda # Make an ext4 volume and a swap partition sudo mkfs.ext4 /dev/sda1 sudo mkswap /dev/sda2 # Mount the partition and copy the root filesystem to it sudo mkdir /mnt/root sudo mount /dev/sda1 /mnt/root sudo mkdir /mnt/root/proc sudo apt -y install rsync sudo rsync -axHAWX --numeric-ids --info=progress2 --exclude=/proc / /mnt/root Edit /boot/extlinux/extlinux.conf so that the kernel points at /dev/sda1 instead of /dev/mmcblk0p1 (the microSD) Enable zswap in extlinux.conf sudo sed -i \u0026#39;s/mmcblk0p1/sda1/\u0026#39; /boot/extlinux/extlinux.conf sudo sed -i \u0026#39;s/rootwait/rootwait zswap.enabled=1/\u0026#39; /boot/extlinux/extlinux.conf Reboot, and you should be running from the USB SSD.\nIf you mess something up and the Nano doesn’t boot, don’t worry, you can always plug the microSD into a Linux PC and mount it, go to the boot partition and open the extlinux/extlinux.conf file and replace sda1 with mmcblk0p1 so it boots from the microSD.\nIf you somehow end up messing up the extlinux.conf file, I’ve provided a copy for you down below, so you can overwrite it as stated in the tip above and boot back to the microSD:\nTIMEOUT 30 DEFAULT primary MENU TITLE p3450-porg eMMC boot options LABEL primary MENU LABEL primary kernel LINUX /boot/Image INITRD /boot/initrd APPEND ${cbootargs} rootfstype=ext4 root=/dev/sda1 rw rootwait Tweaking swap One last thing to do after you boot from your SSD, enable swap:\necho \u0026#34;/dev/sda2 none swap \\ defaults 0 1\u0026#34; | sudo tee -a /etc/fstab And you’re done. Congrats on hacking the kernel and running your Nano from an external SSD. It’ll be worth it!\n"
},
{
	"uri": "http://localhost:1313/hardware/building-the-car/",
	"title": "Assembling the RC Car",
	"tags": [],
	"description": "",
	"content": "Now to the fun part: building the RC car. This will vary very much based on your RC Car kit. If it’s ready-to-run (RTR), you should be good out of the box. If it’s an unassembled kit, you’ve got a ton of work ahead, just follow the instructions that came with the car. If you’re like me, and got something in between, you’ll have just a bit of work before running it.\nIn my case, I had to assemble the wheels and mount them on the car, and that was pretty much it. Other than that, I had to calibrate my ESC as per the instructions and my car was ready to go.\nI found it helpful to have a ‘bro nearby that’s willing to help you assemble and Google stuff for you since you’re lazy to read the instructions completely. YMMV based on the brother that’s available to you.\nPics or it didn’t happen Test drive After assembling the car, take it for a spin. If it works, you’re ready to move on to the electronics part of the project. :)\n"
},
{
	"uri": "http://localhost:1313/software/donkeycar/",
	"title": "DonkeyCar",
	"tags": [],
	"description": "",
	"content": " Donkey is a high level self driving library written in Python. It was developed with a focus on enabling fast experimentation and easy contribution.\nSource: Official Donkey docs\nWe\u0026rsquo;ll be using Donkey® as an interface between our RC car and the neural net we\u0026rsquo;d like to drive it for us.\nAs you can see above, we\u0026rsquo;d like to send the camera data from our RC to a model which would analyse it and tell the RC where to steer and how fast to go, in order to stay on the road.\nDonkey will provide us with an interface to do just so, without having to worry about the details of:\n(Pre) Processing the camera data Converting the steering/throttle values to actual signals for the RC to understand Actually steering the RC and increasing/decreasing throttle It will also enable us to:\nCollect and label training data Define and train custom models Control the car using a Web interface or a gamepad And even use a simulator to rapidly test and train our models How to get it up and running There are two ways to go about this:\nDoing everything on your RC car/Jetson Nano, including model training Training and developing the model on a host PC, running and testing it on your RC I\u0026rsquo;d very much recommend going with the second option, since you\u0026rsquo;ll need the extra horsepower of a PC in order to be able to train and develop complex models, preferably with a GPU. But, if you want, you can disregard my advice and skip installing the Donkey to your host PC and just install it on your RC and do everything from there.\nWe\u0026rsquo;ll continue our project by explaining how to:\nInstall Donkey on your host PC Install a simulator on your host PC Install Donkey on your RC Calibrate your RC and actually control it using Donkey, via the Web interface or a gamepad Do a sanity check by training a very simple model to see if everything works as it should Further Donkey resources Before we get to work, here are some links to Donkey resources you can check out to get familiar with it, and maybe better understand how it works:\nThe official Donkey documentation and the Slack channel/Discourse group I highly recommend going through these two. The docs are great and the Slack channel has a bunch of people trying to do the same thing you are, who love helping one another solve issues and bounce ideas. I love the Donkey community and can\u0026rsquo;t recommend enough going over to the channel and just saying hi.\nOne more thing; The Slack channel for the Donkey community is getting too big to be usable with the free plan, so you can only see messages going back a couple of months. That\u0026rsquo;s why they\u0026rsquo;re trying to migrate to the new Discourse group, so I\u0026rsquo;d recommend going there instead of the Slack channel.\nThe following videos are also an interesting watch, albeit not necessary to continue with the tutorial. One of the reasons why we\u0026rsquo;re using Donkey is so we don\u0026rsquo;t have to worry about all of the details it solves for us. But if you\u0026rsquo;re like me, you\u0026rsquo;re gonna wanna know a bit how it works before using it to piece your RC together. :)\nCircuitBread\u0026rsquo;s overview of the Donkey platform with Ed Murphy DonkeyCar\u0026rsquo;s founder, Adam Conway\u0026rsquo;s video of assembling a DonkeyCar Make magazine\u0026rsquo;s video of building a Raspberry Pi DonkeyCar Tawn Kramer\u0026rsquo;s two part overview of the DonkeyCar framework William Roscoe\u0026rsquo;s quick get started video) "
},
{
	"uri": "http://localhost:1313/artificial-intelligence/how-to-train-your-model/",
	"title": "Master recipe: How to learn your machines",
	"tags": [],
	"description": "",
	"content": "These are notes I made while I was taking the Deeplearning.ai Deep Learning Specialization, more specifically, the awesome Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization course, through which Andrew Ng lays down a basic recipe for training your machines! I highly recommend you taking it, it\u0026rsquo;s free and you\u0026rsquo;ll absolutely learn something even if you\u0026rsquo;re an experienced ML practicioner.\nThis may very well be the most useful part of my project. Be sure to at least take a look. 🙂\nPreparing your data One of the first questions while preparing you should answer is how to split your dataset? One good practice is to split the data into three parts;\nTraining set, cross validation/dev set and test set. You train the data on the training set, and validate it using the dev set. Since you\u0026rsquo;re using the dev set to tune the model hyperparameters, you\u0026rsquo;re kinda fitting them to the data in the dev set. That\u0026rsquo;s why after you\u0026rsquo;ve chosen the exact hyperparameters you want and after you\u0026rsquo;ve completely trained your model, you should test it using the test set, which is data it has never ever seen before, and should give you a \u0026ldquo;real-world\u0026rdquo; benchmark on how it performs. How to split the data (size-wise): If you have a small amount of data, in the tens of thousands, you can use the \u0026ldquo;olden\u0026rdquo; way (pre big-data) of splitting the data: 60% for your training set, 20% for your dev and 20% for your test set If however your data is in the millions, using 20% or 200 000 examples for your dev and test is surely a bit of an overkill. You can decide to use something like 10 000 examples for your dev/test sets, so you\u0026rsquo;d have a split of 99%/1%/1% for your train/dev/test sets respectively. If your dataset is even bigger, you can also use something like 99.5%/0.4%/0.1%, whatever lets you quickly decide how well the model is training and performing on data it\u0026rsquo;s never seen before. Make sure your training and dev/test sets come from the same distribution. If your training data is really high res, but the data you\u0026rsquo;ll use during inference is lower quality due to lower resolutions, shaky camera, dirty lenses, and so on, your model won\u0026rsquo;t perform well.\nYour training data should be as similar as possible to real world data you\u0026rsquo;ll use during inference, and you should try to cover as much possible situations you can, which includes different weather conditions, bumpy roads, and so on.\nThe main reason for this type of mismatch is that you don\u0026rsquo;t have enough training data, so you get some online, e.g. if you took a YouTube video of a car driving around town that has a super high res camera and added it to your training data, but your RC car has a really low res camera.\nHow to know if your model has high variance or bias (or both): Here\u0026rsquo;s a great illustration by Andrew Ng on what high bias and variance look like:\nHigh variance: the model is overfitting the training data and performing poorly on new unseen data.\nHigh bias: the model isn\u0026rsquo;t even performing well on the training data.\nHigh bias and high variance: the model underfits the training data but performs even worse on the dev data.\nThe two key things to take away are:\nBy looking at how well the model performs on the training data, you can see how much bias it has; if it has a low error, it has a low bias, if it has a large error, it has large bias.\nBy looking at how well the model performs on the dev set, you can see how much variance it has; if it performs much worse than on the training set, it has high variance, if it performs slightly worse or the same, it has low variance and generalizes well.\nBasic recipe for learning your machines Get the model to perform well on the training data (low bias) Get the model to perform well on the dev set data (low variance) If you have large bias: (Almost always works): Try a bigger network: more hidden layers, more hidden units. (Sometimes works, but never hurts): Try training it longer: give it more time or use more advanced optimization algorithms. (Maybe it\u0026rsquo;ll work, maybe it won\u0026rsquo;t): Try finding a better neural net architecture: try finding an architecture that\u0026rsquo;s proven to work for your specific problem. Consider asking yourself: is this even possible to do? If you\u0026rsquo;re trying to train a classifier on very blurry low res images, is it even possible to do so? What\u0026rsquo;s the base error for that problem? If you have large variance:\n(Best way to solve it): Get more data, it can only help. But sometimes it\u0026rsquo;s impossible to get more data. (Almost always helps): Regularization (Maybe it\u0026rsquo;ll work, maybe it won\u0026rsquo;t): Try finding a better neural net architecture. Same as for the large bias problem. Don\u0026rsquo;t worry about the bias-variance tradeoff if you\u0026rsquo;re doing deep learning. It isn\u0026rsquo;t much of a thing anymore:\nAs long as you can train a bigger network (and use regularization), you\u0026rsquo;ll almost certainly be able to get rid of high bias without increasing variance (by much). As long as you can get more data (not always, but mostly possible), you\u0026rsquo;ll almost certainly be able to get rid of high variance without increasing bias (by much). Regularizing neural nets To try and gain some intuition about regularization in neural nets, let\u0026rsquo;s walk through implementing L2 regularization in a neural net. Since we\u0026rsquo;re mostly doing computer vision stuff with our self-driving car, we\u0026rsquo;ll most likely be using batch normalization and dropout regularization rather than L2, which is used much more often in computer vision, so feel free to skip this part if you\u0026rsquo;re not interested.\nIf our cost function is defined as:\n++\r\\mathcal{J}(w^{[1]}, b^{[1]}, ..., w^{[l]}, b^{[l]}) = \\frac{1}{n}\\sum_{i=1}^{n}\\mathcal{L}(\\hat{y}^{(i)},y^{(i)})\r++\rWe can regularize it by adding a regularization term:\r++\r\\mathcal{J}(w^{[1]}, b^{[1]}, ..., w^{[l]}, b^{[l]}) = \\frac{1}{n}\\sum_{i=1}^{n}\\mathcal{L}(\\hat{y}^{(i)},y^{(i)}) + \\frac{\\lambda}{2n}\\sum_{l=1}^l ||\\mathcal{w}^{[l]}||^2\r++\rLet's break it down.\rThe first part uses the regularization parameter $\\lambda$, which is a hyperparameter to be tuned using the dev set, and divides it by $2n$, which is just a scaling constant.\nThe second part uses the Frobenius norm (basically the L2 norm of a matrix), and it basically sums up the squares of all the elements of all $\\mathcal{w}$ matrices we use:\n++\r||\\mathcal{w}^{[l]}||^2 = \\sum_{i=1}^{n^{l}}\\sum_{j=1}^{n^{[l-1]}}(\\mathcal{w}_{i,j}^{[l]})^2\r++\r$\\mathcal{w}$ is a $(n^{[l]}, n^{[l-1]})$ dimensional matrix, where the first dimension represents the number of units in the $l$-th layer, and the second represents the number of units in the previous layer.\rWhen implemented, during backprop the $W^{[l]}$ matrix gets multiplied by $\\frac{\\lambda}{n}$, a number slightly smaller than 1, which is why it\u0026rsquo;s also called weight decay, since the weight loses just a bit of it\u0026rsquo;s value.\nWhy does this help? If we set the $\\lambda$ parameter in the regularization term below to a large value, it will set the values of $w^{[l]}$ very close to zero.\n++\r\\mathcal{J}(w^{[1]}, b^{[1]}, ..., w^{[l]}, b^{[l]}) = \\frac{1}{n}\\sum_{i=1}^{n}\\mathcal{L}(\\hat{y}^{(i)},y^{(i)}) + \\frac{\\lambda}{2n}\\sum_{l=1}^l ||\\mathcal{w}^{[l]}||^2\r++\rSo what happens if $w^{[l]}$ is close to zero? It will cause $z^{[l]}$ to have a very small range of values, close to zero.\r++\rz^{[l]} = w^{[l]}a^{[l-1]}+b^{[l]}\r++\rLet's assume we're using $tanh(z)$ as our activation function.\rIf the value of $z^{[l]}$ can be only a small range of values close to zero, our activation function will start looking like a linear function, as shown below:\nWhich in turn means every layer in our neural net will begin to look like a linear layer, which will cause our network to be able to approximate only a linear function to our data, which will cause our model to go from high variance to high bias.\nOf course, we don\u0026rsquo;t (and won\u0026rsquo;t) set our $\\lambda$ parameter to a very large value, but rather set it to somewhere in between, which will get rid of our high variance problem and not cause high bias instead.\nDropout regularization This illustration was also taken from Andrew Ng\u0026rsquo;s course video:\nImagine we have a neural net that looks like the one pictured above, on the left.\nTo stop our model from overfitting our data, we can randomly knock-out some neurons while training our model. This in turn causes the model to be unable to rely on just a particular set of neurons, or a pathway of neurons, so it has to train multiple subsets of neural nets, which makes it much more difficult to overfit the data, since it never knows when some of the neurons will be knocked out. $$ S(i,j) = (I*K)(i,j) = \\sum_m\\sum_nI(m,n)K(i-m,j-n) $$\nOther tips to prevent overfitting/bias Augment your data Let\u0026rsquo;s say we\u0026rsquo;re training our car on the following track:\nWe run a few laps on the track and save our data. If we\u0026rsquo;ve only driven in the usual, counter-clockwise direction, we have inadvertently caused our model to be slightly biased to always turn a bit to the left.\nWhy?\nWe can see that, globally, the car is always turning a bit to the left. So if you only include this data in your training set, the car will always be inclined to turn just a bit to the left.\nHow to solve it Other than driving the very same track, just in the opposite direction, you can simply mitigate this issue by augmenting your data with the same images, just horizontally flipped, thus getting:\nNow your car shouldn\u0026rsquo;t be inclined much to steer to one side or the other for no reason.\nMake sure to make copies of your JSON files and invert their steering values as well.\nAugmentation by Neural Style transfer You can even apply preprocessing like this to your images to try and make it focus on the more important features such as lane lines:\nAlso, I wrote this just to have an excuse to make some artful Baby Yoda memes, which had to be put into this project somehow, so here goes: You could still tell that it\u0026rsquo;s Baby Yoda, even though the images lost a lot of the original information about him, e.g. the hairs on his head. So in contrast, you could use this approach and still be able to identify lane lines on images, but just by focusing on the things that make us recognize them the most.\nOr not. Anyways, artful Baby Yoda memes.\nEarly stopping When training your model, you can often see the validation error getting lower over time, following the training error curve, but then at one moment you see it take off and get much worse over time:\nOne possible solution is stopping the training early, when you see the validation error starting to increase.\nJust keep in mind that by doing so, you\u0026rsquo;re now coupling two optimization problems that you were solving as separate before: the optimization of your cost function and the prevention of overfitting the dataset. By doing so, you\u0026rsquo;re not doing quite the best job you could at minimizing the cost function, since it\u0026rsquo;s obvious that the error could get much lower if the training wasn\u0026rsquo;t stopped early, while you\u0026rsquo;re simultaneously trying not to overfit your data.\nOne alternative could be using L2 regularization or some other regularization technique, but you can often see early stopping being used in practice. Just keep in mind the trade off you\u0026rsquo;re doing.\nIterating quicker: a single number performance metric First, lets define two terms using a classifier example; precision and recall.\nPrecision: of the examples that your classifier recognizes as X, how many examples actually are X. Recall: of the examples that are actually X, how many of them does your classifier recognize as X.\nWe can combine these two metrics into a single number, as a harmonic mean of the two, which is called the F1 score:\n++\rF_1 = \\frac{2}{\\frac{1}{precision}+\\frac{1}{recall}}\r++\rBut why?\rSure, this doesn't help much if you're just using a binary classifier like mentioned above, to see if something is X or not. Or even if you're classifying things into two classes. But imagine trying to decide between three implementations of a classifier with 3 possible classes:\nImplementation/Classes A (precision) A (recall) B (precision) B (recall) C (precision) C (recall) First implementation 95% 90% 89% 88% 92% 89% Second implementation 93% 89% 92% 90% 92% 90% Third implementation 89% 88% 95% 93% 90% 89% Which one is the best? Sure isn\u0026rsquo;t easy to tell from the table above. At least not to me.\nNow look what it would look like if we used the F1 score for all of the possible classes:\nImplementation/Classes A ($F_1$ score) B ($F_1$ score) C ($F_1$ score) First implementation 92.4% 88.5% 90.5% Second implementation 91% 91% 91% Third implementation 88.5% 94% 89.5% Looks much simpler than the first table, but still could be simpler. Since we\u0026rsquo;re probably interested in our classifier working as best as possible over all classes, we can take the average of all of the 3 $F_1$ scores to get one simple number that tells us how well our classifier works:\nImplementation/Classes Average $F_1$ score First implementation 90.5% Second implementation 91% Third implementation 90.7% Why do this? The example above could\u0026rsquo;ve easily had a hundred different classes, with a dozen different implementations. That would\u0026rsquo;ve been impossible to look at even if you used $F_1$ scores for every class.\nWhen trying out a bunch of different values of hyperparameters and implementation details, it\u0026rsquo;s much faster to iterate and much easier to decide between all possible implementations when you have a single number that tells you how well it performs. And I believe this is a pretty good way to get one, even considering the amount of some fine-grained details you lose. You can always look them up after you eliminate some implementations, and have only a few you\u0026rsquo;re actually considering.\nOne important thing to notice is that there could be other important metrics to look at other than how accurate our classifier is. For an example, we could be making a classifier that classifies obstacles our car will collide with if we don\u0026rsquo;t avoid them. Surely, we\u0026rsquo;d like to correctly classify as much of those as possible, so make a table, as the one above:\nClassifier/Score $F_1$ score First classifier 92% Second classifier 97% Third classifier 99% Cool, so we pick the third classifier. And our car crashes. Wait, wut?\nWe picked the best classifier in terms of it correctly classifying as much of the objects in our path as possible, but there\u0026rsquo;s one thing we didn\u0026rsquo;t look at; the time it takes it to classify an object.\nClassifier/Score $F_1$ score Average runtime First classifier 92% 20 ms Second classifier 97% 70 ms Third classifier 99% 3000 ms So yeah, we did recognize most of the objects we were on a collision path with, but by the time that we did, the car had already crashed into them.\nTaking that in mind, the second classifier is obviously the better choice.\nSo it\u0026rsquo;s important to keep in mind that while we\u0026rsquo;re trying to optimize one of the metrics of our model, we also have to pay attention to the other minimum requirements the model must have in order to actually be usable in the real world, as per our real-time necessary example.\nChoosing Hyperparameters Hyperparameters ordered by importance are:\nlearning rate (alpha) beta, number of hidden units, mini-batch size number of layers, learning rate decay A traditional way to choose hyperparameters is to sample them from a grid:\nFirst we\u0026rsquo;d define a grid, which can either be a randomized or a regular grid, and then iteratively sample the parameters from it:\nIllustration by Andrew Ng:\nKeep in mind that the grid search approach suffers from the curse of dimensionality.\nThere\u0026rsquo;s a lot of ways to do this, most frameworks even have built-in ways of doing this so I\u0026rsquo;d highly recommend you look into the topic of hyperparameter optimization.\nHyperparameters can get stale Also keep in mind that you should re-evaluate your hyperparameters every once in a while, because as you get a lot more data, they can get stale.\nThey can also get stale if you get a new GPU/CPU, change your network a bit or for any number of reasons. Just re-evaluate them from time to time, depending on how much you\u0026rsquo;re changing stuff around.\nClosing thoughts The more you do this stuff, the easier it\u0026rsquo;ll get, and the more tricks you\u0026rsquo;ll learn.\nThe thing about ML is that you have to rely on your intuition a lot. If a model is misbehaving, you can\u0026rsquo;t simply set a breakpoint on it and debug it, there can be a plethora of reasons for it:\nMaybe your data is bad Maybe there is a bug in your architecture Maybe you\u0026rsquo;re just using the wrong hyperparameters \u0026hellip; You have to keep trying and not give up. It can take a few days even to get your model to perform well, even on smaller testing datasets, depending on the complexity of the task you\u0026rsquo;re trying to achieve.\nDo your best to make sure that the things that you can control, e.g. the quality of your data and its distribution, are okay.\nFor the parts you can\u0026rsquo;t directly see, like how your convolutional layers are doing stuff, you can use all sorts of tools like the visualization techniques we\u0026rsquo;ve done earlier to try and gain some intuition on how it\u0026rsquo;s performing and what could go wrong.\nThat\u0026rsquo;s pretty much it! Have fun experimenting with your model.\nNext up: learning behaviours, e.g. lane changing!\n"
},
{
	"uri": "http://localhost:1313/draft-and-todo/hough-transform/",
	"title": "Raspisivanje Hough transform-a",
	"tags": [],
	"description": "",
	"content": "\rTHIS IS A DRAFT\nI\u0026rsquo;ll probably just delete this!\nHough transform Linearnu funkciju u $xy$ - ravnini definiramo kao: $f(x) = y = ax + b$\nU toj funkciji, $a$ i $b$ su fiksni i određuju nagib/smjer funkcije. Pravac $y = ax+b$ predstavlja sve moguće kombinacije $x$ i $y$ uz zadane $a$ i $b$.\nPretpostavimo da znamo dvije točke na tom pravcu (dvije moguće kombinacije $x$ i $y$ za zadane $a$ i $b$):\n$(x_i, y_i)$ $(x_j, y_j)$ Znamo da je pravac moguće definirati sa dvije točke kroz koje prolazi. Tako bi funkciju $y = ax + b$ trebali moći definirati pomoću te dvije točke.\nDosad smo gledali funkciju unutar $xy$ - ravnine, koja nam kroz svoje dvije dimenzije prikazuje sve moguće kombinacije $x$ i $y$ parametara funkcije.\nIsto tako možemo funkciju gledati kroz $ab$ - ravninu, koja prikazuje sve moguće kombinacije $a$ i $b$ parametara funkcije.\nZamislimo da više nismo u $xy$ - ravnini, nego u $ab$ - ravnini:\nZamislimo da više nemamo fiksne $a$ i $b$ pomoću kojih smo definirali funkciju $y = ax + b$, nego imamo dvije točke koje smo gore prikazali:\n$(x_i, y_i)$ $(x_j, y_j)$ Kako pomoću njih možemo dobiti početni pravac/funkciju koju smo imali, sa fiksnim vrijednostima $a$ i $b$.\nPošto smo u $ab$ - ravnini, sada su nam $x$ i $y$ fiksni, budući da ih ne možemo direktno vidjeti u ovoj ravnini nego njihove vrijednosti samo utječu na konačan položaj $a$ i $b$ parametara.\nBudući da su nam sada $a$ i $b$ nepoznanice unutar funkcije $y=ax + b$, želimo ih iskazati kao kombinaciju poznatih parametara (fiksnih $x$ i $y$), što možemo učiniti:\nPrebacimo $b$ na drugu stranu: $-by = ax$ Prebacimo $y$ na drugu stranu: $-b = ax - y$ Pomnožimo sa minus jedan: $b = -ax + y$ Sada u $ab$ - ravnini možemo prikazati sve moguće kombinacije $a$ i $b$ za zadane $x$ i $y$. Za prvu zadanu točku $(x_i, y_i)$:\nZa drugu zadanu točku $(x_j, y_j)$:\nAko uzmemo njihov presjek, dobit ćemo vrijednosti $a$ i $b$ pomoću kojih bi u $xy$ ravnini mogli proći kroz obje zadane točke $(x_i, y_i)$ i $(x_j, y_j)$, odnosno dobit ćemo pravac na kojemu se obje točke nalaze, koji je definiran sa dobivenim vrijednostima $a$ i $b$:\nOvo u biti intuitivno radimo kadgod kroz dvije točke želimo provući pravac.\nKroz jednu točku možemo provući beskonačno mnogo pravaca sa beskonačno mnogo smjerova ($a$ i $b$). Taj beskonačan skup pravaca možemo vidjeti u $ab$ - ravnini na desnoj slici, za svaku od zadanih točaka. Svaki od pravaca u $ab$ - ravnini za zadanu točku definira beskonačno mnogo smjerova (kombinacija $a$ i $b$) koje pravci koji prolaze kroz tu točku mogu imati.\nAko imamo dvije točke, postoji samo jedan smjer (kombinacija $a$ i $b$) koji se nalazi u oba skupa svih mogućih smjerova za svaku od te dvije točke, što bi značilo da se on u $ab$ - ravnini nalazi točno na presjeku pravaca zadanih tim točkama.\nhttps://www.youtube.com/watch?v=4zHbI-fFIlI\n++\rcos(\\varphi) = \\frac{x}{\\rho} \\rightarrow x = \\rho * cos(\\varphi)\\\\\rsin(\\varphi) = \\frac{y}{\\rho} \\rightarrow y = \\rho * sin(\\varphi)\\\\\r\\\\\ry=ax+b\\\\\ry=(- \\frac{cos \\ \\theta}{sin \\ \\theta})x+(\\frac{r}{sin \\ \\theta})\r++\r![http://en.wikipedia.org/wiki/File:Unit_circle.svg](/images/hough/Wg7IU.png) Povučemo tangentu na točku:\nTangenta siječe $x$ na: $x=1/cos(t)$ Tangenta siječe $y$ na: $y = 1/sin(t)$ Tangenta je udaljena 1 od ishodišta pa je: $a = 1/cos(t)$ $b = 1/sin(t)$ Intercept slope line equation (odsječak i nagib):\nPretvorimo $y = ax+b$ u intercept slope oblik $\\rightarrow$ $\\frac{x}{a} + \\frac{y}{b} = 1$ Uvrstimo gornje vrijednosti za $a$ i $b$ : $\\frac{x}{1/cos(t)} + \\frac{y}{1/sin(t)} = 1$ Što možemo pojednostaviti (dvojni razlomak) u: $x \\ cos(t) + y \\ sin(t) = \\rho$ Ako zamijenimo $t$ sa $\\theta$: $x \\ cos \\ \\theta + y \\ sin \\ \\theta = \\rho$ "
},
{
	"uri": "http://localhost:1313/software/donkeycar-host/",
	"title": "DonkeyCar installation: Host PC",
	"tags": [],
	"description": "",
	"content": "Let\u0026rsquo;s install the Donkey software on your host PC. The only part where this differs between the three platforms, Mac OS X, Linux and Windows, is in the Miniconda software installation, so we\u0026rsquo;ll get that out of the way first.\nMac OS X Download and install: Miniconda here, git here Open up a terminal and follow the rest of the tutorial Windows Download and install: Miniconda here, git here Open an Anaconda Prompt via Start Menu and follow the rest of the tutorial Linux Download Miniconda here and install it Open up a terminal and follow the rest of the tutorial The rest of the tutorial: Go to a place where you want the stuff we\u0026rsquo;ll be working on to be.\n# e.g. on Linux or Mac cd ~ # e.g. on Windows cd c:\\\\Users\\\\yourUsername Make a folder for your projects and cd to it:\nmkdir projects cd projects Clone the Donkey repository using git:\ngit clone https://github.com/autorope/donkeycar cd donkeycar git checkout master Create the Python Anaconda environment using the yml file from the repository:\n# Windows conda env create -f install\\envs\\windows.yml # Mac conda env create -f install\\envs\\mac.yml # Linux/Ubuntu conda env create -f install\\envs\\ubuntu.yml # All three OS\u0026#39;s conda activate donkey pip install -e .[pc] If you\u0026rsquo;re not using a host PC with a GPU, you\u0026rsquo;re done! If you\u0026rsquo;re using a NVidia GPU and not using a Mac (sorry, no TensorFlow GPU support for you folks): Install the TensorFlow software requirements for Nvidia GPUs, which basically means:\nDownload and install NVIDIA drivers\nDownload and install the CUDA Toolkit\nDownload and install cuDNN (you should just copy the bin, lib and include folders from the zip to your cuda installation folder)\nDownload and install TensorRT 5.0 to improve latency and throughput for inference on some models (same as the above)\nWhich means installing PyCUDA (make sure nvcc is in your PATH):\npip install \u0026#39;pycuda\u0026gt;=2017.1.1\u0026#39; If you\u0026rsquo;re getting errors, check the requirements here. If you\u0026rsquo;re on Windows, you probably need the VS C++ 14, just download it through the VS Build Tools Downloading and installing TensorRT 5.0\nThen you can finally:\nconda install tensorflow-gpu==1.13.1 "
},
{
	"uri": "http://localhost:1313/artificial-intelligence/adding-behaviours/",
	"title": "Adding behaviours: automated lane changing",
	"tags": [],
	"description": "",
	"content": "The only thing left to do, in order to test my idea with multiple specialized networks converging into a final decision layer, is to implement the behavioural specialized network.\nThis is what the model will look like:\nThe behavioural part of the network can be seen just before the 100-unit dense layer.\nHere it is in action: note the lower left terminal to see when I pressed the button to change lanes\nLet\u0026rsquo;s get to it!\nDonkey and behaviours After a bit of research, I found that other people have already tried (and succeded) to implement behaviours such as passing using Donkey. There\u0026rsquo;s even a behavioural model sitting in the keras.py script, which is great.\nThere\u0026rsquo;s also some out of the box support for behaviours, but worry not, I\u0026rsquo;ll walk you through it.\nAlso, the behavioural part of the network is pretty small, but it works! It\u0026rsquo;s based off other behavioural networks I\u0026rsquo;ve found people used with Donkey. You can use the built-in one if you\u0026rsquo;d like to quickly get your feet wet.\nGetting behavioural data If we want to train the network for certain behaviours, we have to give it labeled training data with said behaviours.\nWe can do this many ways, we can even make the neural network trigger certain behaviours, e.g. when a classifier detects a stop sign, trigger the stop behaviour. But for this PoC, a simple button click on our controller will do.\nI was going to implement a class that took in a list of possible behaviours, that would set certain behaviours to currently active on a button press, and wrote the currently set behaviour to the training data json file. And, for the I don\u0026rsquo;t know-what-time now, I\u0026rsquo;ve found that Donkey has just such a class already implemented. And since it\u0026rsquo;s there and it\u0026rsquo;s really easy to understand and use, why wouldn\u0026rsquo;t we.\nYou can find it in donkeycar/parts/behavior.py. It takes in a list of possible behaviors and provides methods to set the behaviours from the list iteratively and a method to explicitly set a behaviour. Beautiful.\nIf we set the TRAIN_BEHAVIORS variable in myconfig.py to be True, the myconfig.py script adds the behaviour part to the current vehicle, initializing it with a list of behaviours we set in the config file using the BEHAVIOR_LIST variable.\nSay we\u0026rsquo;ve defined a behaviour list:\nbehaviours = [\u0026#39;Left_Lane\u0026#39;, \u0026#39;Right_Lane\u0026#39;] The behaviour class will outputs the current behaviour state as an index, a one-hot array and it\u0026rsquo;s label as the following key/value pairs to the dataset file:\n\u0026#34;behavior/state\u0026#34;: 0, \u0026#34;behavior/label\u0026#34;: \u0026#34;Left_Lane\u0026#34;, \u0026#34;behavior/one_hot_state_array\u0026#34;: [1.0, 0.0] To map a controller button as a trigger for incrementing or setting the behaviours, we can use the set_button_down_trigger from the controller.py script:\ndef set_button_down_trigger(self, button, func): # assign a string button descriptor to a given function call self.button_down_trigger_map[button] = func We would pass the button we want (the name we\u0026rsquo;ve used in our custom mapping, e.g. \u0026lsquo;L1\u0026rsquo;) along with the method that increments or sets the behaviour from behavior.py.\nYou can also use an axis as a trigger or a button up event as a trigger.\nAnd that\u0026rsquo;s pretty much it, Donkey will just append the behavioural data to our dataset!\nAdding the behavioural subnetwork It\u0026rsquo;s actually pretty simple, the new part in the Keras model should look something like:\n# New input layer behaviourInput = Input(shape=(numberOfBehaviourInputs,), name=\u0026#34;behaviourInput\u0026#34;) # ConvNet parts ... # Behavioural net z = behaviourInput z = Dense(numberOfBehaviourInputs * 2, activation=\u0026#39;relu\u0026#39;)(z) z = Dense(numberOfBehaviourInputs * 2, activation=\u0026#39;relu\u0026#39;)(z) z = Dense(numberOfBehaviourInputs * 2, activation=\u0026#39;relu\u0026#39;)(z) # Concatenating the convolutional networks with the behavioural network o = Concatenate(axis=1)([z, c]) o = Dense(100, activation=\u0026#39;relu\u0026#39;)(o) o = Dense(50, activation=\u0026#39;relu\u0026#39;)(o) # Output layers ... # Update the model inputs model = Model(inputs=[imageInput, laneInput, behaviourInput], outputs=[steering_out, throttle_out]) You just need to add the number of behaviour inputs as an input to the class constructor, and the behavioural data as an input to the model. This way we can easily train any number of behaviours we\u0026rsquo;d like.\nTraining the model After training it on a smaller dataset of around 7k records with about 20 lane changes, I got the following:\nFurther ideas Now that we\u0026rsquo;ve seen that we can actually train behaviours like this, I\u0026rsquo;m planning on creating a behaviour that uses an object tracker/classifier network to trigger certain behaviours. So let\u0026rsquo;s make that network!\n"
},
{
	"uri": "http://localhost:1313/hardware/mounting-plates/",
	"title": "Building the mounting plates for the hardware",
	"tags": [],
	"description": "",
	"content": "There are many options to go for when it comes to mounting your Nano and the rest of the hardware to your RC car.\nOption 1: 3D printing If you’re planning on 3D printing your parts, take a look at:\nThe official DonkeyCar docs which contain a lot of 3D models for printing: chassis and adapters, mounting plates, more mounting plates, etc. Markku.ai’s Chilicorn Rail for the Tamiya cars I actually used the Chilicorn Rail for the first iteration of my build, and was very lucky to have been introduced by my mentor to Mitch, who printed out the parts and helped me out with a ton of stuff since then:\nEven if you’re not planning on using this method, I’d still recommend at least getting the RPi camera mount 3D printed, since it’s kinda difficult to get it to the right angle and position if you’re making something out of raw materials yourself.\nOption 2: Hack something together yourself If you’re planning on hacking something together by yourself, you can pretty much do anything you want. I used a special aluminium plate I got as a donation for my thesis from my mentor’s father :):\nAfter some modeling and tinkering, I came up with the following idea for my mounts:\nThe only thing left now is to measure the aluminium, mark down where to drill the screw holes and assemble it together, but being the lucky man that I am, my hand drill broke down just as I wanted to drill the aluminium, but fear not, I had this bad boy standing by:\rAnd after measuring up the dimensions and taking my materials, much to my surprise, I successfully drilled up my plates and got this:\nWith some zip-tie magic and screwing around 🙃 this was the end result:\nNevermind the disconnected antennae, we’ll get to that in a second.\nThat should be it as far as the ‘dumb’ part of the hardware is concerned. On to connecting all of the stuff together and setting up the Nano! "
},
{
	"uri": "http://localhost:1313/software/donkeycar-simulator/",
	"title": "DonkeyCar Installation: The Simulator",
	"tags": [],
	"description": "",
	"content": "\rEven if you don\u0026rsquo;t have an RC car, you can start here and follow the rest of the project by just substituting the RC car with the simulator!\nThis is one of the coolest parts of DonkeyCar for me, and probably one of the most useful ones. It\u0026rsquo;s also a good way to get your feet wet with this kind of a project without building an actual RC. If it turns out you like it, you can always go back to the beginning and build an actual platform.\nAnd if you\u0026rsquo;re thinking: boo, why use a simulator when we have a real RC car!, remember, even Tesla uses a simulator.\nIt is true that, as Andrej Karpathy says in the video above: there is no substitute for real data. But that being said, the simulator gives us a chance to rapidly prototype and even test multiple models at once. It also gives us an environment where we don\u0026rsquo;t have to worry about the physical RC crashing into something or hurting someone. Also, if you wanted to use reinforcement learning, would you really be willing to let your RC smash into the wall for episodes and episodes until it learns basic stuff?\nAlso, if you\u0026rsquo;ve got the time, do take a look at the video above (Tesla Autonomy Day stream), it\u0026rsquo;s full of insights on how they do stuff that actually drives on actual roads.\nLet\u0026rsquo;s get it up and running: Download and unzip the simulator for your platform from the DonkeyCar GitHub release page\nPlace the simulator into your projects folder (where you cloned the Donkey repo)\nInstall DonkeyGym:\ncd ~/projects git clone https://github.com/tawnkramer/gym-donkeycar conda activate donkey pip install -e gym-donkeycar Create a new Donkey application for your simulator:\ndonkey createcar --path ~/mysim cd ~/mysim Edit the myconfig.py file inside the application folder you just created (mysim):\n# Enables the Donkey Gym simulator wrapper DONKEY_GYM = True DONKEY_SIM_PATH = \u0026#34;/home/wherever/your/projects/are/DonkeySimFolder/DonkeySim.exe\u0026#34; # Choose the track you want to run, you can change this later DONKEY_GYM_ENV_NAME = \u0026#34;donkey-generated-track-v0\u0026#34; Download this test dataset that contains data of a car recovering from dropping out from the track and some standard driving data and put it in your data folder inside your application folder (/mysim/data/)\nTrain your model by running\npython manage.py train --model models/mymodel.h5 You can choose different architectures or create your own by going into the DonkeyCar parts folder (/projects/donkeycar/parts/) and opening up the keras.py script. You can define a new class that inherits the base class for models and implement your own neural network architecture, but we\u0026rsquo;ll get to that further on in the project! If you\u0026rsquo;ve created your own architecture/class, you can train the model using it by passing the flag \u0026ndash;type=yourClassName Some of the built-in models are: categorical, linear, rnn, 3d, latent, etc. When using your custom model to drive the car, if you get some dimensions errors, you\u0026rsquo;re probably forgetting to pass the \u0026ndash;type flag with your class name while running it, it should fix it. Test your model by running\npython manage.py drive --model models/mymodel.h5 Open up your browser and go to: http://localhost:8887 and set the mode to Local Pilot and watch your car go! If you\u0026rsquo;re using Linux, you can also pass the \u0026ndash;js parameter and use your gamepad if it\u0026rsquo;s mounted to /dev/js0\nDownload this big dataset that contains 16 different venues with tape lined tracks on concrete and tile (some are on carpet and cobblestones)\nCredits to Tawn from the Donkey Slack channel The dataset is big. And it contains a lot of small files, which means you should pay attention where you\u0026rsquo;re extracting the files, since moving/copying them will take a long while since OS\u0026rsquo;s don\u0026rsquo;t like working with millions of small files.\n"
},
{
	"uri": "http://localhost:1313/artificial-intelligence/object-detection-and-tracking/",
	"title": "Detecting and tracking objects on images",
	"tags": [],
	"description": "",
	"content": "\rImplemented. Need to sit down and write it up! "
},
{
	"uri": "http://localhost:1313/software/donkeycar-rc/",
	"title": "DonkeyCar installation: RC car",
	"tags": [],
	"description": "",
	"content": "Connecting to your RC via SSH To connect and work with your RC throughout the rest of the project, you\u0026rsquo;ll need two things:\nAn SSH client The IP address of your RC SSH Clients: If you\u0026rsquo;re using Linux or a Mac, you\u0026rsquo;re all set. They come with a SSH client pre-installed, and you just need to open up a terminal and type:\nssh username@ipAddress If you\u0026rsquo;re using Windows, you need to install one. I\u0026rsquo;d recommend using MobaXTerm:\nDownload the installer or the portable version and install/unpack it\nTo SSH to a device:\nOpen MobaXTerm\nPress on the Start local terminal button\nssh username@ipAddress SSH via Ethernet: If your RC is connected to your network via an ethernet cable, you should be able to find your IP address:\nThrough your Router/Gateway interface, by looking at the DHCP leases;\nOr by connecting your Nano to a monitor, keyboard and mouse, opening up a terminal and writing:\nip addr show SSH via WiFi: I would highly recommend this approach, so you can connect your RC to a WiFi network and take it and your laptop with you and connect to it on the fly.\nTo connect it to a WiFi network, you either need to first SSH into it over ethernet, or connect it to a monitor, keyboard and mouse, and do the following:\nConnect to a WiFi network:\nnmcli device wifi connect YOUR-SSID password YOUR-PASSWORD Or make the Jetson into a hotspot so you can connect your laptop to it:\nnmcli dev wifi hotspot ifname wlan0 ssid HOTSPOT-SSID password HOTSPOT-PASSWORD What I like to do is connect it to both my home network, and a hotspot on my mobile phone, so I can use it anywhere and still have Internet access on both it and my laptop.\nTo do so, I use the nmcli autoconnect.priority property, so my home network has a higher priority than my phone hotspot, in case I forget to turn it off while I\u0026rsquo;m at home, so it doesn\u0026rsquo;t eat up my data plan.\nYou can find all of your network connections saved in /etc/NetworkManager/system-connections/, which you can open up with a text editor and edit the autoconnect.priority property for each network. The higher the integer you assign to it, the higher the priority.\nAs an example, the network connection profile for my hotspot looks something like:\n[connection] id=Hotspot uuid=random-long-string type=wifi autoconnect_priority=2 # The home network has a priority of 3, in my case permissions= If your Nano keeps dropping the connection for some reason, try disabling the power saving mode found in /etc/NetworkManager/conf.d, using a text editor.\nAlso, I assumed your Nano already has the nmcli or the NetworkManager utility installed, since it, at the time of writing, comes pre-installed with any Ubuntu distro. If, for some reason, you don\u0026rsquo;t have it, you can install it using sudo apt install network-manager.\nAfter connecting your Nano to a WiFi network you want, find out its IP Address by opening up a terminal and typing:\nip addr show Now you can use your SSH client and SSH into the Nano, type in your username and password and you\u0026rsquo;re ready to follow the rest of the tutorial. Dependencies Open up a terminal on your Nano and install the following dependencies:\nsudo apt-get update sudo apt-get upgrade sudo apt-get install build-essential python3 python3-dev python3-pip libhdf5-serial-dev hdf5-tools nano ntp Set up a Virtual Env # Install the venv package pip3 install virtualenv python3 -m virtualenv -p python3 env --system-site-packages # Activate the venv automatically at boot echo \u0026#34;source env/bin/activate\u0026#34; \u0026gt;\u0026gt; ~/.bashrc source ~/.bashrc Compiling and installing OpenCV First, since OpenCV needs more than 4GB of RAM to be built from source, and our Jetson Nano just doesn\u0026rsquo;t have that much RAM, we have to define some swap space to prevent it from going bonkers while compiling it:\n# Allocates 4G of additional swap space at /var/swapfile sudo fallocate -l 4G /var/swapfile # Permissions sudo chmod 600 /var/swapfile # Make swap space sudo mkswap /var/swapfile # Turn on swap sudo swapon /var/swapfile # Automount swap space on reboot sudo bash -c \u0026#39;echo \u0026#34;/var/swapfile swap swap defaults 0 0\u0026#34; \u0026gt;\u0026gt; /etc/fstab\u0026#39; # Reboot sudo reboot Now, we need to get all the prerequisites needed to build OpenCV from source:\n# Update sudo apt-get update sudo apt-get upgrade # Pre-requisites sudo apt-get install build-essential cmake unzip pkg-config sudo apt-get install libjpeg-dev libpng-dev libtiff-dev sudo apt-get install libavcodec-dev libavformat-dev libswscale-dev libv4l-dev sudo apt-get install libxvidcore-dev libx264-dev sudo apt-get install libgtk-3-dev sudo apt-get install libatlas-base-dev gfortran sudo apt-get install python3-dev Okay, let\u0026rsquo;s download the source code for OpenCV which we\u0026rsquo;ll be building it from:\n# Create a directory for opencv mkdir -p projects/cv2 cd projects/cv2 # Download sources wget -O opencv.zip https://github.com/opencv/opencv/archive/4.1.0.zip wget -O opencv_contrib.zip https://github.com/opencv/opencv_contrib/archive/4.1.0.zip # Unzip unzip opencv.zip unzip opencv_contrib.zip # Rename mv opencv-4.1.0 opencv mv opencv_contrib-4.1.0 opencv_contrib Also we\u0026rsquo;ll need numpy in our virtual environment for this to work:\n# Install Numpy pip install numpy We also need to make sure CMake correctly generates the OpenCV bindings for our virtual environment:\n# Create a build directory cd projects/cv2/opencv mkdir build cd build # Setup CMake cmake -D CMAKE_BUILD_TYPE=RELEASE \\ -D CMAKE_INSTALL_PREFIX=/usr/local \\ -D INSTALL_PYTHON_EXAMPLES=ON \\ -D INSTALL_C_EXAMPLES=OFF \\ -D OPENCV_ENABLE_NONFREE=ON \\ # Contrib path -D OPENCV_EXTRA_MODULES_PATH=~/projects/cv2/opencv_contrib/modules \\ # Your virtual environment\u0026#39;s Python executable # You need to specify the result of echo $(which python) -D PYTHON_EXECUTABLE=~/env/bin/python \\ -D BUILD_EXAMPLES=ON ../opencv The cmake command shows a summary of its configuration, and you should make sure that the Interpreter is set to the Python executable of your virtual environment, not the base OS one.\nNow, to compile the code from the build folder, run the following:\nmake -j2 # Install OpenCV sudo make install sudo ldconfig This will take a while. And by a while, I mean: Go grab a cup of coffee and watch a TV Show or a movie or something while.\nNow we just need to link it to our virtual environment:\ncd to: /usr/local/lib/python[YOUR.VERSION]/site-packages/cv2/python[YOUR.VERSION] and do ls to find out the exact name of the .so we built.\nIt should look something like: cv2.cpython-[YOURVERSION]m-[***]-linux-gnu.so\nRename it to cv2.so: mv cv2.cpython-whatever-the-full-name-is.so cv2.so\nAnd finally:\n# Go to your virtual environments site-packages folder cd ~/env/lib/python[YOUR.VERSION]/site-packages/ # Symlink the native library ln -s /usr/local/lib/python[YOUR.VERSION]/site-packages/cv2/python-[YOUR.VERSION]/cv2.so cv2.so To make sure everything works as it should, run:\nimport cv2 # Should print 4.1.0 print(cv2.__version__) Install DonkeyCar First, go to a directory where you\u0026rsquo;d like your stuff to be:\n# Probably cd ~/projects Install the latest Donkey from GitHub:\n# Clone it from GitHub git clone https://github.com/autorope/donkeycar cd donkeycar # Checkout the master branch git checkout master pip install -e .[nano] pip install --extra-index-url https://developer.download.nvidia.com/compute/redist/jp/v42 tensorflow-gpu==1.13.1+nv19.3 "
},
{
	"uri": "http://localhost:1313/hardware/jetson-nano-installation/",
	"title": "Preparing the Jetson Nano",
	"tags": [],
	"description": "",
	"content": "Before we begin assembling our hardware together, we should first prepare our Jetson Nano by installing an OS on it and verifying everything works before it gets buried among all the other hardware on the RC.\nPreparing the microSD First, we\u0026rsquo;ll prepare the microSD by installing an OS on it for the Nano to run.\nThe official Jetson Nano docs are great and you can just follow them until the Next Steps step.\nIf you want a TL;DR version:\nDownload the Jetson Nano Developer Kit SD Card Image\nFormat your microSD card and then flash the image on it using whatever tool you’d like.\nYou can use Etcher Insert the microSD into the Jetson and connect your peripherals to it: monitor, keyboard, mouse and an ethernet cable.\nIf you’re using Windows, after the flashing it’ll complain you need to format the microSD before you use it, since it doesn’t recognize the Linux filesystem you just flashed on it. Don’t format it, just ignore the warning.\nFirst boot The TL;DR version would be:\nCarefully read through the NVIDIA Jetson software EULA and decide if you’ll accept it 🙃 Select the system language, keyboard layout and time zone Create your user and name your Jetson Log in If all went well, you should see the following screen:\n"
},
{
	"uri": "http://localhost:1313/hardware/assembling-the-nano/",
	"title": "Assembling the Jetson Nano",
	"tags": [],
	"description": "",
	"content": "Now we can finish up our Nano by connecting the WLAN card, microSD and the fan to it.\nPlugging in the microSD I actually already did a lot of assembly some time ago, and I\u0026rsquo;m writing this in retrospect, so don\u0026rsquo;t be worried if my Nano has a lot of stuff already hooked up to it and if it looks a bit different than yours, just focus on the stuff we\u0026rsquo;re going through and disregard the rest.\nFirst, we have to take out the Nano module from the breakout board we got it with. We begin by unscrewing these two Phillips screws, highlighted in yellow:\nAfter unscrewing them, pull these two levers outwards (highlighted in yellow) and your Nano module will pop out:\nThe Nano should pop out like this:\nAnd after pulling it out, you can plug in your microSD on the bottom side:\nConnecting the Noctua fan to the Nano On the other side of the module, you can find the heatsink with four screw holes into which, if you haven\u0026rsquo;t done so already, you can screw in the fan you bought for your Nano:\nYou also need to plug in the 3-pin (or 4-pin) fan connector to the Jetson board right below the ethernet port:\nPlugging in the WLAN+BT card The only thing left is the wireless card, which is plugged into the slot at the middle of the breakout board. You just need to unscrew one Phillips screw, insert the card, put the screw back in and connect your antennae connectors to the WLAN card. The antennae are the same, so don\u0026rsquo;t worry where to plug which one, it doesn\u0026rsquo;t matter.\nAnd that\u0026rsquo;s it for the Nano board! After putting it all back together, and connecting the antennae, it looks something like this:\n"
},
{
	"uri": "http://localhost:1313/hardware/connecting-the-car-to-the-nano/",
	"title": "Connecting the RC to the Nano",
	"tags": [],
	"description": "",
	"content": "Now comes the part that should differ the most, based on the RC you got. But don\u0026rsquo;t worry, it\u0026rsquo;s very much doable no matter the RC you got!\nFinding your ESC/Servo The first thing you should do is find your ESC and your Servo connectors, which should be a three wire connector coming from your RC car.\nIf you\u0026rsquo;ve bought a car that came with a wireless receiver, both the connectors should be connected to it. Here\u0026rsquo;s what it looked like on my RC:\nIf you didn\u0026rsquo;t have a wireless receiver, you should be able to see a connector coming out of your ESC, which is hooked to the RC motor, and a connector coming out of your steering servo, found at the front of your RC.\nHere\u0026rsquo;s a closeup of the two connectors connected to the receiver, with the first channel being the steering servo and the second one being the ESC, in my case:\nOkay, we found the connectors, how do we hook them up to our car?\nEnter: the PCA9685 We connect them to the first two channels (0 and 1) of our PCA9685, highlighted in green and blue:\nIt doesn\u0026rsquo;t matter which channel you connect the servo to, and which one the ESC to, just take note which one went where, you\u0026rsquo;ll need it later on. You can see the numbers denoting the channels above the three pins. I\u0026rsquo;ve connected mine to the zeroth and first channel:\nOkay, so we hooked up our car to the PCA9685. Now we need to hook it up to our Nano to be able to control the car through it.\nBut first, let\u0026rsquo;s talk a bit about the way the Nano communicates with the RC through the PCA9685, using a protocol called I²C (Inter-Integrated Circuit) (pronounced: I-squared-C).\nFeel free to skip this part if you aren\u0026rsquo;t interested in the alternatives to I²C and why we\u0026rsquo;re using it. You don\u0026rsquo;t need to understand this stuff in order to continue with the tutorial. That being said, it wouldn\u0026rsquo;t hurt to get to know a few other protocols, just to know what\u0026rsquo;s out there and how they differ.\nCLICK HERE TO SKIP I2C What the heck is I²C?\nIt was designed back in the \u0026rsquo;80s by Philips, to enable components on a circuit board easily communicate with each other. So it\u0026rsquo;s a protocol components use to talk to each other.\nIf you want to read a bit more, here\u0026rsquo;s the specification by NXP, since Phillips semiconductors migrated to NXP back in 2006.\nThere\u0026rsquo;s also a pretty neat primer you can read on the i2c-bus.org site.\nWhy I2C? Alternatives? This is basically a TL;DR of the awesome SparkFun I²C tutorial, be sure to check it out if you wanna go into more depth. I\u0026rsquo;ll assume you have some basics of UART and SPI while going through them.\nIf you are interested in this kinda stuff, but haven\u0026rsquo;t really done any embedded/electronics work before, please do check out the Engineering Essentials tutorials on SparkFun, they\u0026rsquo;re great.\nSerial UART: Serial ports are asynchronous (no CLK), so both devices have to agree on a data rate ahead of time: any excessive clock difference will result in garbled data. Requires hardware overhead: the UART at either end is relatively complex and difficult to accurately implement in software if necessary. At least one start and stop bit is a part of each frame of data, meaning that 10 bits of transmission time are required for each 8 bits of data sent, which eats into the data rate. Two, and only two devices: bus contention (where two devices attempt to drive the same line at the same time) is always an issue and must be dealt with carefully to prevent damage to the devices in question, usually through external hardware. Data rate - only a fixed number of baud rates available, highest of which is usually 230400 bits per second. SPI: Number of pins needed: four lines for a single master -\u0026gt; slave connection and each additional slave needs another chip select I/O pin on the master. A larger number of devices rapidly proliferates the connections - routing signals sucks, sometimes impossible in tight PCB situations. It\u0026rsquo;s pretty good for high data rate full-duplex connections (10MHz - bits), scales nicely. The hardware is pretty simple - a shift register, easy software implementation. I2C I2C requires a mere two wires, like asynchronous serial, but those two wires can support up to 1008 slave devices. Unlike SPI, I2C can support a multi-master system, allowing more than one master to communicate with all devices on the bus (although the master devices can\u0026rsquo;t talk to each other over the bus and must take turns using the bus lines). Data rates fall between asynchronous serial and SPI; most I2C devices can communicate at 100kHz or 400kHz. There is some overhead with I2C; for every 8 bits of data to be sent, one extra bit of meta data (the \u0026ldquo;ACK/NACK\u0026rdquo; bit) must be transmitted. The hardware required to implement I2C is more complex than SPI, but less than asynchronous serial. It can be fairly trivially implemented in software. I2C Signal basics Each I2C bus consists of two signals: SCL and SDA.\nSCL is the clock signal, and SDA is the data signal. The clock signal is always generated by the current bus master; some slave devices may force the clock low at times to delay the master sending more data (or to require more time to prepare data before the master attempts to clock it out). This is called \u0026ldquo;clock stretching\u0026rdquo;. PWM basics PWM is a digital (i.e. square wave) signal that oscillates according to a given frequency and duty cycle.\nThe frequency (expressed in Hz) describes how often the output pulse repeats. The period is the time each cycle takes and is the inverse of frequency. The duty cycle (expressed as a percentage) describes the width of the pulse within that frequency window. You can adjust the duty cycle to increase or decrease the average \u0026ldquo;on\u0026rdquo; time of the signal. The following diagram shows pulse trains at 0%, 25%, and 100% duty:\nNote: Most PWM hardware has to toggle at least once per cycle, so even duty values of 0% and 100% will have a small transition at the beginning of each cycle.\nExample: LED Brightness 20% duty cycle at 100 Hz or above will just look dimmer than fully on. PCA9865 overview Adjustable frequency PWM up to about 1.6 KHz.\n12-bit resolution for each output: for servos, that means about 4us resolution at 60Hz update rate, 4096 levels.\nMultiple Drivers (up to 62) can be chained to control still more servos. With headers at both ends of the board, the wiring is as simple as connecting a 6-pin parallel cable from one board to the next.\nBoard 0: Address = 0x40 Offset = binary 00000 (no jumpers required)\nBoard 1: Address = 0x41 Offset = binary 00001 (bridge A0)\nBoard 2: Address = 0x42 Offset = binary 00010 (bridge A1)\nBoard 3: Address = 0x43 Offset = binary 00011 (bridge A0 \u0026amp; A1)\nBoard 4: Address = 0x44 Offset = binary 00100 (bridge A2)\nBoard 5: \u0026hellip;\nPWM Controlled Servo basics PWM signals go into the signal demodulation circuit through the receiving channel, so to generate a DC bias voltage. It will then be compared with the voltage of the potentiometer, and thus a voltage gap is obtained and input into the motor driver IC to drive the motors to rotate clockwise or anticlockwise. When the speed reaches to a certain number, it will drive the potentiometer R to rotate by the cascaded reduction gear, until the gap is reduced to 0 and the servo stops spinning. A servo is controlled by PWM signals, and the change of duty cycle control that of the position the servo rotates to. A typical servo motor expects to be updated every 20 ms with a pulse between 1 ms and 2 ms, or in other words, between a 5 and 10% duty cycle on a 50 Hz waveform.\nThe period of 20 ms (50 Hz) comes from the days where the signal was encoded in PPM format to be sent over the air.\nThe PPM period was around 22.5 ms, and the conversion to PWM was trivial: the time of the PWM high state was the time position of the PPM pulse for that servo.\nRC Servo basics Modern RC servo position is not defined by the PWM duty cycle (ON/OFF time) but only by the width of the pulse.\nThe frequency doesn\u0026rsquo;t matter as long as it is between 40 Hz and 200 Hz.\nTypically expects around 4.8V to 6V input on the power wire (varies by car) and a PWM control signal on the signal wire.\nThree wires are colored black-red-white, or brown-red-yellow, where the dark wire (black/brown) is ground, and the center wire (red) is power, and the light wire (white/yellow) is control.\nRC-style PWM:\nOne pulse is sent 60 times a second, and the width of this pulse controls how left/right the servo turns. 1500 microseconds - the servo is centered; 1000 microseconds - the servo is turned all the way left (or right); 2000 microseconds - the servo is turned all the way in the other direction RC ESC basics Pulse between 1000 and 2000 microseconds Controls the power to the motor so the motor spins with different amounts of power in forward or reverse. Again, 1500 microseconds typically means \u0026ldquo;center\u0026rdquo; which for the motor means \u0026ldquo;dead stop.\u0026rdquo; My schematic: Note to self: It is not a good idea to use the Jetson 5V pin to power your PCA9865. Electrical noise and \u0026lsquo;brownouts\u0026rsquo; from excess current draw can cause the Nano to act erratically, reset and/or overheat. The PCA9685 should get its own, separate power supply.\nConnecting the PCA9865 to the Nano We\u0026rsquo;ll be using the SCL, SDA I2C pins from the PCA9865, along with the GND and VCC pins for power:\nTo connect them to the Nano, we first have to find out which of the Nano pins on its J41 header we should use. Luckily, the Jetson Nano J41 header has the same pinout as the Raspberry Pi, which you can see here, or just use the TL;DR below:\n3V3: labeled red, position 1 and 17, connect PCA VCC pin to either of them GND: labeled black, all over the place, connect PCA GND pin to any of them I2C busses: labeled yellow, there are two of them: Bus 0: position 27 (SDA) and position 28 (SCL) Bus 1: position 3 (SDA) and position 5 (SCL) Connect the PCA SDA and SCL pins to either bus Note: the SDA/SCL pins come in pairs, you have to choose a pair/bus which you\u0026rsquo;ll use I\u0026rsquo;ve connected mine to the first bus (Bus 0):\nPCA -\u0026gt; Nano: 3V3 -\u0026gt; 1 SDA -\u0026gt; 3 SCL -\u0026gt; 5 GND -\u0026gt; 6 The hardware part is done! "
},
{
	"uri": "http://localhost:1313/software/setting-up-donkeycar-on-the-rc/",
	"title": "DonkeyCar configuration: RC car",
	"tags": [],
	"description": "",
	"content": "\rFrom now until the end of this chapter, I\u0026rsquo;ll assume you\u0026rsquo;re working on your car via SSH.\nCreating a DonkeyCar application First, we\u0026rsquo;ll run the createcar command, which will create a new directory with all of the files needed to run and train our RC.\nCommand usage from the docs:\ndonkey createcar --path \u0026lt;dir\u0026gt; [--overwrite] [--template \u0026lt;donkey2\u0026gt;] Run the following command to create a new donkeycar application:\ndonkey createcar --path ~/mycar Open the newly created directory:\ncd ~/mycar Configuring the DonkeyCar application We can find a bunch of settings for the application we just created in a file called myconfig.py. We\u0026rsquo;ll have to edit this file to make our RC work as intended.\nOpen up the myconfig.py file in a text editor:\nnano myconfig.py Note: Some settings are commented out by default (read: have a # at the beginning of the line). Whenever we\u0026rsquo;re filling in a value of some setting, you should uncomment it first/remove the # at the beginning.\nCamera settings: If you\u0026rsquo;re using Nano as your text editor, you can press CTRL+W and type in CAMERA and press enter to jump to the camera settings.\nIf you\u0026rsquo;re using Vim, you probably don\u0026rsquo;t need me to tell you how to search for stuff, but I\u0026rsquo;ll do it anyways: type in /CAMERA to search forward for the camera settings, and use n and N to go back and forth between matches.\nHere are the settings:\n# #CAMERA CAMERA_TYPE = \u0026#34;CVCAM\u0026#34; # (PICAM|WEBCAM|CVCAM|CSIC|V4L|MOCK) IMAGE_W = 1280 IMAGE_H = 720 IMAGE_DEPTH = 3 The different types of camera are:\nPICAM: the standard OV5647 Raspberry Pi camera CSIC: the Sony IMX219 Raspberry Pi camera v2+ CVCAM: a USB camera connected to the Nano, I\u0026rsquo;m using this for my GoPro clone WEBCAM: also for USB cameras, but this requires further setup since it uses pygame V4L: Video4Linux MOCK: A fake camera that outputs a static frame. If your image is flipped, or you want to mount your camera in a rotated position, use:\nCSIC_CAM_GSTREAMER_FLIP_PARM: for flipping your camera output Example: to flip your image vertically, use CSIC_CAM_GSTREAMER_FLIP_PARM = 3 Also, you can specify your resolution/image depth here:\nIMAGE_W: image width in pixels IMAGE_H: image height in pixels IMAGE_DEPTH: number of channels, 3 for RGB/BGR PCA9685 settings: We also need to specify which bus we\u0026rsquo;ve connected our PCA9685 to.\nSee this to refresh your memory:\nIf you used pins 3 and 5 on your Nano, then you\u0026rsquo;re connected to Bus 1. If you used pins 27 and 28, you\u0026rsquo;re connected to Bus 0. We\u0026rsquo;ll also need the I2C address our PCA is connected to, for which we\u0026rsquo;ll need to:\n# Add our user to the i2c group sudo usermod -aG i2c YOUR-USERNAME # Reboot so it takes effect sudo reboot After rebooting, type:\n# i2c-tools should come preinstalled with your Nano, but in case they aren\u0026#39;t: # sudo apt install i2c-tools # For Bus 0: use 0 # For Bus 1: use 1 sudo i2cdetect -r -y 1 If your PCA is wired/connected correctly to the Nano, you should get something like:\n0 1 2 3 4 5 6 7 8 9 a b c d e f 00: -- -- -- -- -- -- -- -- -- -- -- -- -- 10: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 20: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 30: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 40: 40 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 50: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 60: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 70: 70 -- -- -- -- -- -- -- The results explained:\n40 and 70: The addresses of our device. We\u0026rsquo;ll be using the first one, 40, to address it.\n\u0026ndash;: the address was probed, no chip was found\nUU: probing was skipped because the address is currently in use\nIf you\u0026rsquo;re getting jibberish or all of the addresses are filled, check your connections, you probably swapped SDA and SCL or plugged them into the wrong pins.\nFinally, in the myconfig.py file, find and fill the following:\nPCA9685_I2C_ADDR = 0x40 PCA9685_I2C_BUSNUM = 1 PCA9685_I2C_ADDR: the I2C address of our PCA, in hex format (0xNumber) PCA9685_I2C_BUSNUM: the I2C bus our PCA is connected to Steering and throttle channels: We also need to specify to which PCA channels our steering servo and our ESC are connected to.\nSee this to refresh your memory or just look at the numbers on the PCA, above the places you\u0026rsquo;ve connected your servo and ESC connectors to, they correspond to their respective channel numbers.\nIn the settings, find and fill out the steering:\n# #STEERING STEERING_CHANNEL = 1 #channel on the 9685 pwm board 0-15 And the throttle channel values:\n# #THROTTLE THROTTLE_CHANNEL = 0 #channel on the 9685 pwm board 0-15 In my case, the throttle/ESC is the zeroth channel and the steering/servo is the first channel.\nWe will also need to fill out the following values for steering:\nSTEERING_LEFT_PWM = ??? #pwm value for full left steering STEERING_RIGHT_PWM = ??? #pwm value for full right steering And throttle:\nTHROTTLE_FORWARD_PWM = ??? #pwm value for max forward throttle THROTTLE_STOPPED_PWM = ??? #pwm value for no movement THROTTLE_REVERSE_PWM = ??? #pwm value for max reverse throttle To do so, we\u0026rsquo;ll need to calibrate our steering and throttle first, and then come back with the values we\u0026rsquo;ve found and fill them in.\nFeel free to save the changes you\u0026rsquo;ve made to the myconfig.py file so far:\nNano: CTRL+O and Enter/Return to save, CTRL+X to close the editor Vim: :wq! :) "
},
{
	"uri": "http://localhost:1313/software/calibrating-steering-and-throttle/",
	"title": "Calibrating steering and throttle",
	"tags": [],
	"description": "",
	"content": "\rMake sure your car wheels are not touching the ground. Prop it up using a shoebox, or in my case, an eGPU dock. We will be calibrating the throttle which means your car will start accelerating very fast, without warning, so you wouldn\u0026rsquo;t want it slamming into a wall at full throttle.\nCalibrating the throttle: First, you\u0026rsquo;ll need to turn on your car; the actual RC, not the Nano.\nDepending on your RC, it\u0026rsquo;ll probably beep because it\u0026rsquo;s turned on and the controller isn\u0026rsquo;t connected to it, but don\u0026rsquo;t worry, it\u0026rsquo;ll stop once we connect to it via the I2C bus.\nRun the following:\ndonkey calibrate --channel YOUR_ESC\\THROTTLE_CHANNEL --bus=YOUR_I2C_PCA_BUS The output should look like:\nusing donkey v3.1.1 ... init PCA9685 on channel 0 address 0x40 bus 0 Using PWM freq: 60 Enter a PWM setting to test (\u0026#39;q\u0026#39; for quit) (0-1500): If you\u0026rsquo;re using the wrong bus, you\u0026rsquo;ll probably get a OSError: [Errno 121] Remote I/O error\nType in 370 and press enter. The ESC should stop beeping, indicating it is calibrated to the neutral throttle position.\nTo detect your forward values for the throttle:\nTry entering 400 and seeing if your car starts throttling forwards. If it\u0026rsquo;s not, then try entering 330 instead. After finding out which direction forwards is: Number larger than 370 Number smaller than 370 Start from 370 and enter values +/- 10 and choose a value that you want your maximum forward throttle to be and write it down. To detect your reverse values for the throttle:\nIn order to go in reverse, the RC needs to get a reverse throttle value, followed by the neutral throttle value, followed by the reverse throttle value again.\nIf your forward values were larger than 370:\nEnter 330, then 370, then 330. If your forward values were smaller than 370:\nEnter 400, then 370, then 400. After confirming how to get your car in reverse throttle, once again start from 370 and enter values +/- 10 and choose a value that you\u0026rsquo;d like your max reverse throttle to be, and write it down.\nYou can now enter q to finish the throttle calibration procedure.\nCalibrating the steering Once again, run the following:\rdonkey calibrate --channel YOUR_STEERING_CHANNEL --bus=YOUR_I2C_PCA_BUS Enter 360, and you should see your car slightly steering. If you don\u0026rsquo;t, try 350 or 370. Once again, enter +/- 10 values to find out what values steer your car completely to the left and to the right and write them down. You can now enter q to finish the steering calibration procedure.\nEntering the values in the config file Open up the myconfig.py in the editor of your choice:\nnano ~/mycar/myconfig.py Find the THROTTLE and STEERING sections and enter the values you wrote down during calibration:\nSTEERING_RIGHT_PWM: the value which steers your car completely to the right. STEERING_LEFT_PWM: the value which steers your car completely to the right. THROTTLE_STOPPED_PWM: the neutral throttle value. THROTTLE_FORWARD_PWM: the maximum forward throttle value. THROTTLE_REVERSE_PWM: the maximum reverse throttle value. Save the file and exit the editor and you\u0026rsquo;re done!\n"
},
{
	"uri": "http://localhost:1313/software/connecting-a-bluetooth-gamepad/",
	"title": "Using a gamepad",
	"tags": [],
	"description": "",
	"content": "\nYou can, and should, use a gamepad to control your RC. It\u0026rsquo;s much easier to generate good training data using a gamepad, and it\u0026rsquo;s much easier to drive the thing compared to the Web interface that Donkey provides.\nSo how do we connect and use one?\nCompatible controllers First, let\u0026rsquo;s make sure you have one that\u0026rsquo;ll actually work\nThe official Donkey docs list that the following are known to work:\nLogitech Gamepad F710 Sony PS3 Dualshock OEM Sony PS3 Sixaxis OEM (Not compatible with Jetson Nano) Sony PS4 Dualshock OEM WiiU Pro XBox Controller SteelSeries Nimbus (works only on TX2 jetpack 4.2+, may work on the Nano) Depending on the controller you\u0026rsquo;re going to use, open up the myconfig.py file and find the # JOYSTICK section and uncomment and fill the following (using one of the types suggested inline with the setting):\nCONTROLLER_TYPE=\u0026#39;xbox\u0026#39; #(ps3|ps4|xbox|nimbus|wiiu|F710|rc3) If your gamepad isn\u0026rsquo;t officially supported, try following the rest of the tutorial and connecting it to the Nano. If it shows up as /dev/input/js0, you should be able to use it, since the Donkey platform just uses that OS device mount as the gamepad.\nConnecting your controller Xbox (One) Controller The Xbox controller doesn\u0026rsquo;t like the Enhanced Re-Transmission Mode, so we\u0026rsquo;ll disable it first.\nOpen up the /etc/modprobe.d/xbox_bt.conf file (this may actually create the file if it doesn\u0026rsquo;t exist yet):\nsudo nano /etc/modprobe.d/xbox_bt.conf # Add this line and save the file options bluetooth disable_ertm=1 # Reboot sudo reboot After rebooting, check it\u0026rsquo;s actually disabled by running:\ncat /sys/module/bluetooth/parameters/disable_ertm It should say \u0026lsquo;Y\u0026rsquo;. If it does, open up bluetoothctl:\nsudo bluetoothctl # Register the default agent\ragent on\r# Default agent request\rdefault-agent\r# Scan for devices\rscan on Turn on your Xbox controller (by pressing the big Xbox logo button) and start the pairing mode by pressing the sync button on the back of the controller, next to the microUSB port.\nYou should see your controller show up in bluetoothctl similar to:\n[NEW] Device YO:UR:MA:CA:DD:RS XBox One Wireless Controller Type in:\n# This may take a few tries ... connect YOUR_MAC_ADDRESS # If it connects but immediately disconnects, the disable_ertm setting isn\u0026#39;t properly set up, try doing that again # When it connects and the big Xbox logo button is solid white: trust YOUR_MAC_ADDRESS quit Once connected, it should automatically reconnect anytime the Jetson and it are both powered on. If it doesn\u0026rsquo;t, you\u0026rsquo;ll have to run the steps above again.\nPS4 Controller Install ds4drv:\nsudo pip install ds4drv Grant it permissions:\nsudo wget https://raw.githubusercontent.com/chrippa/ds4drv/master/udev/50-ds4drv.rules -O /etc/udev/rules.d/50-ds4drv.rules sudo udevadm control --reload-rules sudo udevadm trigger sudo reboot After rebooting run ds4drv:\n# --led changes the light bar color, you can modify it or not use it at all # --hidraw doesn\u0026#39;t play well with some controllers, if you\u0026#39;re having issues, try not using it ds4drv --hidraw --led 00ff00 Start the controller in pairing mode:\nPress and hold Share button Press and hold PS button until the light bar starts blinking If it goes green after a few seconds, pairing is successful. To run ds4drv on boot, open up /etc/rc.local and paste in the command you used to start ds4drv:\nsudo nano /etc/rc.local # Paste in (with or without --hidraw and --led): /home/YOUR-USERNAME/env/bin/ds4drv --led 00ff00 PS3 Controller The Donkey docs say to follow this guide, or you can just run:\nsudo apt-get install bluetooth libbluetooth3 libusb-dev sudo systemctl enable bluetooth.service sudo usermod -G bluetooth -a pi sudo reboot After rebooting, plug in the controller with an USB cable, press the PS button and:\nwget http://www.pabr.org/sixlinux/sixpair.c gcc -o sixpair sixpair.c -lusb sudo ./sixpair Run bluetoothctl as a super user and pair your controller:\nsudo bluetoothctl # Enable the default agent agent on # List all found devices and get your controller\u0026#39;s MAC address devices # Trust your controller trust \u0026lt;MAC ADDRESS\u0026gt; # Default agent request default-agent # Quit quit Unplug the USB cable and press the PS button. Your controller should be mounted at ls /dev/input/js0.\nAny other gamepad/joystick/controller If it\u0026rsquo;s a Bluetooth one, try pairing it via bluetoothctl and see if it mounts at /dev/input/js0. If it does, great, you can move on.\nIf not, try connecting it via USB and see if it mounts at /dev/input/js0, if it does, you\u0026rsquo;re good.\nIf neither goes, try searching how to connect that particular controller to a Linux device online, if there\u0026rsquo;s a way (mostly there is), it should mount at the above mentioned js0 and you should be able to follow along.\nCreating a Button/Axis mapping: After your controller is connected, run:\ndonkey createjs The output should be:\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~## ## Welcome to Joystick Creator Wizard. ## ##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~## This will generate code to use your joystick with a Donkey car. Overview: First we name each button, then each axis control. Next we map names to actions. Finally we output a python file you can use in your project. Hit Enter to continue # After pressing Enter Please plug-in your controller via USB or bluetooth. Make sure status lights are on and device is mapped. Enter to continue # You can press enter since we\u0026#39;ve already gone through connecting your controller Where can we find the device file for your joystick? Hit Enter for default: /dev/input/js0 or type alternate path: # Should be /dev/input/js0 if it\u0026#39;s a PS3/PS4/Xbox/Standard USB one # Hit enter and see if it finds it (it will if its on /dev/input/js0) Attempting to open device at that file... Opening /dev/input/js0... Device name: The type of Controller you\u0026#39;re using Found and accessed input device. # After pressing Enter Next we are going to look for gyroscope data. For 5 seconds, move controller and rotate on each axis. Hit Enter then start moving: # You can skip this, since you most probably won\u0026#39;t be using it Ok, we didn\u0026#39;t see any events. So perhaps your controller doesn\u0026#39;t emit gyroscope data. No problem. Finally, you\u0026rsquo;ll get to this part:\nWe will display the current progress in this set of tables: Button Map: +-------------+-------------+ | button code | button name | +-------------+-------------+ +-------------+-------------+ Axis Map: +-----------+-----------+ | axis code | axis name | +-----------+-----------+ +-----------+-----------+ Control Map: +---------+--------+ | control | action | +---------+--------+ +---------+--------+ As you name buttons and map them to controls this table will be updated. While in here, you should go through pressing every button on your controller, and giving it a name.\nNote: during this first step, you can only map buttons, the axes, such as triggers or analog sticks will come next.\nFor my Xbox One Controller, I made the following button map:\nButton Map: +-------------+-------------+ | button code | button name | +-------------+-------------+ | 0x130 | A | | 0x131 | B | | 0x133 | Y | | 0x132 | X | | 0x137 | Start | | 0x136 | Select | | 0x135 | R1 | | 0x134 | L1 | | 0x139 | RS | | 0x138 | LS | +-------------+-------------+ After mapping all the buttons, wait for 10 seconds and enter Y when the program asks you if you\u0026rsquo;ve finished mapping all the buttons.\nThis is what my axis map looks like:\nAxis Map: +-----------+-------------------------+ | axis code | axis name | +-----------+-------------------------+ | 0x5 | RT | | 0x2 | LT | | 0x0 | Left Stick: Horizontal | | 0x1 | Left Stick: Vertical | | 0x4 | Right Stick: Vertical | | 0x3 | Right Stick: Horizontal | +-----------+-------------------------+ You can enter D when you\u0026rsquo;re done and move on to the control map, through which we\u0026rsquo;ll map buttons and axes to specific car controls.\nThis is what my control map looks like:\nControl Map: +------------------------+--------------------------+ | control | action | +------------------------+--------------------------+ | Start | toggle_mode | | Select | erase_last_N_records | | A | emergency_stop | | R1 | increase_max_throttle | | L1 | decrease_max_throttle | | B | toggle_constant_throttle | | X | toggle_manual_recording | | Left Stick: Horizontal | set_steering | | Right Stick: Vertical | set_throttle | +------------------------+--------------------------+ If you\u0026rsquo;ve messed something up, don\u0026rsquo;t worry, at the next menu you can go back to any step you\u0026rsquo;d like:\nNow we are nearly done! Are you happy with this config or would you like to revisit a topic? H)appy, please continue to write out python file. B)uttons need renaming. A)xes need renaming. T)hrottle and steering need remap. R)emap buttons to controls. If you\u0026rsquo;re happy with your maps, enter H and it will prompt you for a name under which to save your mapping. The default one is my_joystick.py, but you can enter a custom one, if you\u0026rsquo;re planning to use multiple different controllers, or just for non-generic-naming\u0026rsquo;s sake:\nNow we will write these values to a new python file. What is the name of python file to create joystick code? [default: my_joystick.py] xbox_one_controller.py It will then ask what to name the custom Python class of the controller you\u0026rsquo;ve just created:\nWhat is the name of joystick class? [default: MyJoystick] XboxOneController xbox_one_controller.py written. Check your new python file to see the controller implementation. Import this in manage.py and use for control. Almost there, we just need to import our custom mapping in the manage.py script to be able to use it with our RC.\nOpen up manage.py (nano or vim) and at the end of the imports, find the following line:\nfrom donkeycar.parts.controller import get_js_controller ctr = get_js_controller(cfg) And replace it with:\n# This assumes you haven\u0026#39;t changed the default names from my_joystick import MyJoystick ctr = MyJoystick(throttle_dir=cfg.JOYSTICK_THROTTLE_DIR, throttle_scale=cfg.JOYSTICK_MAX_THROTTLE, steering_scale=cfg.JOYSTICK_STEERING_SCALE, auto_record_on_throttle=cfg.AUTO_RECORD_ON_THROTTLE) ctr.set_deadzone(cfg.JOYSTICK_DEADZONE) Or if you\u0026rsquo;ve defined a custom name for the python file containing your mapping, and the class it contains, which I did, then modify the import line so it uses your custom script and class name:\n# In my case from xbox_one_controller import XboxOneController ctr = XboxOneController(throttle_dir=cfg.JOYSTICK_THROTTLE_DIR, throttle_scale=cfg.JOYSTICK_MAX_THROTTLE, steering_scale=cfg.JOYSTICK_STEERING_SCALE, auto_record_on_throttle=cfg.AUTO_RECORD_ON_THROTTLE) ctr.set_deadzone(cfg.JOYSTICK_DEADZONE) And you\u0026rsquo;re done! Now we can start actually driving the car using our controller!\n"
},
{
	"uri": "http://localhost:1313/software/test-driving-the-rc/",
	"title": "Test drive",
	"tags": [],
	"description": "",
	"content": "First of all: congrats on getting this far! Let\u0026rsquo;s spin our RC for a ride.\nBefore continuing:\nMake sure your RC is powered up (not the Jetson Nano, the actual RC). Make sure that the camera is connected and powered up (if you\u0026rsquo;re using a USB camera). Make sure that your RC has enough space around it, depending on what throttle values you\u0026rsquo;ve defined. Test drive using a gamepad Change to the directory you\u0026rsquo;ve created with the donkey createcar command:\ncd ~/mycar Start the manage.py script with the drive command and the --js flag, to control your RC with your gamepad:\npython manage.py drive --js The output should look like:\nusing donkey v3.1.1 ... loading config file: /home/your_username/mycar/config.py loading personal config over-rides config loaded cfg.CAMERA_TYPE CVCAM cfg.CAMERA_TYPE CVCAM Adding part CvCam. Adding part XboxOneJoystickController. Adding part ThrottleFilter. Adding part PilotCondition. Adding part RecordTracker. Adding part ImgPreProcess. Adding part DriveMode. Adding part AiLaunch. Adding part AiRunCondition. Init ESC Adding part PWMSteering. Adding part PWMThrottle. Tub does NOT exist. Creating new tub... New tub created at: /home/your_username/mycar/data/tub_15_20-01-02 Adding part TubWriter. You can now move your joystick to drive your car. It\u0026rsquo;ll also show you your controller mapping and the controller name:\nJoystick Controls: +------------------+---------------------------+ | control | action | +------------------+---------------------------+ | a_button | toggle_mode | | b_button | toggle_manual_recording | | x_button | erase_last_N_records | | y_button | emergency_stop | | right_shoulder | increase_max_throttle | | left_shoulder | decrease_max_throttle | | options | toggle_constant_throttle | | circle | show_record_acount_status | | R2 | enable_ai_launch | | left_stick_horz | set_steering | | right_stick_vert | set_throttle | | right_trigger | set_magnitude | | left_trigger | set_magnitude | +------------------+---------------------------+ Opening /dev/input/js0... Starting vehicle... Device name: Xbox Wireless Controller You can also see it mentioning something called a *tub*, and if you drive your RC around, it'll say that it's making records:\rStarting vehicle... recorded 10 records recorded 20 records recorded 30 records recorded 40 records recorded 50 records recorded 60 records recorded 70 records recorded 80 records recorded 90 records recorded 100 records ... A tub is a folder containing data from a drive. It\u0026rsquo;s located in the data directory inside the mycar directory. The data is made up of camera images (JPEGs), along with steering and throttle values (JSONs).\nWe\u0026rsquo;ll use this data later on to train our models.\nIf you make a mistake while driving, you can delete the last 100 records by using the button you\u0026rsquo;ve mapped to the erase_last_N_records control.\nTest drive using the Web interface The Web interface can sometimes be a bit simpler to use, since it allows you to see the camera output and has a nice GUI for all of the autopilot settings. Let\u0026rsquo;s try it out.\nFirst, change to the directory you\u0026rsquo;ve created with the donkey createcar command:\ncd ~/mycar Start the manage.py script with the drive command:\npython manage.py drive If all went well, the end of the output should say:\nYou can now go to \u0026lt;your pis hostname.local\u0026gt;:8887 to drive your car. Starting vehicle... 8887 You can now open up your browser, and enter your Jetson Nano\u0026rsquo;s IP with the port 8887 to control your car:\n192.168.x.xxx:8887 Before starting up your RC, set the Max Throttle value to 50% or less. By default it will allow your RC to go as fast as it can, regardless of the values you\u0026rsquo;ve set in myconfig.py. A lot of RC\u0026rsquo;s can easily go over 50MPH/80KPH, so this could cause a world of pain.\nYou should be seeing something like this:\nLet\u0026rsquo;s break it down:\nThe Control Mode has three options: Joystick: controls the RC via the blue touch/click area on the screen. You can also use I/K/J/L as forward/reverse/left/right controls. Gamepad: uses a gamepad connected to the device that\u0026rsquo;s browsing the page. Haven\u0026rsquo;t tried this one yet, it seems it only works on Linux with some controllers. Device Tilt: intended for smartphones browsing the site, you tilt the phone to control the car. Max Throttle: defines the maximum throttle the vehicle can achieve when the user presses full forward throttle via the controls. Throttle Mode has two options: User: where the user gives the throttle manually via the controls. Constant (Selected Max) which constantly keeps the throttle at the value defined in the Max Throttle dropdown. Angle \u0026amp; Throttle show the current values of throttle and steering. Useful when looking at how your autopilot drives. Mode \u0026amp; Pilot has three modes: User: where the user manually controls both the steering and throttle. Local Pilot: where the autopilot/model controls both the steering and throttle. Local Angle: where the autopilot controls the steering, but the user controls the throttle. Start Recording: toggles data recording. Start Vehicle starts the vehicle if it\u0026rsquo;s in autopilot mode. A quick note if you\u0026rsquo;re using an USB camera and it\u0026rsquo;s showing a blue-ish picture. The default color space for the CVCAM is BGR, since it\u0026rsquo;s an OpenCV camera. Don\u0026rsquo;t worry about it, it makes no practical difference for your model.\nIf, however, you insist on seeing the CVCAM output in RGB, open up the cv.py script, found in your projects folder, inside the donkeycar/donkeycar/parts directory:\nnano ~/projects/donkeycar/donkeycar/parts/cv.py Find the CvCam class, and:\nAdd an instance of the ImgBGR2RGB class to a class variable Call the run() method of the ImgBGR2RGB instance any time the class returns an image frame The resulting CvCam class code should look like this:\nclass CvCam(object): def __init__(self, image_w=1280, image_h=720, image_d=3, iCam=0): # Added an instance of the converter class self.imgConverter = ImgBGR2RGB() self.frame = None self.cap = cv2.VideoCapture(iCam) self.running = True self.cap.set(3, image_w) self.cap.set(4, image_h) def poll(self): if self.cap.isOpened(): ret, self.frame = self.cap.read() def update(self): while(self.running): self.poll() # poll the camera for a frame def run_threaded(self): return self.imgConverter.run(self.frame) # Convert the image to RGB def run(self): self.poll() return self.imgConverter.run(self.frame) # Convert the image to RGB def shutdown(self): self.running = False time.sleep(0.2) self.cap.release() Now you should be seeing your camera output in RGB. But as I said, it doesn\u0026rsquo;t really matter to the model.\n"
},
{
	"uri": "http://localhost:1313/software/sanity-check---first-autopilot/",
	"title": "First Autopilot: sanity check",
	"tags": [],
	"description": "",
	"content": "\nAfter doing a bunch of work like we just did, it\u0026rsquo;s always important to periodically check that everything works as intended, before moving on to even more complex stuff.\nSo that\u0026rsquo;s what we\u0026rsquo;ll be doing.\nBuilding a test track First, you need to build a test track. For this sanity checking, I wouldn\u0026rsquo;t do anything over the top. Just take some duct-tape and make a circular track that\u0026rsquo;s easy to drive around.\nThis is what mine looked like:\nYou can also make something more fancy, like this, but it isn\u0026rsquo;t neccessary:\nCollecting the training data After your RC is on the track, connect your gamepad to it and from the ~/mycar directory run:\npython manage.py drive --js Now, try driving one lap around the track. It should be collecting records. If it is, you\u0026rsquo;re good to go.\nYou should now learn how to drive around the test track without making any errors. After you're confident in your driving abilities, you should stop the `manage.py` script, position your car on the track and start it up again.\rTo get your training data:\nDrive the RC for at least 10 laps, without any errors.\nIf you make a mistake while driving, press the Y/Triangle button to erase the last 5 seconds of the drive, put your car back on the track and continue driving. Stop the manage.py script.\nScroll up to see in which tub the data was saved:\n# The line you\u0026#39;re looking for is: New tub created at: /home/your_username/mycar/data/tub_**_**-**-** Copy the name of the tub\nTransferring the data to your host PC Now we just need to transfer our training data to our host PC:\nMac OS X / Linux: (use the tub name you copied)\nrsync -r username@your_nano_IP_address:~/mycar/data/tub_**_**-**-** ~/mycar/data/ Windows (MobaXTerm):\nMobaXTerm has a built in SFTP viewer to the left of your terminal To copy your data, go to: /home/your_username/mycar/data/ using the SFTP viewer Find the tub with the same name you\u0026rsquo;ve copied Right click on it, and click Download Save the files to the mycar directory on your host PC (should be at C:\\Users\\your_username\\mycar\\) Training your first model As mentioned previously, Donkey comes with a number of different neural network architectures you can use to train your autopilot. You can find them at the directory where you've cloned the donkey repository, inside the `parts\\keras.py` script.\rThe usual location, if you\u0026rsquo;ve followed the tutorials and used the suggested directory, would be:\n# Linux/Mac OS X ~/projects/donkeycar/donkeycar/parts/keras.py # Windows C:\\Users\\your_username\\projects\\donkeycar\\donkeycar\\parts\\keras.py We\u0026rsquo;ll be using one of the simplest ones, the linear model.\nYou can see a list of all the architectures explained here.\nTo train your first autopilot model, go to the mycar directory on your host PC, and run:\npython ~/mycar/manage.py train --model ~/mycar/models/firstAutopilot.h5 --type=linear This will use all of the tubs it can find in your data folder.\nIf you\u0026rsquo;d like to specify specific tubs Donkey should use to train your model, you can run:\npython ~/mycar/manage.py train --tub /path/to/the/tub,/path/to/other/tub --model ~/mycar/models/theNameOfTheModel.h5 --type=architectureName Notice that you can either separate specific tub paths by using a comma, or you can use a mask, e.g.: /somePath/tubs/tub-20* to train all tubs starting with the name tub-20.\nFeel free to train a number of models using different architectures to see how they work. I\u0026rsquo;d recommend trying the linear, rnn and 3d architectures.\nMoving the trained model back to your RC: After the training script has finished, we just need to move our model(s) back to our RC in order to use it.\nLinux/Mac OS X:\nrsync -r ~/mycar/models/ username@your_nano_IP_address:~/mycar/models/ Windows (MobaXTerm):\nNavigate to the/home/mycar/models/ on the Nano, using the built-in SFTP viewer and just drag and drop all of the models (.h5 files) you\u0026rsquo;ve trained from your host PC to the Nano. Let the autopilot drive by itself After training and moving your models to your RC, let the model control the RC by running:\npython manage.py drive --model ~/mycar/models/yourModel.h5 Go to the Web interface by opening up a browser and entering yourNanoIP:8887 into the address bar.\nI\u0026rsquo;d recommend setting the Mode \u0026amp; Pilot to Local Angle, starting the vehicle and either giving the throttle by yourself, using the blue touch/click area on the right, or by giving it a constant throttle by setting the Max Throttle to 50% and setting the Throttle Mode to Constant.\nAfter making sure your model doesn\u0026rsquo;t just ram your RC into a wall, you can use the Local Pilot mode and it\u0026rsquo;ll control the throttle by itself.\nWhat\u0026rsquo;s next? The autopilot works, but it\u0026rsquo;s far from perfect. But it\u0026rsquo;s still pretty impressive considering we started out with a regular RC, and now we have a thing that can drive around a track on its own. :)\nThe more important thing is, we now have everything we need in order to make a complex self-driving model. We can automatically collect data by just driving our RC around a track, we have a pipeline established that can take that data, run it through an architecture we\u0026rsquo;ve defined and spit out a model we can just plug our car into and it\u0026rsquo;ll control it via it\u0026rsquo;s output. That\u0026rsquo;s pretty sweet.\nNow we can go onto the most interesting part of this project, the machine (and deep) learning part.\n"
},
{
	"uri": "http://localhost:1313/",
	"title": "Home Page",
	"tags": [],
	"description": "",
	"content": "Hello there! My name is Ivan, and I built a self-driving RC Car for my Master\u0026rsquo;s thesis. This website is a guide meant for everyone that would like to build a self-driving RC car model. Even if you don\u0026rsquo;t have/want a physical RC, you can still follow by using the simulator we\u0026rsquo;ll build!\nRead the Master\u0026rsquo;s thesis here and take look at the presentation here.\nI\u0026rsquo;ll go through everything from building your RC car and the low level electronics stuff to hacking kernels and teaching your car how to drive and even do stuff like change lanes using deep learning.\nThis is still a work in progress which gets updated regularly, but there\u0026rsquo;s more than enough material for you to make an RC drive itself. :)\nScroll below for a sneak peek and a couple of recommended posts! ***\r### I'd recommend checking out:\rMy idea of how to teach a vehicle to drive itself: an idea similar to networks like the \u0026quot;HydraNets\u0026quot; Tesla uses (but probably certainly dumber) Master recipe: How to learn your machines: a basic recipe for model training, might be interesting if you\u0026rsquo;re already doing ML/DL stuff Lane changing behaviour and object tracking and recognition: PoCs and demos, I\u0026rsquo;ll publish the latter in a couple of days! ***\rFeel free to ping me at Twitter\ror at my E-mail\rif you want to say hi or ask anything. "
},
{
	"uri": "http://localhost:1313/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]