[
{
	"uri": "https://ori.codes/rc-car/",
	"title": "RC Cars: a primer",
	"tags": [],
	"description": "",
	"content": "Prerequisite knowledge RC Cars: a primer Through this chapter we'll explore what kind of parts is a RC car comprised of, and what are the most important ones for us. We’ll use that knowledge while buying parts and assembling them for our hardware platform.\n"
},
{
	"uri": "https://ori.codes/artificial-intelligence/camera-calibration/camera-distortions/",
	"title": "Camera calibration: Explaining camera distortions",
	"tags": [],
	"description": "",
	"content": "Since we're using only cameras to obtain the entirety of data we'll use to drive our car around the real world, we're sure trusting them a lot. We're trusting that they'll provide us with accurate representations of real world 3D objects as 2D images we'll feed into our neural network.\nBut we shouldn't take that for granted. Cameras, albeit cheap and easy to use, come with all sorts of issues when it comes to mapping the 3D world onto a 2D sensor/image correctly. But luckily, we can do something about it.\nThe pinhole camera model The pinhole camera model is a model of an ideal camera, that describes the mathematical relationship between the real world 3D object's coordinates and its 2D projection on the image plane. 1\nPinhole cameras were the very beginning of photography 2, and are used even today to explain basic photography to students.\nThey posses some advantages over our regular lens cameras:\n They have near infinite depth of field; everything appears in focus. No lenses are used to focus light, so they have no lens distortion and wide-images remain absolutely rectilinear.  Basically, the smaller the pinhole gets, the more the resolution increases, until we reach the diffraction limit, at which point the image just gets darker and blurrier. Also, the smaller the pinhole, less light comes in, so the exposure time needs to be increased. Which brings us to the big issue with them:\n Their exposure times are really long, which causes significant motion blur around any moving objects or it causes their complete absence if they've moved too fast.  How can we get a small pinhole that gets a lot of light? We can use a convex lens, for starters.\nWhy does this help: instead of a single ray of light illuminating a single image point, now pencils of light illuminate each image point. Even our eyes use lenses. :)\nBut of course, lenses bring the issues we've mentioned earlier:\n They have finite aperture so blurring of unfocused objects appears. They contain geometric distortions due to lenses, which increase as we get closer to the edges of the lenses.  Types of distortions The first, and most common type of camera lens distortion is called radial distortion.\nThere are two types of this distortion, the positive or barrel radial distortion, and the negative or pincushion radial distortion.\nIn the context of self driving RCs, you'll most probably be dealing with the barrel distortion, that will most probably be caused by fisheye lenses, since we'd like to get as big a field of view as we can. Some action cams even have a FOV of 170 to 180 degrees, which causes a lot of positive radial distortion.\nThe other type of distortion you can come across is called tangential distortion, which occurs when the lens is not aligned perfectly parallel to the imaging plane (the sensor). It causes the image to look tilted, which is obviously bad for us since some objects look further away than they really are.\nThat being said, we have to expect that no camera is actually really perfect. Most, if not all have some amount of radial and tangential distortion, since the lenses are imperfect in real life, and the lens isn't always perfectly in line with the imaging plane.\nGetting rid of distortions with OpenCV Luckily for us, the radial and tangential distortions can be described using a couple of coefficients:\n $k_n$ coefficients will describe radial distortion $p_n$ coefficients will describe tangential distortion  The worse the distortion, the more coefficients we need to accurately describe it.\nOpenCV works with up to six ($k_1$, $k_2$, $k_3$, $k_4$, $k_5$ and $k_6$) radial distortion coefficients, which should be more than enough for us, and with two ($p_1$, $p_2$) tangential distortion coefficients.\nIf we have the barrel radial distortion type, $k_1$ will typically be larger than zero. If we have the pincushion distortion, $k_1$ will typically be smaller than zero.\nOpenCV uses a pinhole camera model to describe how an image is created by projecting 3D points into the image plane using a perspective transformation:\n\r$$\rs \\begin{bmatrix}{u}\\\\{v}\\\\{1}\\end{bmatrix} = \\begin{bmatrix}{f_x}\u0026{0}\u0026{c_x}\\\\{0}\u0026{f_y}\u0026{c_y}\\\\{0}\u0026{0}\u0026{1}\\end{bmatrix} \\begin{bmatrix} r_{11} \u0026 r_{12} \u0026 r_{13} \u0026 t_1 \\\\ r_{21} \u0026 r_{22} \u0026 r_{23} \u0026 t_2 \\\\ r_{31} \u0026 r_{32} \u0026 r_{33} \u0026 t_3 \\end{bmatrix} \\begin{bmatrix} X \\\\ Y \\\\ Z \\\\ 1 \\end{bmatrix}\r$$\r\r $(X, Y, Z)$ are the coordinates of a 3D point we're imaging $(u,v)$ are the 2D coordinates of the projection point in pixels The first matrix after the equation is the camera matrix, containing intrinsic camera parameters  $(c_x, c_y)$ defines the principle point which is usually the center of the image $f_x$ and $f_y$ are the focal lengths expressed in pixel units   The matrix containing the $r_{mn}$ parameters is the joint rotation-translation matrix, a matrix of extrinsic parameters which describes camera motion around a static scene. It's used to translate the 3D coordinates to a 2D coordinate system, fixed with respect to the camera.  Since we're imaging 2D images, we'd like to map the 3D coordinates to a coordinate system:\n\r$$\r\\begin{bmatrix}{x}\\\\{y}\\\\{z}\\end{bmatrix} = R \\\\\r\\begin{bmatrix}{X}\\\\{Y}\\\\{Z}\\end{bmatrix} + t \\\\\rx' = x/z \\\\\ry' = y/z \\\\\ru = f_x*x' + c_x \\\\\rv = f_y*y' + c_y\r$$\r\rAlso, since we're not using a pinhole camera, we need to add the distortion coefficients to our model:\n\r$$\rx' = x/z \\\\ y' = y/z \\\\ x'' = x' \\frac{1 + k_1 r^2 + k_2 r^4 + k_3 r^6}{1 + k_4 r^2 + k_5 r^4 + k_6 r^6} + 2 p_1 x' y' + p_2 \\\\ y'' = y' \\frac{1 + k_1 r^2 + k_2 r^4 + k_3 r^6}{1 + k_4 r^2 + k_5 r^4 + k_6 r^6} + p_1 (r^2 + 2 y'^2) + 2 p_2 x' y' \\\\ \\text{where} \\quad r^2 = x'^2 + y'^2 \\\\ u = f_x*x'' + c_x \\\\\rv = f_y*y'' + c_y\r$$\r\rSince we're primarily interested in efficiently removing the radial distortion, we'll be using Fitzgibbon's division model as opposed to Brown-Conrady's even-order polynomial model, since it requires fewer terms in cases of severe distortion. It is also a bit easier to work with, since inverting the single parameter division model requires solving a one degree less polynomial than inverting the single-parameter polynomial model. [^Bukhari]\nFinding the camera's intrinsic and extrinsic parameters Now that we've laid out all of the formulas we use to correct radial and tangential distortion, the only question that remains is how do we get the intrinsic and extrinsic parameters.\nFor those purposes, we'll be using the OpenCV calibrateCamera function, along with its findChessboardCorners function.\nThe calibrateCamera function is based on Zhang's A Flexible New Technique for Camera Calibration and Caltech's Camera Calibration Toolbox.\nIt needs the coordinates of a 3D object we're imaging and its corresponding 2D projected coordinates in order to detect the intrinsic and extrinsic parameters of the camera we're using to image the object.\nTo easily get those coordinates, we'll be using a chessboard. A chessboard is an object with a known geometry to us and it has easily detectable feature points. Such objects are called calibration rigs or patterns, and OpenCV has a built-in function that uses a chessboard as a calibration rig, the findChessboardCorners function.\nThe findChessboardCorners function attempts to determine whether the input image is a view of the chessboard pattern and automatically locate the internal chessboard corners.\nThe cool thing with this is that we can print out a 3D object (a chessboard) whose geometry is well known to us, and map its 3D coordinates to our 2D image projection. The 3D points of the chessboard from the real world are called object points and their 2D mappings on our image are called image points.\nSo, we print out a chessboard, take multiple pictures of it from different angles to better capture the camera distortions, and feed them to the findChessboardCorners function. It will give us the detected object points and corresponding image points back to us, which we can use to then calibrate the camera.\nThe calibrateCamera function, given the object points and image points by the findChessboardCorners function, performs the following:\n Computes the initial intrinsic parameters. The distortion coefficients are all set to zeros initially. Estimates the initial camera pose as if the intrinsic parameters have been already known. This is done using [solvePnP()](https://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html#bool solvePnP(InputArray objectPoints, InputArray imagePoints, InputArray cameraMatrix, InputArray distCoeffs, OutputArray rvec, OutputArray tvec, bool useExtrinsicGuess, int flags)) . Runs the global Levenberg-Marquardt optimization algorithm to minimize the reprojection error, that is, the total sum of squared distances between the observed feature points imagePoints and the projected (using the current estimates for camera parameters and the poses) object points objectPoints. See [projectPoints()](https://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html#void projectPoints(InputArray objectPoints, InputArray rvec, InputArray tvec, InputArray cameraMatrix, InputArray distCoeffs, OutputArray imagePoints, OutputArray jacobian, double aspectRatio)) for details.  The function returns a matrix with the intrinsic camera parameters and a matrix with the distortion coefficients, which we can use to undistort our images.\nOther stuff if you like computer vision Optics is a pretty interesting field of physics, and you if you're planning to do any computer vision work or research, there's a bunch of stuff to learn and read to understand how cameras work, which would help you dive deeper into the field.\nIf you're not interested in it, feel free to skip the small paragraph below.\nI'd recommend at least reading about the two most important parameters of optical lenses: the focal length and the maximum aperture of the camera.\nIt's useful to know how different focal lengths affect the represented size of distant objects, for example:\n![https://upload.wikimedia.org/wikipedia/commons/e/e5/Focal_length.jpg?1580214480612](/images/ai/focal length.jpg)\nYou can also learn a lot by reading how the focal length determines the angle of view, how the focal ratio (or f-number) defines the maximum usable aperture of a lens and so on. It's really interesting.\n  Wikipedia: Pinhole camera model \u0026#x21a9;\u0026#xfe0e;\n Camera Obscura \u0026#x21a9;\u0026#xfe0e;\n   "
},
{
	"uri": "https://ori.codes/artificial-intelligence/custom-architecture/",
	"title": "Creating our first custom network architecture",
	"tags": [],
	"description": "",
	"content": "Where do we even begin with the AI part of the project?\nWell, I think it'd be a good idea to get the details of plugging a custom network into Donkey out of the way first.\nIf you remember the first autopilot we trained for sanity checking purposes, you'll recall we've used an architecture that came with Donkey, whose source can be found at donkeycar/parts/keras.py.\nAnd that's pretty cool, we've already got a fair number of architectures to play around with out of the box. But we'll want to do things our way, make some custom architectures and play around with all sorts of stuff. It's also beneficial to be able to visualize how all of the pieces of your project come together before you dive all the way down into the details of it.\nFor now, I won't get into the details of machine and deep learning. That's a whole other bullet to bite, and a science of its own. One that I'm also in the process of getting to know. If you want to understand all of the details of the following content, you'll need to have some working knowledge of ML/DL. It's also okay if you don't. You'll be able to follow along and get your feet wet, you just probably won't get some implied details and minutiae.\n Enough philosophising, let's get to work!\nHow do the architectures that come with Donkey work? First things first, let's see how the architectures we get out of the box with Donkey work.\nYou don't have to, and usually probably won't, do this the way I'm doing it, which is going through the source code files and seeing what happens. You can consult the documentation or ask for help on the official project channels, but I love this approach since I believe you'll get a better sense of how something works if you look at the insides of it. I'm also used to hacking and taking things apart just to see how they work, (see ivanorsolic.github.io for examples) and sometimes you don't have the luxury of good docs and people willing to help you, so it's useful to be able to dive into the internals of something and make sense out of it. I also just think this way is much more fun 🤓, but YMMV.\n SKIP TO THE TL;DR When we wanted to train our first autopilot with Donkey, we used the train flag to tell the manage.py script to train a model.\nSo if we open up the manage.py script and look at the main method, we can see how the script handles the train flag:\nif args[\u0026#39;train\u0026#39;]:\rfrom train import multi_train, preprocessFileList\rtub = args[\u0026#39;--tub\u0026#39;]\rmodel = args[\u0026#39;--model\u0026#39;]\rtransfer = args[\u0026#39;--transfer\u0026#39;]\rmodel_type = args[\u0026#39;--type\u0026#39;]\rcontinuous = args[\u0026#39;--continuous\u0026#39;]\raug = args[\u0026#39;--aug\u0026#39;] dirs = preprocessFileList( args[\u0026#39;--file\u0026#39;] )\rif tub is not None:\rtub_paths = [os.path.expanduser(n) for n in tub.split(\u0026#39;,\u0026#39;)]\rdirs.extend( tub_paths )\rmulti_train(cfg, dirs, model, transfer, model_type, continuous, aug)\rMost notably, it uses the multi_train function that it imports from the train.py script. Other than that, it parses the rest of the arguments it needs to call the multi_train function and does some preprocessing with the list of tubs we want to use.\nSo, we'll open up the train.py script, and find the definition of the multi_train function:\ndef multi_train(cfg, tub, model, transfer, model_type, continuous, aug):\r# choose the right regime for the given model type\r train_fn = train\rif model_type in (\u0026#34;rnn\u0026#34;,\u0026#39;3d\u0026#39;,\u0026#39;look_ahead\u0026#39;):\rtrain_fn = sequence_train\rtrain_fn(cfg, tub, model, transfer, model_type, continuous, aug)\rWe can see that it chooses the train function as the default function for training the models, but can also use the sequence_train function if the architecture is a sequential network. Cool!\nSince we won't be implementing a custom sequential network, we'll take a look at the default train function. There's a lot going on inside the function, from managing the data and creating generators to split it into batches for training to handling different model filetypes, but we don't have to, and won't go through all the details. That's one of the reasons we're using Donkey, so we don't have to everything by ourselves.\nOur goal is to use a custom architecture with Donkey, and we're trying to find out how Donkey uses the pre-defined architectures, so this line is very much of interest to us:\nkl = get_model_by_type(train_type, cfg=cfg)\rThe function above is imported from donkeycar/utils.py, so we'll open that script up and find the definition of the get_model_by_type function:\ndef get_model_by_type(model_type, cfg):\r\u0026#39;\u0026#39;\u0026#39;given the string model_type and the configuration settings in cfg create a Keras model and return it.\r\u0026#39;\u0026#39;\u0026#39;\rfrom donkeycar.parts.keras import KerasRNN_LSTM, KerasBehavioral, \\\rKerasCategorical, KerasIMU, KerasLinear, Keras3D_CNN, \\\rKerasLocalizer, KerasLatent\rfrom donkeycar.parts.tflite import TFLitePilot\rif model_type is None:\rmodel_type = cfg.DEFAULT_MODEL_TYPE\rprint(\u0026#34;\\\u0026#34;get_model_by_type\\\u0026#34;model Type is: {}\u0026#34;.format(model_type))\rinput_shape = (cfg.IMAGE_H, cfg.IMAGE_W, cfg.IMAGE_DEPTH)\rroi_crop = (cfg.ROI_CROP_TOP, cfg.ROI_CROP_BOTTOM)\rif model_type == \u0026#34;tflite_linear\u0026#34;:\rkl = TFLitePilot()\relif model_type == \u0026#34;localizer\u0026#34; or cfg.TRAIN_LOCALIZER:\rkl = KerasLocalizer(num_locations=cfg.NUM_LOCATIONS, input_shape=input_shape)\r# And so on ...\r else:\rraise Exception(\u0026#34;unknown model type: %s\u0026#34; % model_type)\rreturn kl\rThis is exactly what we were looking for. We can see that the function:\n Takes in the name of the wanted architecture as a string (passed to the manage.py script using the --type flag) Imports all of the architectures from donkeycar/parts/keras.py Creates a model using the appropriate architecture (based on the name) Returns the created model  It also defines the image shape and the region of interest crop that some models use, and sets the model type to the default type (defined in myconfig.py) if the type isn't explicitly passed through the type flag.\nOkay, so now we know how the manage.py script gets the Keras model it then trains.\nLet's take a look at keras.py to see how to define a custom architecture/model.\nKerasPilot base class We can see that there is a base class already prepared for us, that implements functions that all models use, such as model and weight loading, training and setting the optimizer of the model:\nclass KerasPilot(object):\r\u0026#39;\u0026#39;\u0026#39;Base class for Keras models that will provide steering and throttle to guide a car.\r\u0026#39;\u0026#39;\u0026#39;\rdef __init__(self):\rself.model = None\rself.optimizer = \u0026#34;adam\u0026#34;\rdef load(self, model_path):\rself.model = keras.models.load_model(model_path)\rdef load_weights(self, model_path, by_name=True):\rself.model.load_weights(model_path, by_name=by_name)\rdef shutdown(self):\rpass\rdef compile(self):\rpass\rdef set_optimizer(self, optimizer_type, rate, decay):\rif optimizer_type == \u0026#34;adam\u0026#34;:\rself.model.optimizer = keras.optimizers.Adam(lr=rate, decay=decay)\relif optimizer_type == \u0026#34;sgd\u0026#34;:\rself.model.optimizer = keras.optimizers.SGD(lr=rate, decay=decay)\relif optimizer_type == \u0026#34;rmsprop\u0026#34;:\rself.model.optimizer = keras.optimizers.RMSprop(lr=rate, decay=decay)\relse:\rraise Exception(\u0026#34;unknown optimizer type: %s\u0026#34; % optimizer_type)\rdef train(self, train_gen, val_gen, saved_model_path, epochs=100, steps=100, train_split=0.8,\rverbose=1, min_delta=.0005, patience=10, use_early_stop=True):\r# And so on ...\r The only three functions we need to implement ourselves in our custom class are: compile and run (and the constructor)\nLet's look at the default architecture, the KerasLinear class:\nclass KerasLinear(KerasPilot):\r\u0026#39;\u0026#39;\u0026#39;The KerasLinear pilot uses one neuron to output a continous value via the Keras Dense layer with linear activation. One each for steering and throttle.\rThe output is not bounded.\r\u0026#39;\u0026#39;\u0026#39;\rdef __init__(self, num_outputs=2, input_shape=(120, 160, 3), roi_crop=(0, 0), *args, **kwargs):\rsuper(KerasLinear, self).__init__(*args, **kwargs)\rself.model = default_n_linear(num_outputs, input_shape, roi_crop)\rself.compile()\rdef compile(self):\rself.model.compile(optimizer=self.optimizer,\rloss=\u0026#39;mse\u0026#39;)\rdef run(self, img_arr):\rimg_arr = img_arr.reshape((1,) + img_arr.shape)\routputs = self.model.predict(img_arr)\rsteering = outputs[0]\rthrottle = outputs[1]\rreturn steering[0][0], throttle[0][0]\rWe can see that this particular class uses the roi_crop variable along with the input_shape, that gets passed to it through the utils.py script.\nThe class inherits the base KerasPilot class, and sets the self.model using the default_n_linear function, which actually implements the architecture in Keras.\nIt also compiles itself using the default parent class optimizer (adam), and uses the mean squared error as the error function.\nLet's look at the actual Keras implementation in the default_n_linear function:\ndef default_n_linear(num_outputs, input_shape=(120, 160, 3), roi_crop=(0, 0)):\rdrop = 0.1\rinput_shape = adjust_input_shape(input_shape, roi_crop)\rimg_in = Input(shape=input_shape, name=\u0026#39;img_in\u0026#39;)\rx = img_in\rx = Convolution2D(24, (5,5), strides=(2,2), activation=\u0026#39;relu\u0026#39;, name=\u0026#34;conv2d_1\u0026#34;)(x)\rx = Dropout(drop)(x)\rx = Convolution2D(32, (5,5), strides=(2,2), activation=\u0026#39;relu\u0026#39;, name=\u0026#34;conv2d_2\u0026#34;)(x)\rx = Dropout(drop)(x)\rx = Convolution2D(64, (5,5), strides=(2,2), activation=\u0026#39;relu\u0026#39;, name=\u0026#34;conv2d_3\u0026#34;)(x)\rx = Dropout(drop)(x)\rx = Convolution2D(64, (3,3), strides=(1,1), activation=\u0026#39;relu\u0026#39;, name=\u0026#34;conv2d_4\u0026#34;)(x)\rx = Dropout(drop)(x)\rx = Convolution2D(64, (3,3), strides=(1,1), activation=\u0026#39;relu\u0026#39;, name=\u0026#34;conv2d_5\u0026#34;)(x)\rx = Dropout(drop)(x)\rx = Flatten(name=\u0026#39;flattened\u0026#39;)(x)\rx = Dense(100, activation=\u0026#39;relu\u0026#39;)(x)\rx = Dropout(drop)(x)\rx = Dense(50, activation=\u0026#39;relu\u0026#39;)(x)\rx = Dropout(drop)(x)\routputs = []\rfor i in range(num_outputs):\routputs.append(Dense(1, activation=\u0026#39;linear\u0026#39;, name=\u0026#39;n_outputs\u0026#39; + str(i))(x))\rmodel = Model(inputs=[img_in], outputs=outputs)\rreturn model\rAnd that's the last piece of the puzzle. It takes the number of outputs, the input shape and the region of interest via parameters, and implements a Keras model.\nSo, in summary, what are the steps for making your own architecture?\nTL;DR: How to implement your own architecture   Create a new Python script (or open up the keras.py script and work in there)\n  Import the KerasPilot base model class:\nfrom donkeycar.parts.keras import KerasPilot\r  Define a custom function that implements your architecture in Keras\n  Define a custom class that inherits the KerasPilot class, implements compile and run and initializes the model using the above mentioned function\n  Add your architecture to the get_model_by_type function in utils.py\n  Use your architecture by passing the model_type you defined in utils.py as the type flag to the manage.py script.\n  Implementing a custom architecture First, we have to come up with an architecture to implement.\nWe'll use Nvidia's CNN architecture from the End to End Learning for Self-Driving Cars paper by M. Bojarski et al.\nWhy this particular paper? Well, I think it's fitting since it has a nice bit of self-driving RC history behind it.\nAs Nvidia states on their blog, explaining the project behind the paper:\n The groundwork for this project was actually done over 10 years ago in a Defense Advanced Research Projects Agency (DARPA) seedling project known as DARPA Autonomous Vehicle (DAVE), in which a sub-scale radio control (RC) car drove through a junk-filled alley way. DAVE was trained on hours of human driving in similar, but not identical, environments. The training data included video from two cameras and the steering commands sent by a human operator.\n Hey, would you look at that! Back in 2004, DARPA had a seedling project which had an RC car autonomously drive through a junk-filled alley way. It was around the time the first DARPA Grand Challenge took place, in which a full-sized car had to autonomously navigate a 240 km route in the Mojave desert.\nNo cars managed to finish the course that year, with the best competitor's car (Carnegie Mellon) traveling a mere 11.78 kilometers.\nHere's a picture of DAVE, taken from the DARPA-IPTO Autonomous Off-Road Vehicle Control using End-to-End learning report.\nI recommend reading about the original (and subsequent) DARPA Grand Challenges, and the DAVE project. It's pretty fun to see how the history of autonomous vehicles began, and how the early researches found out about all of the challenges and their solutions on the way.\nThere's also a documentary about the DARPA Grand Challenge called The Great Robot Race by Nova. You can easily find it online.\nThe CNN Architecture Here's a picture of the architecture, taken from Nvidia's blog:\nAs Nvidia says in their blog post, the architecture consists of 9 layers, including a normalization layer, 5 convolutional layers, and 3 fully connected layers.\n The first layer of the network performs image normalization. The normalizer is hard-coded and is not adjusted in the learning process. Performing normalization in the network allows the normalization scheme to be altered with the network architecture, and to be accelerated via GPU processing.\nThe convolutional layers are designed to perform feature extraction, and are chosen empirically through a series of experiments that vary layer configurations. We then use strided convolutions in the first three convolutional layers with a 2×2 stride and a 5×5 kernel, and a non-strided convolution with a 3×3 kernel size in the final two convolutional layers.\nWe follow the five convolutional layers with three fully connected layers, leading to a final output control value which is the inverse-turning-radius. The fully connected layers are designed to function as a controller for steering, but we noted that by training the system end-to-end, it is not possible to make a clean break between which parts of the network function primarily as feature extractor, and which serve as controller.\n Pretty cool. Let's implement it in Keras!\nI've created a new Python source file in the donkeycar/parts folder, named nvidia.py.\nI used the KerasLinear class as a starting point, using its adjust_input_shape method to crop the image to the ROI passed to the model, since I think that's a good way to get better performance.\nI've also made the following adjustments to the original architecture:\n I've omitted the normalization layer for now, which can be implemented using Keras\u0026rsquo; normalization layers. I've added a 25 unit fully connected layer between the 50 and 10 unit layers, and a 5 unit layer before the output layer. I've added dropout regularization, with a 90% keep probability. I used two separate output units for steering and throttle, using the KerasLinear model as a starting point.  This is what the above described architecture looks like implemented in Keras:\ndef customArchitecture(num_outputs, input_shape, roi_crop):\rinput_shape = adjust_input_shape(input_shape, roi_crop)\rimg_in = Input(shape=input_shape, name=\u0026#39;img_in\u0026#39;)\rx = img_in\r# Dropout rate\r keep_prob = 0.9\rrate = 1 - keep_prob\r# Convolutional Layer 1\r x = Convolution2D(filters=24, kernel_size=5, strides=(2, 2), input_shape = input_shape)(x)\rx = Dropout(rate)(x)\r# Convolutional Layer 2\r x = Convolution2D(filters=36, kernel_size=5, strides=(2, 2), activation=\u0026#39;relu\u0026#39;)(x)\rx = Dropout(rate)(x)\r# Convolutional Layer 3\r x = Convolution2D(filters=48, kernel_size=5, strides=(2, 2), activation=\u0026#39;relu\u0026#39;)(x)\rx = Dropout(rate)(x)\r# Convolutional Layer 4\r x = Convolution2D(filters=64, kernel_size=3, strides=(1, 1), activation=\u0026#39;relu\u0026#39;)(x)\rx = Dropout(rate)(x)\r# Convolutional Layer 5\r x = Convolution2D(filters=64, kernel_size=3, strides=(1, 1), activation=\u0026#39;relu\u0026#39;)(x)\rx = Dropout(rate)(x)\r# Flatten Layers\r x = Flatten()(x)\r# Fully Connected Layer 1\r x = Dense(100, activation=\u0026#39;relu\u0026#39;)(x)\r# Fully Connected Layer 2\r x = Dense(50, activation=\u0026#39;relu\u0026#39;)(x)\r# Fully Connected Layer 3\r x = Dense(25, activation=\u0026#39;relu\u0026#39;)(x)\r# Fully Connected Layer 4\r x = Dense(10, activation=\u0026#39;relu\u0026#39;)(x)\r# Fully Connected Layer 5\r x = Dense(5, activation=\u0026#39;relu\u0026#39;)(x)\routputs = []\rfor i in range(num_outputs):\r# Output layer\r outputs.append(Dense(1, activation=\u0026#39;linear\u0026#39;, name=\u0026#39;n_outputs\u0026#39; + str(i))(x))\rmodel = Model(inputs=[img_in], outputs=outputs)\rreturn model\rNow we need to implement our custom class, inheriting the base KerasPilot class:\n I used the adam optimizer and the mean squared error as the error function I've set the model using the above customArchitecture function, passing it the number of outputs, the input shape and the region of interest I've copied the KerasLinear run method, since it already implements everything needed to run  class NvidiaModel(KerasPilot):\rdef __init__(self, num_outputs=2, input_shape=(120, 160, 3), roi_crop=(0, 0), *args, **kwargs):\rsuper(NvidiaModel, self).__init__(*args, **kwargs)\rself.model = customArchitecture(num_outputs, input_shape, roi_crop)\rself.compile()\rdef compile(self):\rself.model.compile(optimizer=\u0026#34;adam\u0026#34;,\rloss=\u0026#39;mse\u0026#39;)\rdef run(self, img_arr):\rimg_arr = img_arr.reshape((1,) + img_arr.shape)\routputs = self.model.predict(img_arr)\rsteering = outputs[0]\rthrottle = outputs[1]\rreturn steering[0][0], throttle[0][0]\rNow we need to add our custom class to the utils.py script so we can use it through the manage.py script. I've appended the following lines to the get_model_by_type function, right after the rest of the pre-defined architectures:\nelif model_type == \u0026#34;nvidia\u0026#34;:\rfrom donkeycar.parts.nvidia import NvidiaModel\rkl = NvidiaModel(input_shape=input_shape,roi_crop=roi_crop)\rAnd that's it! We can now train a model using the manage.py script. Here's the result of training the architecture using this test dataset and running it in the simulator:\n\rNot bad, considering it's trained on a really small dataset. Also, it's not the best driving data around. You can see it on the sharp 90 degree turns. It first goes as near as it can to the middle line, to *take a swing* so it can enter the turn faster. If we'd trained it on a much bigger dataset, with better driving data, it'd work better. Let's move on to the next chapter, how to train your model.\n"
},
{
	"uri": "https://ori.codes/artificial-intelligence/simulator-mod/high-resolution-images/",
	"title": "Simulator mod: High resolution images",
	"tags": [],
	"description": "",
	"content": "If we want to use the simulator to gather training data that's larger than the default 160x120 image size, we'll need to create a modified version of it.\nSetting up the simulator locally First off, let's clone (or fork) the original simulator Tawn Kramer made for Donkey from GitHub.\ngit clone --single-branch --branch donkey https://github.com/tawnkramer/sdsandbox\r If you're wondering why aren't I just using the git clone --branch command: it clones all branches, but checks out just the one you've passed to the flag. Since Unity projects can be quite large, and Tawn has several branches on the repo, we'd end up downloading a lot of stuff we don't need.\n We'll also need to install Unity. First you need to download and install the Unity Hub:\n Windows download Mac OS X download Linux download  After installing it, open it and select the Installs tab from the left sidebar, click the blue Add button on the right and install the latest official release.\nAfter installing the latest release of Unity, select the Projects tab on the left sidebar, click the silver Add button and select the sdsim folder from inside the folder where you've cloned/downloaded the simulator.\nYou can now click on the project you've added and you're good to go.\nModifying the camera resolution After opening the project in Unity, open up the Prefabs folder (in the project view, lower left by default) and then open the donkey prefab, which is the default RC model prefab for the simulator.\nInside the prefab hierarchy (upper left by default) you can see the cameraSensorBase which contains the CameraSensor, which is the RC camera sensor that we use to generate our training data (take pictures from inside the simulator).\nAfter selecting the CameraSensor, we can see, in the inspector on the right side of the screen, that there is a CameraSensor script connected to it. You can open it by double clicking on it or finding it in the lower left project viewer inside Assets/Scripts/CameraSensor.cs.\nWe'll make the width and height fields of the class static, so we can edit them from another script we'll create:\n\\\\ You can put whatever resolution you want to be default here.\rpublic static int width = 640;\rpublic static int height = 480;\rAnd we'll change the parameters of the ReadPixels function to use our width and height:\ntex.ReadPixels(new Rect(0, 0, width, height), 0, 0);\rWe'll also edit the CameraHelper script to use our static fields for the width and height:\nTexture2D texture2D = new Texture2D(CameraSensor.width, CameraSensor.height, TextureFormat.RGB24, false);\rtexture2D.ReadPixels(new Rect(0, 0, CameraSensor.width, CameraSensor.height), 0, 0);\rNow go to the Scenes folder and open the menu scene and add a Dropdown element to the menu by right clicking on the Canvas and selecting UI \u0026gt; Dropdown.\nWe'll be using this dropdown as our resolution picker, so go ahead and click on it, and inside the inspector panel add all of the resolutions you'd like to be able to use as options:\nThen resize and position the dropdown on a place you'd like it to be on the menu:\nNow go back to the Scripts folder and add a new class called ResolutionSetter. This class will set the resolution inside the CameraSensor class based on the selected option in the dropdown.\nHere's the code for the ResolutionSetter class:\nusing System.Collections;\rusing System.Collections.Generic;\rusing UnityEngine;\rusing UnityEngine.UI;\rpublic class ResolutionSetter : MonoBehaviour\r{\r//Attach this script to a Dropdown GameObject\r Dropdown m_Dropdown;\r//This is the string that stores the current selection m_Text of the Dropdown\r string m_Message;\r//This Text outputs the current selection to the screen\r public Text m_Text;\r//This is the index value of the Dropdown\r int m_DropdownValue;\rvoid Start()\r{\r//Fetch the Dropdown GameObject\r m_Dropdown = GetComponent\u0026lt;Dropdown\u0026gt;();\r//Add listener for when the value of the Dropdown changes, to take action\r m_Dropdown.onValueChanged.AddListener(delegate {\rDropdownValueChanged(m_Dropdown);\r});\rsetCameraSensorRes(m_Dropdown.options[m_DropdownValue].text);\r}\rvoid DropdownValueChanged(Dropdown change)\r{\rsetCameraSensorRes(m_Dropdown.options[change.value].text);\r}\rvoid setCameraSensorRes(string resolution){\rCameraSensor.width = System.Convert.ToInt32(resolution.Split(\u0026#39;x\u0026#39;)[0]);\rCameraSensor.height = System.Convert.ToInt32(resolution.Split(\u0026#39;x\u0026#39;)[1]);\r}\r}\rNow go back to the Scenes folder and open the menu scene once again. Select the dropdown element we created and in the inspector panel, drag and drop the ResolutionSetter script on it:\nAnd that's it. Now the output images will have the resolution that you've set in the main menu using the dropdown.\nBuilding and using the modded simulator You can now go to File \u0026gt; Build Settings (or press CTRL + SHIFT + B), select Build at the bottom of the screen and select the folder where your simulator binary is. After building, you can use your modded version the same way you used the default Donkey simulator release.\nHere's what a sample output image when the dropdown is set to 640x480:\n"
},
{
	"uri": "https://ori.codes/hardware/",
	"title": "Hardware",
	"tags": [],
	"description": "",
	"content": "Assembling the real-world stuff Hardware Through this chapter we'll go through assembling the hardware platform, all things hardware.\n"
},
{
	"uri": "https://ori.codes/artificial-intelligence/visualization/",
	"title": "Visualization",
	"tags": [],
	"description": "",
	"content": "Seeing what the network sees Visualization In this subchapter we'll go through some methods that will allow us to easily debug our networks by actually seeing (in a way) what the network sees and how it thinks.\n"
},
{
	"uri": "https://ori.codes/artificial-intelligence/camera-calibration/implementing-camera-calibration/",
	"title": "Camera calibration: Implementing the calibration and undistortion",
	"tags": [],
	"description": "",
	"content": "First we'll import the stuff we need and declare some variables:\nimport numpy as np\rimport cv2, os, glob\robjectPoints = []\rimagePoints = []\rcameraIntrinsicValues = []\r# Distortion coefficients\r cameraExtrinsicValues = []\rNow we'll implement the function that finds and returns the object and image points, given images of a chessboard:\ndef getObjectAndImagePoints():\rglobal objectPoints, imagePoints\r# Number of inside corners per row and column\r cornersPerRow = 10\rcornersPerColumn = 7\r# Initializing the object points to zero\r chessboardObjectPoints = np.zeros((cornersPerColumn * cornersPerRow, 3), np.float32)\r# Prepare a meshgrid for object points\r # (0,0,0), (1,0,0), (2,0,0) ..., (cornersPerRow,cornersPerColumn,0)\r # We can do this since we know how many corners there are on our printed chessboard\r chessboardObjectPoints[:,:2] = np.mgrid[0:cornersPerRow, 0:cornersPerColumn].T.reshape(-1, 2)\r# List of calibration images\r images = []\r# To make sure you can run the script on any image filetype\r extensions = [\u0026#39;*.gif\u0026#39;, \u0026#39;*.png\u0026#39;, \u0026#39;*.jpeg\u0026#39;, \u0026#39;*.jpg\u0026#39;, \u0026#39;*.tiff\u0026#39;]\rfor extension in extensions:\rimages.extend(glob.glob(\u0026#39;calibration_images/\u0026#39;+extension))\rprint(images)\r# Step through the list and search for chessboard corners\r for calibrationImageFileName in images:\rcalibrationImage = cv2.imread(calibrationImageFileName)\r# The detector doesn\u0026#39;t work well with images larger than 1280x720\r # So we\u0026#39;ll resize any until they\u0026#39;re 720p or smaller\r height, width = calibrationImage.shape[:2]\rwhile width \u0026gt; 1280:\rwidth //= 2\rheight //= 2\rcalibrationImage = cv2.resize(calibrationImage, (width, height))\r# Convert it to grayscale\r grayCalibrationImage = cv2.cvtColor(calibrationImage, cv2.COLOR_BGR2GRAY)\r# Find the image points\r cornersFound, foundCorners = cv2.findChessboardCorners(grayCalibrationImage, (cornersPerRow, cornersPerColumn),None)\r# If corners were found on the images\r # Append the found image points and defined object points to global variables\r if cornersFound:\robjectPoints.append(chessboardObjectPoints)\rimagePoints.append(foundCorners)\r# If you want to visualize the found corners\r cv2.drawChessboardCorners(calibrationImage, (cornersPerRow, cornersPerColumn), foundCorners, cornersFound)\rcv2.imshow(\u0026#39;Preview\u0026#39;, calibrationImage)\rcv2.waitKey(500)\rNow we can call the calibrateCamera function to get our extrinsic and intrinsic values:\ndef calibrateCamera(imageSize):\rglobal cameraIntrinsicValues, cameraExtrinsicValues, objectPoints, imagePoints\rretVal, cameraIntrinsicValues, cameraExtrinsicValues, rotationVectors, translationVectors = cv2.calibrateCamera(objectPoints, imagePoints, imageSize, None, None)\rAnd finally, with all of the values, we can undistort our image:\ndef undistortImage(image):\rreturn cv2.undistort(image, cameraIntrinsicValues, cameraExtrinsicValues, None, cameraIntrinsicValues)\r"
},
{
	"uri": "https://ori.codes/rc-car/parts_list/",
	"title": "Parts: an overview",
	"tags": [],
	"description": "",
	"content": "There’s a lot of, and I mean a lot of parts when it comes to building an RC car on your own. But in the context of building a self-driving RC car, here’s a brief overview of some of the stuff we’ll need:\nYou'll want to have:  A RC car (with some batteries) A PWM/Servo Driver (I2C + some jumper cables) A Jetson Nano A powerbank (+ some usb cables) A microSD card (and optionally an external SSD) A WiFi/BT m.2 card (key E) or some USB equivalent Some tools and materials for building the chassis (and optionally access to a 3D printer) An Xbox/PS/PC gamepad  Let’s focus on the RC car (and the batteries) part for now.\nWhat RC car should I get? You can basically get any RC car, as long as it has an ESC controller, or you can hook up one to it.\n Some parts are more important than others, but here’s a overview of the parts we’ll be focusing on:\n Scale Body type Electric motor Servo ESC Receiver Batteries  "
},
{
	"uri": "https://ori.codes/artificial-intelligence/simulator-mod/custom-rc-model/",
	"title": "Simulator mod: Custom RC model",
	"tags": [],
	"description": "",
	"content": "If you want to get better data for your RC, you should edit the default model that comes with the simulator. Most notably, you should edit the camera position, resolution and field of view. You can also add multiple cameras and edit the center of mass of your vehicle, as well as any other properties you think would make a difference if you wanted to use the simulator to pre-train the weights for your RC.\nLet's see how we can do that.\nEditing the camera Open up the Unity project and open the donkey prefab, found in Assets/Prefabs/donkey.fbx.\nIn the prefab hierarchy, select the cameraSensorBase and then select the CameraSensor.\nOn the right, in the Inspector panel, under the Camera settings, you can edit a bunch of settings related to it. I'd suggest you edit the FOV value to match the FOV your real camera has, and I'd recommend disabling the Fisheye and Post Processing scripts.\nYou can also move the camera in different positions using the move and rotate tools and the editor on the center of the screen, and pivot/angle it to match your real RC.\nTesting it out You can open up any a scene, e.g. the warehouse scene, zoom up to the track and drag and drop your prefab onto it. After dropping it on the track, select the camera sensor to see what its output looks like:\nAdding additional parts and changing the center of mass If you'd like, you can always create a custom 3D model of the electronics that are on your RC and insert it into the default donkey model. You can use the default donkey.fbx that comes with the simulator and edit it, or you can make your own from scratch.\nAfter creating a 3D model, drag and drop it into the Models folder. Now open up the donkey prefab and drag your model wherever you'd like on the Hierarchy panel. I'd recommend putting it inside the donkey element. You can then position and resize your elements to fit them on the RC.\nIf you want to reorder any elements or edit them individually, but the element looks blue, like this power bank v1 element, you can right click on it and select unpack prefab:\nThen you'll be able to move individual parts around.\nIf you've made significant changes to the model, and added a lot of parts, your RC's center of mass has probably changed, and you should reflect that in the simulator by changing the defined center of mass, since you want it to realistically handle corners just as it would in real life, and if your center of mass is higher up because you've added a bunch of stuff on your RC, it could perform great in the simulator but tip over in real life.\nYou can find the centerOfMass element inside the donkey prefab, and move and pivot it around just like any other element:\nYou can now build your simulator and try your model out.\n"
},
{
	"uri": "https://ori.codes/artificial-intelligence/visualization/cnn-activations/",
	"title": "Visualization: CNN Activations",
	"tags": [],
	"description": "",
	"content": "We already saw that the custom architecture/model works, but it's surely not the best we could come up with. The question is, how can we improve its performance and get some intuition on how it works?\nSeeing what the network sees One useful visual tool we could use is Donkey's built in cnnactivations command. Per the official docs, it shows us the feature maps for each convolutional layer in the model we pass to it. The feature maps show us how each filter in every conv layer activates, given the input image we pass to it.\nThe tool currently works with only 2D convolutional layers.\n You can use the command as follows:\ndonkey cnnactivations --model pathTo/yourModel.h5 --image pathTo/anImage.jpg\rHere are the results I got while running this picture through the Nvidia model we've trained previously:\nInput resolution? Now, we can see a couple of things, but most notably, that it's hard to actually see things on these images. And in case you're wondering, it's not because they're resized. They're pretty low res, since the input image is only 160x120 and due to the nature of convolutions it only gets smaller.\nThe formula for the output size of a convolutional layer is: $$ \\Big\\lfloor{\\frac{n+2p-f}{s}+1}\\Big\\rfloor $$ Where $n$ is an input dimension, $p$ is the padding, $f$ is the number of filters and $s$ is the stride size.\nLet's calculate the output size of our first convolutional layer, which was:\nx = Convolution2D(filters=12, kernel_size=5, strides=(2, 2), input_shape = input_shape, activation=\u0026#39;relu\u0026#39;, name=\u0026#34;Conv2D_First_layer\u0026#34;)(x)\rThe input dimensions of our image are 160 and 120, the other parameters are easily seen, except for the padding, which is valid (aka zero), since we didn't specify it explicitly.\n$$ width=\\Big\\lfloor{\\frac{160-5}{2}+1}\\Big\\rfloor = \\Big\\lfloor{78.5}\\Big\\rfloor = 78 $$\n$$ height=\\Big\\lfloor{\\frac{120-5}{2}+1}\\Big\\rfloor = \\Big\\lfloor{58.5}\\Big\\rfloor = 58 $$\nSo after the first layer, we'll have 12 filters (or images) with the size of 78x58. Let's look at the other layers:\n Second layer: 37x27, 24 filters Third layer: 17x12, 48 filters Fourth layer: 8x5, 64 filters Fifth layer: 3x2, 64 filters  So yeah, those are some small resolutions. I mean, it will work, but when you think about it, would you be able to perfectly drive a car, if your eyes saw this:\n\rOr this:\r\rThe smaller video is made out of 160x120 images, and the higher res one is made out of 640x360 images. The smaller images are around 3.2KB each, and the bigger ones are around 22KB each, which is almost seven times as big, for a fourfold increase in resolution. So you can probably imagine, it's more expensive computationally, but it also offers way more data to the network to work with.\nThe images are by default smaller because Donkey was originally made for the Raspberry Pi, which doesn't have a discrete GPU, so it can handle a lot less computationally intensive tasks. But since we're working with a Jetson, we should be able to increase the resolution a bit, and if necessary implement some feature engineering and preprocessing to speed things up a little on higher res inputs.\nSo we'll be increasing the input resolution to be able to train more complex models.\nLet's move on to another visualization technique: saliency maps!\n"
},
{
	"uri": "https://ori.codes/artificial-intelligence/simulator-mod/",
	"title": "Simulator modding",
	"tags": [],
	"description": "",
	"content": "Adding high res images and stuff Modding the simulator In this subchapter we'll go through the modification of the simulator to allow custom RC models and higher res datasets.\n"
},
{
	"uri": "https://ori.codes/software/",
	"title": "Software",
	"tags": [],
	"description": "",
	"content": "Making hardware do stuff Software In this chapter we’ll go deal with all things software we need before starting our ML/DL work.\n"
},
{
	"uri": "https://ori.codes/artificial-intelligence/camera-calibration/calibrating-the-camera/",
	"title": "Camera calibration: Calibrating the camera and undistorting images",
	"tags": [],
	"description": "",
	"content": "Finally, we can actually calibrate our camera and undistort our images!\nWe'll start with the real camera on our RC and then we'll also calibrate our simulator camera!\nOnce calibrated on the calibration images, you can use the same undistortion matrix for any other image that the same camera takes (given the focal length hasn't changed)!\n Calibrating your real camera I'll be using my RC camera, the EleCam Explorer 4K, which has an advertised 170 degree FOV.\nFirst, we need to print a checkerboard pattern so we can take some calibration rig photos.\nThere's a pretty cool checkerboard collection by Mark Hedley Jones which you can use to generate a checkerboard/chessboard pattern you like. I'll be using this one:\nIf you use another pattern, make sure to update the number of inner corners for the rows and columns in the getObjectAndImagePoints function we made earlier.\n After printing it out on an A4 paper, you should take at least 10 photos of it in different angles, e.g.:\nAlso, most action cams with a FOV as large as this one (170°) will have some built-in distortion correction, probably named something like fisheye correction or adjust:\n![Fisheye adjust](/images/ai/fisheye adjust.png)\nI've intentionally left mine off, to show how distorted the images are by default at such a big FOV, and to show that they can be undistorted even for those whose camera doesn't have a built-in option to do so.\nAfter copying the images to my calibration_images folder and calling the getObjectAndImagePoints script, here's what the detected image points look like:\nAfter getting the image points, we can call the calibrateCamera function once, and then undistortImage to undistort our images for as many new images as we want, here's what the previous two image look like undistorted:\nAnd that's it! You can undistort every image like this before you input it to the neural network. Here's what the code could look like:\n# At the beginning of run\r getObjectAndImagePoints()\rcalibrateCamera(inputImage.shape[1::-1])\rundistortedImage = undistortImage(inputImage)\r# Pass it along to the NN\r Calibrating the simulator camera First, we need to put a checkerboard object into the simulator so we can take photos of it.\nTODO "
},
{
	"uri": "https://ori.codes/artificial-intelligence/visualization/saliency-maps/",
	"title": "Visualization: Saliency Maps",
	"tags": [],
	"description": "",
	"content": "Another useful visual tool to see how your network works is a saliency map. They were proposed back in 1998 by Itti, Koch and Niebur, a group of neuroscientists working on feature extraction in images, in a paper titled A Model of Saliency-based Visaul Attention for Rapid Scene Analysis.\nIn the context of Deep Learning and convolutional neural networks, they were first mentioned by the Visual Geometry Group at the University of Oxford, in a paper called Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps\nI won't go into excessive detail on how it works, but in our case, a saliency map will show us which pixels on the input image make the most impact on the output inferred by the network. We will make a heat map that shows us which visual features the car uses the most, in order to steer and throttle correctly. Hopefully, it should be the lane lines.\nNow, just a day after I've implemented a script that generates a video with a saliency map, given a trained model and some input data, I've noticed that Tawn Kramer (of Donkey) has already implemented a saliency map visualization, which you can generate using the donkey makemovie command. But since I've already made my implementation, we'll walk through it.\nAs a starting point, I've used this Jupyter Notebook made by ermolenkodev.\nI'll be using the Nvidia model we made earlier throughout the implementation. Feel free to use your own stuff in here.\nImplementing the Saliency Map vizualization First off, we need to import all the Keras stuff we use in our model:\nfrom donkeycar.parts.keras import KerasPilot\rfrom tensorflow.python.keras.layers import Input, Dense\rfrom tensorflow.python.keras.models import Model, Sequential\rfrom tensorflow.python.keras.layers import Convolution2D, Convolution2D, MaxPooling2D, Reshape, BatchNormalization\rfrom tensorflow.python.keras.layers import Activation, Dropout, Flatten, Cropping2D, Lambda\rThen we can copy the function that implements our model:\n# The ROI crop helper function\r def adjust_input_shape(input_shape, roi_crop):\rheight = input_shape[0]\rnew_height = height - roi_crop[0] - roi_crop[1]\rreturn (new_height, input_shape[1], input_shape[2])\r# The definition of our model\r # Also, be sure to name every convolutional layer you have as \u0026#34;convx\u0026#34; (x ∈ ℕ)\r def customModel(num_outputs=2, input_shape=(160,120,3), roi_crop=(0,0)):\rinput_shape = adjust_input_shape(input_shape, roi_crop)\rimg_in = Input(shape=input_shape, name=\u0026#39;img_in\u0026#39;)\rx = img_in\r# Dropout rate\r keep_prob = 0.5\rrate = 1 - keep_prob\r# Convolutional Layer 1\r x = Convolution2D(filters=12, kernel_size=5, strides=(2, 2), activation=\u0026#39;relu\u0026#39;, name=\u0026#34;conv1\u0026#34;)(x)\rx = Dropout(rate)(x)\r# Convolutional Layer 2\r x = Convolution2D(filters=24, kernel_size=5, strides=(2, 2), activation=\u0026#39;relu\u0026#39;, name=\u0026#34;conv2\u0026#34;)(x)\rx = Dropout(rate)(x)\r# Convolutional Layer 3\r x = Convolution2D(filters=48, kernel_size=5, strides=(2, 2), activation=\u0026#39;relu\u0026#39;, name=\u0026#34;conv3\u0026#34;)(x)\rx = Dropout(rate)(x)\r# Convolutional Layer 4\r x = Convolution2D(filters=64, kernel_size=3, strides=(1, 1), activation=\u0026#39;relu\u0026#39;, name=\u0026#34;conv4\u0026#34;)(x)\rx = Dropout(rate)(x)\r# Convolutional Layer 5\r x = Convolution2D(filters=64, kernel_size=3, strides=(1, 1), activation=\u0026#39;relu\u0026#39;, name=\u0026#34;conv5\u0026#34;)(x)\rx = Dropout(rate)(x)\r# Flatten Layers\r x = Flatten()(x)\r# Fully Connected Layer 1\r x = Dense(100, activation=\u0026#39;relu\u0026#39;)(x)\r# Fully Connected Layer 2\r x = Dense(50, activation=\u0026#39;relu\u0026#39;)(x)\r# Fully Connected Layer 3\r x = Dense(25, activation=\u0026#39;relu\u0026#39;)(x)\rx = Dense(10, activation=\u0026#39;relu\u0026#39;)(x)\rx = Dense(5, activation=\u0026#39;relu\u0026#39;)(x)\routputs = []\rfor i in range(num_outputs):\routputs.append(Dense(1, activation=\u0026#39;linear\u0026#39;, name=\u0026#39;n_outputs\u0026#39; + str(i))(x))\rmodel = Model(inputs=[img_in], outputs=outputs)\rreturn model\rWe can then initialize the model and pass the weights we previously trained to it:\nmodel = customModel()\r# Pass the path to your trained .h5 file here\rmodel.load_weights('nvidia.h5')\rNow we can define just the convolutional layers we're interested in visualizing:\nimg_in = Input(shape=(160,120,3), name=\u0026#39;img_in\u0026#39;)\rx = img_in\rx = Convolution2D(filters=12, kernel_size=5, strides=(2, 2), activation=\u0026#39;relu\u0026#39;, name=\u0026#34;conv1\u0026#34;)(x)\rx = Convolution2D(filters=24, kernel_size=5, strides=(2, 2), activation=\u0026#39;relu\u0026#39;, name=\u0026#34;conv2\u0026#34;)(x)\rx = Convolution2D(filters=48, kernel_size=5, strides=(2, 2), activation=\u0026#39;relu\u0026#39;, name=\u0026#34;conv3\u0026#34;)(x)\rx = Convolution2D(filters=64, kernel_size=3, strides=(1, 1), activation=\u0026#39;relu\u0026#39;, name=\u0026#34;conv4\u0026#34;)(x)\rlastConvLayer = Convolution2D(filters=64, kernel_size=3, strides=(1, 1), activation=\u0026#39;relu\u0026#39;, name=\u0026#34;conv5\u0026#34;)(x) convolution_part = Model(inputs=[img_in], outputs=[lastConvLayer])\rWe can now set the weights of the convolutional layers we just created to the actual weights our model contains:\n# If you have more than 5 layers, or less than 5 layers, edit the number here\r numberOfConvLayers = 5\rfor layer_num in range(1, numberOfConvLayers):\rconvolution_part.get_layer(\u0026#39;conv\u0026#39; + str(layer_num)).set_weights(model.get_layer(\u0026#39;conv\u0026#39; + str(layer_num)).get_weights())\rTo get the activation values from each layer, we need to define functors for them:\nfrom tensorflow.python.keras import backend as K\rinp = convolution_part.input # input placeholder\r outputs = [layer.output for layer in convolution_part.layers[1:]] # all layer outputs\r functors = K.function([inp], outputs)\rWe'll also define some helper variables that we'll use for the strides, padding and the kernels while computing the visualization masks:\nimport tensorflow as tf\rimport numpy as np\rimport pdb\r# 3x3 kernel with all ones\r kernel_3x3 = tf.constant(np.array([\r[[[1]], [[1]], [[1]]], [[[1]], [[1]], [[1]]], [[[1]], [[1]], [[1]]]\r]), tf.float32)\r# 5x5 kernel with all ones\r kernel_5x5 = tf.constant(np.array([\r[[[1]], [[1]], [[1]], [[1]], [[1]]], [[[1]], [[1]], [[1]], [[1]], [[1]]], [[[1]], [[1]], [[1]], [[1]], [[1]]],\r[[[1]], [[1]], [[1]], [[1]], [[1]]],\r[[[1]], [[1]], [[1]], [[1]], [[1]]]\r]), tf.float32)\r# Based on the layers in your model, you should assign the kernel sizes you\u0026#39;re using at each layer here.\r # E.g. I\u0026#39;m using a 3x3 kernel in my last two layers, and a 3x3 in my first three layers\r layers_kernels = {5: kernel_3x3, 4: kernel_3x3, 3: kernel_5x5, 2: kernel_5x5, 1: kernel_5x5}\r# Same goes here for the strides you\u0026#39;re using in your layers\r layers_strides = {5: [1, 1, 1, 1], 4: [1, 1, 1, 1], 3: [1, 2, 2, 1], 2: [1, 2, 2, 1], 1: [1, 2, 2, 1]}\rAnd we can finally compute the visualization masks using ermolenkodev's function:\ndef compute_visualisation_mask(input_image):\ractivations = functors([np.array([input_image])])\ractivations = [np.reshape(input_image, (1, input_image.shape[0], input_image.shape[1], input_image.shape[2]))] + activations\rupscaled_activation = np.ones((53, 73))\rfor layer in [5, 4, 3, 2, 1]: # Edit if you have a different # of layers  averaged_activation = np.mean(activations[layer], axis=3).squeeze(axis=0) * upscaled_activation\routput_shape = (activations[layer - 1].shape[1], activations[layer - 1].shape[2])\rx = tf.constant(\rnp.reshape(averaged_activation, (1,averaged_activation.shape[0],averaged_activation.shape[1],1)),\rtf.float32\r)\rconv = tf.nn.conv2d_transpose(\rx, layers_kernels[layer],\routput_shape=(1,output_shape[0],output_shape[1], 1), strides=layers_strides[layer], padding=\u0026#39;VALID\u0026#39;\r)\rwith tf.Session() as session:\rresult = session.run(conv)\rupscaled_activation = np.reshape(result, output_shape)\rfinal_visualisation_mask = upscaled_activation\rreturn (final_visualisation_mask - np.min(final_visualisation_mask))/(np.max(final_visualisation_mask) - np.min(final_visualisation_mask))\rAnd the only thing we need to implement is the code to animate the results and save them as a video:\nimport cv2\rimport numpy as np\rimport matplotlib.pyplot as plt\rfrom matplotlib import animation\rfrom IPython.display import display, HTML\rdef save_movie_mp4(image_array, filename=\u0026#39;output.gif\u0026#39;, fps=30):\rdpi = 72.0\rxpixels, ypixels = image_array[0].shape[0], image_array[0].shape[1]\rfig = plt.figure(figsize=(ypixels/dpi, xpixels/dpi), dpi=dpi)\rim = plt.figimage(image_array[0])\rdef animate(i):\rim.set_array(image_array[i])\rreturn (im,)\rwriter = animation.PillowWriter(fps=fps)\ranim = animation.FuncAnimation(fig, animate, frames=len(image_array))\ranim.save(filename, writer=writer)\rWe can now finally pass our model and our input data to the program and let it create the Saliency Map video for us:\nimport glob, re, time, datetime\r# The path to your dataset\r pathToData = \u0026#39;data/tub/\u0026#39;\r# Output video name\r output = \u0026#34;saliency.gif\u0026#34;\r# Output FPS\r fps = 60\r# Number of frames you want to use\r numberOfFrames = 600\rdef sort_human(l):\rconvert = lambda text: float(text) if text.isdigit() else text\ralphanum = lambda key: [convert(c) for c in re.split(\u0026#39;([-+]?[0-9]*\\.?[0-9]*)\u0026#39;, key)]\rl.sort(key=alphanum)\rreturn l\rinputImages = []\ralpha = 0.004\rbeta = 1.0 - alpha\rcounter = 0\rprint(\u0026#34;Generating %ds of video.\u0026#34; % (numberOfFrames/fps))\raccumulatedTime = 0\rstart = time.time()\rfor path in sort_human(glob.glob(pathToData + \u0026#39;*.jpg\u0026#39;)):\rinputImage = cv2.imread(path)\rsalient_mask = compute_visualisation_mask(inputImage)\rsalient_mask_stacked = np.dstack((salient_mask,salient_mask))\rsalient_mask_stacked = np.dstack((salient_mask_stacked,salient_mask))\rblend = cv2.addWeighted(inputImage.astype(\u0026#39;float32\u0026#39;), alpha, salient_mask_stacked, beta, 0.0)\rinputImages.append(blend)\rcounter += 1\rif counter \u0026gt;= numberOfFrames:\rbreak\relif counter % 100 == 0:\rend = time.time()\raccumulatedTime += end - start\rremainingSeconds = (accumulatedTime/counter)*(numberOfFrames-counter)\rprint(\u0026#34;Generated %d/%dframes.\u0026#34; % (counter, numberOfFrames))\rprint(\u0026#34;Estimated time left: %dm:%ds.\u0026#34; % divmod(remainingSeconds, 60))\rprint(\u0026#34;Runtime so far: %dm:%ds.\u0026#34; % divmod(accumulatedTime, 60))\rstart = time.time()\rAnd save the gif:\nsave_movie_mp4(inputImages, output, fps)\r   You can download the above code as an ipynb here.   SaliencyMap.ipynb  (11 ko)    The results The output video I got using the Nvidia model we made earlier and the dataset we downloaded:\n\rWe can see that the car indeed uses the lane lines, but it also uses the horizon as a feature a lot. That's quite interesting. We can get rid of that problem using a ROI crop or by implementing some computer vision feature extraction/engineering, which we'll do right after we make a high res version of the simulator. :)\rWe can also see that it's far more interested in the right line, than the middle (left) one. That's because, in general, the car tends to go to the right, since we're driving around the circuit clockwise. We need to do some data augmentation to solve this issue, which we'll also do a bit later.\n"
},
{
	"uri": "https://ori.codes/artificial-intelligence/",
	"title": "Artificial Intelligence",
	"tags": [],
	"description": "",
	"content": "Making the RC intelligent(ish) Artificial Intelligence In this chapter we'll finally begin working on the machine learning/deep learning part. Some would even call it AI. :)\n"
},
{
	"uri": "https://ori.codes/artificial-intelligence/camera-calibration/",
	"title": "Camera calibration",
	"tags": [],
	"description": "",
	"content": "Removing distortions Camera calibration In this subchapter we'll go through some basic types of camera distortions and how to fix them.\n"
},
{
	"uri": "https://ori.codes/rc-car/scale/",
	"title": "Scale",
	"tags": [],
	"description": "",
	"content": " Most RC cars are scaled down versions of their real-life equivalent, so they're expressed in ratios, the most common ones being (real-life size : RC model size):\n  1:18 1:16 1:10 1:8 there are also all sorts of scales in between those (and above/below)  Of course, the question is: why do we care, and what's better for a self-driving RC car?\nIt's pretty simple:\n a bigger RC car equals more real estate and more power to carry all of our gadgets on top of it, without damaging the motors while struggling with all of the weight, but a bigger car needs a bigger race track and road size we want to drive it on  "
},
{
	"uri": "https://ori.codes/artificial-intelligence/visualization/donkey-makemovie/",
	"title": "Visualization: donkey makemovie",
	"tags": [],
	"description": "",
	"content": "The makemovie command is a great tool to visually inspect and debug your model. Here are some example uses:\n  To create just a video of the training data, with an overlay that shows steering: donkey makemovie --tub=pathToYour/data/ --out=outputVideo.mp4\n\r  To create a video with an overlay of your model steering and the training data steering: donkey makemovie --tub=pathToYour/data/ --out=outputVideo.mp4  --model=yourModel.h5 --type=modelType\n\r  To create a video with a saliency map and both overlays: donkey makemovie --tub=pathToYour/data/ --out=outputVideo.mp4  --model=yourModel.h5 --type=modelType --salient\n  "
},
{
	"uri": "https://ori.codes/rc-car/body_type/",
	"title": "RC Car body types",
	"tags": [],
	"description": "",
	"content": " The best body type for on road self-driving purposes is the standard race body type.\n But to be thorough, we could roughly group all of the RC cars in 4 distinct categories:\nRACE/STREET Probably the first thing that comes to mind when thinking of an RC car, a standard race car. This body type is the fastest and the best on paved, flat surfaces and is meant for on road use only.\n There is also a specialized group of RC cars of this body type that are meant for drifting on racing tracks. The main difference are the tires, which are much slicker than regular ones.\nBUGGY Buggies are the golden middle between an off-road and on road RC car. That being said, they can go off-road, unlike a regular racing car, so that makes them the best bet for people that aren't sure if they want to go off-road or on road. They are the second best thing on the road but the slowest off-road.\n TRUGGY Truggies are also an in-between body type, but they lean more to the off-road role, as opposed to the buggies. Consider them a buggy with monster truck tires.\nThey're the second fastest body type off-road, and the third fastest on road.\n TRUCKS These are your regular monster trucks. For a self-driving application, they'd be pretty bad, since they tend to flip over while making on road turns, so it's best to stick with them only if you're planning to go off road in the woods, grass or dirt.\nThey are the best for going off-road but the slowest for going on road.\n "
},
{
	"uri": "https://ori.codes/artificial-intelligence/computer-vision-lane-finding/",
	"title": "Computer Vision: Lane Finding",
	"tags": [],
	"description": "",
	"content": "Coming up tomorrow "
},
{
	"uri": "https://ori.codes/rc-car/electric_motors/",
	"title": "Electric motors",
	"tags": [],
	"description": "",
	"content": "The main question concerning electric motors is: brushed or brushless?   Brushed pros: cheaper, simpler, better for off-road.\n  Brushed cons: heavier, bigger, worse power efficiency (75-80%), they wear out in time.\n  Brushless pros: long lifespan, much better speed and handling, better power efficiency (85-90%).\n  Brushless cons: much more expensive, worse for off-road.\n  So what should we get? It depends on your budget, but brushed motors work just fine, and besides, for self-driving purposes, you don’t need a RC car that drives 100 KPH. It’s always possible to swap out a brushed motor for a brushless later on.\n "
},
{
	"uri": "https://ori.codes/rc-car/servo/",
	"title": "Steering servo",
	"tags": [],
	"description": "",
	"content": "An RC servo is used for controlling the steering wheels of the car. It almost always comes with the RC car, so you shouldn’t worry about getting one.\nIt typically expects around 4.8V to 6V input on the power wire (varies by car) and a PWM control signal on the signal wire. Typically, the three wires are colored black-red-white, or brown-red-yellow, where:\n the dark wire (black/brown) is ground, and the center wire (red) is power, and the light wire (white/yellow) is control.  The control signal is RC-style PWM, where one pulse is sent 60 times a second, and the width of this pulse controls how left/right the servo turns. When this pulse is:\n 1500 microseconds, the servo is centered; 1000 microseconds, the servo is turned all the way left (or right) 2000 microseconds, the servo is turned all the way in the other direction.  This is NOT the same kind of PWM that you would use to control the duty cycle of a motor, or the brightness of a LED.\n The power for the servo typically comes from the motor ESC, which has a BEC (Battery Eliminator Circuit) built in.\nSource: DonkeyCar docs.\n"
},
{
	"uri": "https://ori.codes/rc-car/electronic_speed_controller/",
	"title": "Electronic Speed Controller",
	"tags": [],
	"description": "",
	"content": "The role of the ESC is to take a RC PWM control signal (pulse between 1000 and 2000 microseconds) in, and use that to control the power to the motor so the motor spins with different amounts of power in forward or reverse. Many RC car kits come with an ESC preinstalled, in which case you should be just fine.\nAgain, 1500 microseconds typically means \u0026ldquo;center\u0026rdquo; which for the motor means \u0026ldquo;dead stop.\u0026rdquo;\nThe battery typically connects straight to the ESC using thicker wiring than the simple control signals, because the motor draws many more amps than the control. The ESC then connects on to the motor with equally thick power wiring.\nStandard motors and ESCs have a peak current of about 12A; a 1/8th scale RC car with powerful brushless motor can have a peak draw up to 200A.\nAdditionally, the ESC typically contains a linear or switching voltage converter that outputs the power needed to control the steering servo; this is typically somewhere in the 4.8V to 6V range. Most BECs built into ESCs will not deliver more than about 1A of current, so it is not typically possible to power both the steering servo and the Jetson Nano from the BEC.\nThe main thing to look out for when getting an ESC on your own is to be sure to match your motor type to your ESC type (brushed/brushless).\n  Source: DonkeyCar docs.\n "
},
{
	"uri": "https://ori.codes/rc-car/receiver/",
	"title": "Receiver",
	"tags": [],
	"description": "",
	"content": " If you buy a \u0026ldquo;kit car\u0026rdquo; that is listed as \u0026ldquo;needs a receiver,\u0026rdquo; then you don't need to buy a receiver.\n The Jetson Nano and the PWM/Servo driver will replace the receiver, outputting control signals to the car. If you’re buying a kit with a steering servo, motor, and ESC, you should actually try to not get a receiver, since the RC car could be specifically designed for that receivers PWM signals, and you’ll be taking it apart anyways.\n"
},
{
	"uri": "https://ori.codes/rc-car/batteries/",
	"title": "Batteries",
	"tags": [],
	"description": "",
	"content": "There are two types of batteries used for RC cars: Nickel Metal Hydride batteries (NiMH) and Lithium Polymer batteries (LiPo).\nTL;DR: LiPo batteries are much better, but also more expensive.\nLithium Polymer batteries generally have higher current capacity (the amount of Amps the battery can deliver at one point while driving) as well as energy storage (the number of Amp Hours the battery stores when fully charged) so it may also last longer.\nThe amount of charge a battery can hold (how long it runs) is measured in Ampere-hours. The amount of current a battery can instantaneously deliver while driving is measured simply in Amperes.\n Amperes are often re-calculated in terms of multiples of the energy content, divided by one hour which is often called \u0026ldquo;C.\u0026rdquo; Thus, a LiPo rated for 10C and 2000 mAh, can deliver 20 Amperes of current while driving. A NiHM rated for 5C and 1100 mAh can deliver 5.5 Amperes of current while driving.\nBatteries can deliver more than the C rating for very short amounts of time, but will heat up or build up internal resistance so you shouldn’t count on that as being their standard capability.\nFor your custom car, be aware of the voltages needed for the ESC and the motor of the car, and make sure to get a battery that matches them in voltage.\nSmaller RC cars will come with NiMH for affordability, or 2S LiPo for power. Larger RC cars will use 3S (11.1V) or 4S (14.8V) or even 6S (22.2V) Lithium batteries, and thus need to have ESC and motor combinations to match.\nBe sure to get a charger that matches your battery.\n If you have a LiPo battery:\n get a good Lithium battery charger, with a balancing plug, never discharge a Lithium battery below 3.2V per cell  If you discharge a LiPo battery completely, it won’t ever be charged up to its normal voltage again, and trying to do so will overheat the battery and set it on fire.\n To prevent this, get a battery alarm and a LiPo charging bag:\n"
},
{
	"uri": "https://ori.codes/hardware/inventory/",
	"title": "Hardware inventory",
	"tags": [],
	"description": "",
	"content": "Let’s start with a list of all of the hardware I’ll be using through this tutorial, and of course, with the money shot:\nRC Car kit The RC Car I went with was the Tamiya TT-02, which came prebuilt and even included an receiver and a remote, for just a bit over 100€. Very lucky!\nThe reason I went with this is that it was just such a good deal for such a car. It’s 1:10 scaled, so I’ll have plenty of real estate for hooking all sorts of electronics on it, and it’s a really good car for on road driving. There’s plenty of mods available for it and plenty of extra parts, which I even got with it, such as extra wheel bases or additional mounting arms. After working with it for a couple months now, I’d highly recommend it.\nBe sure to pay attention to the level of pre-assembly your kit comes with. There are kits that can take days to assemble if you’re doing all by yourself, and there are kits that are completely pre-built out of the box (commonly denoted RTR - ready to run). If you’re up for a challenge, get a Tamiya kit that isn’t prebuilt. They have great instructions and it’s fun to get to know your car inside out. The difficulty of assembly also varies by car, and is often denoted on the box or in the instructions.\n Charger and batteries I got a Carson Expert Charger LiPo Compact 3A set, with a Carson 3000mAh LiPo battery, for under 30€, which is also pretty sweet!\nI’d very much recommend getting a ‘smarter’ charger, with a balancing connector so you know exactly how much your battery is charged. You’ll want to store them at around 70% of their capacity if you’re not using them.\n I also went ahead and got an extra Fconegy 6000 mAh LiPo battery for around 25€.\nAnd of course, per my warning on the batteries section at the RC Car primer, I got a LiPo alarm and a charging bag to avoid any unnecessary explosions, housefires or general mayhem. It’s just an extra 20€ or so, but it’s very much so worth it.\n![](/images/hardware/lipo bag.png)\nJetson Nano Ah, the 🧠z of the project, the Jetson Nano Developer Kit. It’s pretty much the way to go for 100€ or so, it packs a pretty decent GPU, 4x USB 3.0 ports, a real gigabit ethernet port, and you can punch a bit more power into it with a jumper and a 4A@5V power source with a barrel jack cable.\nJetson Nano: MicroSD card You’ll be needing a microSD card to run your Jetson Nano OS, and you should try getting a faster on (at least Class 10, preferably UHS-II or faster :)).\nI got an Sandisk Extreme 64GB A2, UHS-3 card for about 25€.\nYou don’t have to get a huge (in terms of storage) microSD if you’re planning on using an external SSD with your Jetson. I’d say 32GB would be just enough, but the price difference isn’t that big, so you can go for the 64GB version just ‘cause.\n Jetson Nano: Noctua Fan If you’re planning to run the Jetson at fully capacity (4A barrel jack mode), which you most definitely should, it’s gonna need some help stayin’ cool, so you should get a fan for it.\nYou can pretty much get any 5V, 4-pin PWM, 40x40mm size fan, but I’d very much recommend going for a Noctua NF-A4 fan. You can choose between the 10mm or 20mm version, just be sure to get the 5V version of the fan and not the regular 12V one.\nYou can get the 20mm version for about 10-15€.\nYou’ll also want a couple of self-tapping M3 screws or regular M2 or M2.5 screws with nuts for mounting the fan to the heatsink. I’d recommend going with the self-tapping M3 screws, it’s so much easier. Also watch out for some aluminum shavings when you mount the fan on it, you wouldn’t want them getting to the circuitry below and causing mayhem.\n Also, if you’re going with the 20mm version, get 25mm long screws. If you’re going with the 10mm version, get 15mm long screws.\n Jetson Nano: Wireless LAN and Bluetooth There are two options on getting your Nano (and your car) wirelessly connected:\n An M.2 WiFi + BT card USB WiFi and BT adapters  I’d highly recommend going for the M.2 option. I got the Intel 8265 card for about 10€.\nIf you’re getting the M.2 variant, you’ll need two antennas and an IPEX MHF4 to RP-SMA adapter cable. Pay attention to the connector types, it’s easy to get a wrong one, and vendors often misuse the labels, mixing IPEX, uFL, IPAX, IPX, MHF, and AM. Sometimes they’re right, sometimes they’re not.\n The Jetson Nano has a M.2 key E slot, just to keep in mind when getting a card. It also doesn’t work with NVME SSDs, if you’re wondering.\n Beware of the model of the m.2 card you’re getting. At the time of writing, the Jetson Nano SD Card Image natively supports only the 8265. Technically, you should be able to use any Intel card older than the 8265 since it’s supported in the 4.9 kernel, but some people had issues running even the 7265. If you’re up to it, you can build the core14 branch on the backports git, but if you know how to do that, you probably don’t need me telling it about you.\n Jetson Nano: Camera To enable the car to look around itself and navigate itself, you’ll need a camera. You can use pretty much any USB camera, or you can use a MIPI CSI camera with the Nano.\nThe cool thing about the Jetson Nano is that it’s compatible with almost all Raspberry Pi products, which includes camera.\n I went with the Raspberry Pi Camera v2, which uses the IMX219 sensor.\nI’d recommend getting an extra longer flat cable, so you don’t have to worry about the positioning of the camera relative to the Nano on the vehicle, since you only get a rather short one with the camera you buy.\n Now, about the almost all RPi products part mentioned above. If you’re buying a RPi Camera, make sure to get the newer version with the IMX219 sensor instead of the older (EOL) OV5647 sensor, since it’s not supported by the Jetson Nano image. Of course, you can build the drivers into the kernel yourself, but I’d go with the pragmatic solution on this one.\n I may also be using an Elephone EleCam Explorer Elite 4K cam I had lying around as an additional camera just for fun.\nPCA9685 Servo Driver To enable your Jetson Nano to interface with your steering servo and ESC, you’ll need a PWM/Servo Driver. I’ll be using the PCA9685 over I2C, which is pretty cheap and has lots of extra channels.\nYou might want to get some jumper cables if you don’t have any lying around, since you’ll have to connect this to the Jetson Nano’s GPIO pins somehow.\n Jetson Nano: Power bank To power all of this for a reasonable amount of time, you’ll want to get a pretty good power bank. I went with the RAVPower 20100 mAh USB-C power bank with 45W PD for about 50€.\nYou can pretty much get any power bank you’d like, but I’d recommend getting something above 10Ah and more than 2A if you plan on using the barrel jack connection on the Nano.\nJetson Nano: (Optional) External SSD First of all, why:\n With a microSD, on average you’ll get ~ 87 MB/s read speed with an access time of ~0.52 milliseconds. With a SSD, you’ll get an average read rate of ~367 MB/s and an access time of ~0.29 milliseconds.  So go ahead and get an NVMe SSD and an external USB enclosure for it. I’ll be using a LITE-ON SSD I had lying around after replacing my Razer Blade Stealth OEM one with a Samsung 970 Evo.\nGetting an external NGFF/M.2 SSD enclosure can be a very much hit-or-miss business if you’re trying to save a couple of bucks. I got lucky and got an off-brand one working at very decent speeds for some 20€. Other people I know haven’t been that lucky on their first attempt, so beware when buying it, make sure there’s an option to return it if it doesn’t work. :)\n Although this gives you about 4x more speed with your Nano, it is fairly technical and complicated and includes manually patching and building a Linux kernel, so I can only assume that you’re very very interested, since it’s so much fun. You’re here to build a nothing less than a self-driving vehicle, what’s a little kernel hacking compared to it. :)\n Jetson Nano: (Optional) Camera Lens Attachments If you’re getting the standard Raspberry Pi Camera, it won’t have much of a FOV by itself. One thing you can do, is get one of the clip-on attachment lenses for smartphones, which can greatly increase its FOV.\nI got something like this:\nThese lenses are what they are, cheap. With cheap lenses come all sorts of nasty stuff, like chromatic aberration and barrel distortion. Try to look at some of the customer reviews that contain photos and find one with a bit less distortion.\n The Rest: Tools and stuff Of course, you’ll need some basic tools, such as:\n A good scalpel A cutting board A hand drill (depending on what you’re planning to use as your base) Screwdrivers with an assortment of M3 screws (various lengths, with nuts)  But that’s pretty much it for the inventory. On to building it!\n"
},
{
	"uri": "https://ori.codes/software/kernel-hacking/",
	"title": "Running the OS from an external SSD using a custom kernel",
	"tags": [],
	"description": "",
	"content": "This is the technical, optional upgrade that will enable you to boot your OS from an external SSD.\nSyonyk has an awesome Jetson Nano guide with all of this stuff explained and was the primary source of info while researching how to do this.\n JetsonHacks also have a guide that should make this much easier to do, as they’ve prepared scripts that do all of the work for you, but I haven’t tried it so I can’t say it works for sure, but it should!\n This will take some time. Taking kernels apart, patching them and building them takes a while even on your regular workstations or servers, but doing it on an embedded device is a whole new world of pain when it comes to waiting for stuff to build, compile or extract. Be prepared to be patient while you’re doing this.\n So what is it we’re exactly planning to do here, and why? We want to use our external SSD as the root filesystem because of the huge performance boost it gives us. But the problem is this:\n The USB 3 ports require the kernel to load some firmware on boot to enable them to work, which means the USB ports won’t work until the device boots up That firmware is stored on the root filesystem, which we want to, you know, put on our external SSD That means we’d need the firmware from our SSD in order to use our SSD…  So, how can we work this out? Simple: patch the kernel and embed the firmware we need right into it, so it doesn’t need to read it from the root filesystem, which we can then freely put on our external SSD.\nWhile we’re at it, there’s another thing that would give us a noticeable performance boost which we could build into the kernel, same-filled page merging support for zswap.\nOkay, hack all the kernels All of the steps written below are meant to be run directly on your Jetson Nano.\n First of all, since the Nano doesn’t run the stock Linux kernel, we’ll need the sources for its custom kernel. Miraculously Nvidia actually provides it.\n Download the BSP Sources from the right side, under Jetson Nano on the L4T download page. Unpack the archive Unpack the kernel source from the public sources folder to your home directory Open the kernel folder  Steps shown below:\ncd Downloads\rtar -xf public_sources.tbz2\rcd ~\rtar -xf ~/Downloads/public_sources/kernel_src.tbz2\rcd ~/kernel/kernel-4.9\rOnce we’re in the kernel directory, we’ll save the current kernel configuration from the Nano to it:\nzcat /proc/config.gz \u0026gt; .config\rNow we need to copy the USB firmware from the root filesystem into the kernel directory, since we want to pack it together with the kernel:\ncp /lib/firmware/tegra21x_xusb_firmware ./firmware/\rNow we need to change the kernel config, and since we’re using a GUI desktop environment on the Nano, we can just use menuconfig:\nsudo apt-get install libncurses5-dev\rmake menuconfig\rYou should see something like this:\n![](/images/kernel hacking/menuconfig1.jpg)\nWhen selecting kernel features in menuconfig, * denotes a built-in feature and M denotes a module. You’ll be wanting stars for the features you want to build in.\n  Go to Kernel features:  Select: Enable frontswap to cache swap pages if tmem is present Select: Compressed cache for swap pages (EXPERIMENTAL) (NEW) Select: Low (Up to 2x) density storage for compressed pages    We didn’t select the 3x allocator (z3fold) because isn't reliable prior to mid-4.20 kernel versions. With same-filled page merging, the results are about the same as the zbud 2x allocator.\n  Exit Go to Device Drivers  Generic Driver Options  External firmware blobs to build into the kernel binary  Type: “tegra21x_xusb_firmware”     Exit   Save the new configuration  ![](/images/kernel hacking/menuconfig2.jpg)\n![](/images/kernel hacking/menuconfig3.jpg)\nNow we’ll update ZSwap to enable same-filled page compression. The kernel that the Jetson Nano is running at the time of writing is 4.9, which doesn’t include it, but it can easily be backported from a newer kernel.\nOne of the best things about Linux, what makes it what it is, is that it’s available for everyone, and we can find it on Linus Torvalds’ GitHub:\n Go to the Linux kernel source on GitHub Go to mm/zswap.c Take a look at the commit history by clicking on the History button Find the zswap: same-filled pages handling commit  On GitHub, you can add .patch at the end of the URL to get the patch file for the commit you’re looking at. Just be sure to remove the #diff parameter from the URL if you’re looking at the diff from the commit.\n  Download the patch file for the same-filled pages commit Patch the kernel using patch -p1 and the downloaded patch There is a memset_l call in the patch we’ve just applied, and it doesn’t exist yet in the 4.9 kernel, so we’ll need to replace it with the regular memset.  See instructions below:\nMake sure you’re in the kernel-4.9 directory before proceeding.\n # You should be in the Downloads directory\rwget https://github.com/torvalds/linux/commit/a85f878b443f8d2b91ba76f09da21ac0af22e07f.patch\r# Change to the kernel directory\rcd ~/kernel/kernel-4.9\rpatch -p1 \u0026lt; ~/Downloads/a85f878b443f8d2b91ba76f09da21ac0af22e07f.patch\r# Replace the nonexistent memset_l call with the regular memset\rsed -i \u0026#39;s/memset_l(page, value, PAGE_SIZE \\/ sizeof(unsigned long));/memset(page, value, PAGE_SIZE);/g\u0026#39; mm/zswap.c\r![](/images/kernel hacking/patching kernel.png)\nBuilding and installing the kernel Now that we’ve made all of the changes we wanted to the kernel, we need to build it and place it on the /boot partition.\nThis will take a while.\n # Make sure we\u0026#39;re in the kernel directory\rcd ~/kernel/kernel-4.9\rmake -j5 # -j denotes the number of threads\rsudo make modules_install\rsudo cp /boot/Image /boot/Image.dist\rsudo cp arch/arm64/boot/Image /boot\r![](/images/kernel hacking/building kernel.png)\nAfter this is done, you can reboot. If the Nano boots successfully, it means that you’re running your new custom kernel. You can run uname -r to check it:\n# The stock kernel returns \u0026#39;4.9.140-tegra\u0026#39;\r# Your custom kernel should return only \u0026#39;4.9.140\u0026#39;\runame -r\rIf all is well, we can transfer our root partition to the external SSD.\nThis will also take a while when you get to the copying of the root filesystem to the SSD.\n  Plug the SSD in Wipe the partition table Create a GPT partition table Create a new EXT4 volume 4 gigabytes smaller than the SSD Create a 4 gigabytes swap partition  See steps below:\n# Wipe the partition table\rsudo dd if=/dev/zero of=/dev/sda bs=1M count=1\r# Create a GPT partition table, then create a new EXT4 volume\r# Create a Linux swap partition (4GB) - arrow over to \u0026#34;Type\u0026#34; and select \u0026#34;Linux swap\u0026#34;\r# Go over to \u0026#34;Write\u0026#34; and type \u0026#34;yes\u0026#34; and then quit\rsudo cfdisk /dev/sda\r# Make an ext4 volume and a swap partition\rsudo mkfs.ext4 /dev/sda1\rsudo mkswap /dev/sda2\r# Mount the partition and copy the root filesystem to it\rsudo mkdir /mnt/root\rsudo mount /dev/sda1 /mnt/root\rsudo mkdir /mnt/root/proc\rsudo apt -y install rsync\rsudo rsync -axHAWX --numeric-ids --info=progress2 --exclude=/proc / /mnt/root\r Edit /boot/extlinux/extlinux.conf so that the kernel points at /dev/sda1 instead of /dev/mmcblk0p1 (the microSD) Enable zswap in extlinux.conf  sudo sed -i \u0026#39;s/mmcblk0p1/sda1/\u0026#39; /boot/extlinux/extlinux.conf\rsudo sed -i \u0026#39;s/rootwait/rootwait zswap.enabled=1/\u0026#39; /boot/extlinux/extlinux.conf\rReboot, and you should be running from the USB SSD.\nIf you mess something up and the Nano doesn’t boot, don’t worry, you can always plug the microSD into a Linux PC and mount it, go to the boot partition and open the extlinux/extlinux.conf file and replace sda1 with mmcblk0p1 so it boots from the microSD.\n If you somehow end up messing up the extlinux.conf file, I’ve provided a copy for you down below, so you can overwrite it as stated in the tip above and boot back to the microSD:\n TIMEOUT 30\rDEFAULT primary\rMENU TITLE p3450-porg eMMC boot options\rLABEL primary\rMENU LABEL primary kernel\rLINUX /boot/Image\rINITRD /boot/initrd\rAPPEND ${cbootargs} rootfstype=ext4 root=/dev/sda1 rw rootwait\rTweaking swap One last thing to do after you boot from your SSD, enable swap:\necho \u0026#34;/dev/sda2 none swap \\\rdefaults 0 1\u0026#34; | sudo tee -a /etc/fstab\rAnd you’re done. Congrats on hacking the kernel and running your Nano from an external SSD. It’ll be worth it!\n"
},
{
	"uri": "https://ori.codes/hardware/building-the-car/",
	"title": "Assembling the RC Car",
	"tags": [],
	"description": "",
	"content": "Now to the fun part: building the RC car. This will vary very much based on your RC Car kit. If it’s ready-to-run (RTR), you should be good out of the box. If it’s an unassembled kit, you’ve got a ton of work ahead, just follow the instructions that came with the car. If you’re like me, and got something in between, you’ll have just a bit of work before running it.\nIn my case, I had to assemble the wheels and mount them on the car, and that was pretty much it. Other than that, I had to calibrate my ESC as per the instructions and my car was ready to go.\n\rI found it helpful to have a ‘bro nearby that’s willing to help you assemble and Google stuff for you since you’re lazy to read the instructions completely. YMMV based on the brother that’s available to you.\n Pics or it didn’t happen Test drive After assembling the car, take it for a spin. If it works, you’re ready to move on to the electronics part of the project. :)\n\r"
},
{
	"uri": "https://ori.codes/software/donkeycar/",
	"title": "DonkeyCar",
	"tags": [],
	"description": "",
	"content": " Donkey is a high level self driving library written in Python. It was developed with a focus on enabling fast experimentation and easy contribution.\nSource: Official Donkey docs\n We'll be using Donkey® as an interface between our RC car and the neural net we'd like to drive it for us.\nAs you can see above, we'd like to send the camera data from our RC to a model which would analyse it and tell the RC where to steer and how fast to go, in order to stay on the road.\nDonkey will provide us with an interface to do just so, without having to worry about the details of:\n (Pre) Processing the camera data Converting the steering/throttle values to actual signals for the RC to understand Actually steering the RC and increasing/decreasing throttle  It will also enable us to:\n Collect and label training data Define and train custom models Control the car using a Web interface or a gamepad And even use a simulator to rapidly test and train our models  How to get it up and running There are two ways to go about this:\n Doing everything on your RC car/Jetson Nano, including model training Training and developing the model on a host PC, running and testing it on your RC  I'd very much recommend going with the second option, since you'll need the extra horsepower of a PC in order to be able to train and develop complex models, preferably with a GPU. But, if you want, you can disregard my advice and skip installing the Donkey to your host PC and just install it on your RC and do everything from there.\nWe'll continue our project by explaining how to:\n Install Donkey on your host PC Install a simulator on your host PC Install Donkey on your RC Calibrate your RC and actually control it using Donkey, via the Web interface or a gamepad Do a sanity check by training a very simple model to see if everything works as it should  Further Donkey resources Before we get to work, here are some links to Donkey resources you can check out to get familiar with it, and maybe better understand how it works:\nThe official Donkey documentation and the Slack channel/Discourse group I highly recommend going through these two. The docs are great and the Slack channel has a bunch of people trying to do the same thing you are, who love helping one another solve issues and bounce ideas. I love the Donkey community and can't recommend enough going over to the channel and just saying hi.\nOne more thing; The Slack channel for the Donkey community is getting too big to be usable with the free plan, so you can only see messages going back a couple of months. That's why they're trying to migrate to the new Discourse group, so I'd recommend going there instead of the Slack channel.\nThe following videos are also an interesting watch, albeit not necessary to continue with the tutorial. One of the reasons why we're using Donkey is so we don't have to worry about all of the details it solves for us. But if you're like me, you're gonna wanna know a bit how it works before using it to piece your RC together. :)\nCircuitBread's overview of the Donkey platform with Ed Murphy   DonkeyCar's founder, Adam Conway's video of assembling a DonkeyCar   Make magazine's video of building a Raspberry Pi DonkeyCar   Tawn Kramer's two part overview of the DonkeyCar framework     William Roscoe's quick get started video)   "
},
{
	"uri": "https://ori.codes/software/donkeycar-host/",
	"title": "DonkeyCar installation: Host PC",
	"tags": [],
	"description": "",
	"content": "Let's install the Donkey software on your host PC. The only part where this differs between the three platforms, Mac OS X, Linux and Windows, is in the Miniconda software installation, so we'll get that out of the way first.\nMac OS X  Download and install:  Miniconda here, git here   Open up a terminal and follow the rest of the tutorial  Windows  Download and install:  Miniconda here, git here   Open an Anaconda Prompt via Start Menu and follow the rest of the tutorial  Linux  Download Miniconda here and install it Open up a terminal and follow the rest of the tutorial  The rest of the tutorial:   Go to a place where you want the stuff we'll be working on to be.\n# e.g. on Linux or Mac\rcd ~\r# e.g. on Windows\rcd c:\\\\Users\\\\yourUsername\r  Make a folder for your projects and cd to it:\nmkdir projects\rcd projects\r  Clone the Donkey repository using git:\ngit clone https://github.com/autorope/donkeycar\rcd donkeycar\rgit checkout master\r  Create the Python Anaconda environment using the yml file from the repository:\n# Windows\rconda env create -f install\\envs\\windows.yml\r# Mac\rconda env create -f install\\envs\\mac.yml\r# Linux/Ubuntu\rconda env create -f install\\envs\\ubuntu.yml\r# All three OS\u0026#39;s\rconda activate donkey\rpip install -e .[pc]\r  If you're not using a host PC with a GPU, you're done! If you're using a NVidia GPU and not using a Mac (sorry, no TensorFlow GPU support for you folks):   Install the TensorFlow software requirements for Nvidia GPUs, which basically means:\n  Download and install NVIDIA drivers\n  Download and install the CUDA Toolkit\n  Download and install cuDNN (you should just copy the bin, lib and include folders from the zip to your cuda installation folder)\n  Download and install TensorRT 5.0 to improve latency and throughput for inference on some models (same as the above)\n  Which means installing PyCUDA (make sure nvcc is in your PATH):\npip install \u0026#39;pycuda\u0026gt;=2017.1.1\u0026#39;\r If you're getting errors, check the requirements here. If you're on Windows, you probably need the VS C++ 14, just download it through the VS Build Tools    Downloading and installing TensorRT 5.0\n      Then you can finally:\nconda install tensorflow-gpu==1.13.1\r  "
},
{
	"uri": "https://ori.codes/hardware/mounting-plates/",
	"title": "Building the mounting plates for the hardware",
	"tags": [],
	"description": "",
	"content": "There are many options to go for when it comes to mounting your Nano and the rest of the hardware to your RC car.\nOption 1: 3D printing If you’re planning on 3D printing your parts, take a look at:\n The official DonkeyCar docs which contain a lot of 3D models for printing: chassis and adapters, mounting plates, more mounting plates, etc. Markku.ai’s Chilicorn Rail for the Tamiya cars  I actually used the Chilicorn Rail for the first iteration of my build, and was very lucky to have been introduced by my mentor to Mitch, who printed out the parts and helped me out with a ton of stuff since then:\nEven if you’re not planning on using this method, I’d still recommend at least getting the RPi camera mount 3D printed, since it’s kinda difficult to get it to the right angle and position if you’re making something out of raw materials yourself.\n Option 2: Hack something together yourself If you’re planning on hacking something together by yourself, you can pretty much do anything you want. I used a special aluminium plate I got as a donation for my thesis from my mentor’s father :):\nAfter some modeling and tinkering, I came up with the following idea for my mounts:\n\rThe only thing left now is to measure the aluminium, mark down where to drill the screw holes and assemble it together, but being the lucky man that I am, my hand drill broke down just as I wanted to drill the aluminium, but fear not, I had this bad boy standing by:\rAnd after measuring up the dimensions and taking my materials, much to my surprise, I successfully drilled up my plates and got this:\nWith some zip-tie magic and screwing around 🙃 this was the end result:\nNevermind the disconnected antennae, we’ll get to that in a second.\nThat should be it as far as the ‘dumb’ part of the hardware is concerned. On to connecting all of the stuff together and setting up the Nano! "
},
{
	"uri": "https://ori.codes/software/donkeycar-simulator/",
	"title": "DonkeyCar Installation: The Simulator",
	"tags": [],
	"description": "",
	"content": " Even if you don't have an RC car, you can start here and follow the rest of the project by just substituting the RC car with the simulator!\n \rThis is one of the coolest parts of DonkeyCar for me, and probably one of the most useful ones. It's also a good way to get your feet wet with this kind of a project without building an actual RC. If it turns out you like it, you can always go back to the beginning and build an actual platform.\nLet's get it up and running:   Download and unzip the simulator for your platform from the DonkeyCar GitHub release page\n  Place the simulator into your projects folder (where you cloned the Donkey repo)\n  Install DonkeyGym:\ncd ~/projects\rgit clone https://github.com/tawnkramer/gym-donkeycar\rconda activate donkey\rpip install -e gym-donkeycar\r  Create a new Donkey application for your simulator:\ndonkey createcar --path ~/mysim\rcd ~/mysim\r  Edit the myconfig.py file inside the application folder you just created (mysim):\n# Enables the Donkey Gym simulator wrapper\r DONKEY_GYM = True\rDONKEY_SIM_PATH = \u0026#34;/home/wherever/your/projects/are/DonkeySimFolder/DonkeySim.exe\u0026#34;\r# Choose the track you want to run, you can change this later\r DONKEY_GYM_ENV_NAME = \u0026#34;donkey-generated-track-v0\u0026#34;\r  Download this test dataset that contains data of a car recovering from dropping out from the track and some standard driving data and put it in your data folder inside your application folder (/mysim/data/)\n  Train your model by running\npython manage.py train --model models/mymodel.h5\r You can choose different architectures or create your own by going into the DonkeyCar parts folder (/projects/donkeycar/parts/) and opening up the keras.py script. You can define a new class that inherits the base class for models and implement your own neural network architecture, but we'll get to that further on in the project! If you've created your own architecture/class, you can train the model using it by passing the flag \u0026ndash;type=yourClassName Some of the built-in models are: categorical, linear, rnn, 3d, latent, etc. When using your custom model to drive the car, if you get some dimensions errors, you're probably forgetting to pass the \u0026ndash;type flag with your class name while running it, it should fix it.    Test your model by running\npython manage.py drive --model models/mymodel.h5\r Open up your browser and go to: http://localhost:8887 and set the mode to Local Pilot and watch your car go!    If you're using Linux, you can also pass the \u0026ndash;js parameter and use your gamepad if it's mounted to /dev/js0\n  Download this big dataset that contains 16 different venues with tape lined tracks on concrete and tile (some are on carpet and cobblestones)\n Credits to Tawn from the Donkey Slack channel    The dataset is big. And it contains a lot of small files, which means you should pay attention where you're extracting the files, since moving/copying them will take a long while since OS's don't like working with millions of small files.\n "
},
{
	"uri": "https://ori.codes/software/donkeycar-rc/",
	"title": "DonkeyCar installation: RC car",
	"tags": [],
	"description": "",
	"content": "Connecting to your RC via SSH To connect and work with your RC throughout the rest of the project, you'll need two things:\n An SSH client The IP address of your RC  SSH Clients: If you're using Linux or a Mac, you're all set. They come with a SSH client pre-installed, and you just need to open up a terminal and type:\nssh username@ipAddress\rIf you're using Windows, you need to install one. I'd recommend using MobaXTerm:\n  Download the installer or the portable version and install/unpack it\n  To SSH to a device:\n  Open MobaXTerm\n  Press on the Start local terminal button\n  ssh username@ipAddress\r    SSH via Ethernet: If your RC is connected to your network via an ethernet cable, you should be able to find your IP address:\n  Through your Router/Gateway interface, by looking at the DHCP leases;\n  Or by connecting your Nano to a monitor, keyboard and mouse, opening up a terminal and writing:\nip addr show\r  SSH via WiFi: I would highly recommend this approach, so you can connect your RC to a WiFi network and take it and your laptop with you and connect to it on the fly.\nTo connect it to a WiFi network, you either need to first SSH into it over ethernet, or connect it to a monitor, keyboard and mouse, and do the following:\n  Connect to a WiFi network:\nnmcli device wifi connect YOUR-SSID password YOUR-PASSWORD\r  Or make the Jetson into a hotspot so you can connect your laptop to it:\nnmcli dev wifi hotspot ifname wlan0 ssid HOTSPOT-SSID password HOTSPOT-PASSWORD\r  What I like to do is connect it to both my home network, and a hotspot on my mobile phone, so I can use it anywhere and still have Internet access on both it and my laptop.\nTo do so, I use the nmcli autoconnect.priority property, so my home network has a higher priority than my phone hotspot, in case I forget to turn it off while I'm at home, so it doesn't eat up my data plan.\nYou can find all of your network connections saved in /etc/NetworkManager/system-connections/, which you can open up with a text editor and edit the autoconnect.priority property for each network. The higher the integer you assign to it, the higher the priority.\nAs an example, the network connection profile for my hotspot looks something like:\n[connection]\rid=Hotspot\ruuid=random-long-string\rtype=wifi\rautoconnect_priority=2 # The home network has a priority of 3, in my case\rpermissions=\rIf your Nano keeps dropping the connection for some reason, try disabling the power saving mode found in /etc/NetworkManager/conf.d, using a text editor.\nAlso, I assumed your Nano already has the nmcli or the NetworkManager utility installed, since it, at the time of writing, comes pre-installed with any Ubuntu distro. If, for some reason, you don't have it, you can install it using sudo apt install network-manager.\nAfter connecting your Nano to a WiFi network you want, find out its IP Address by opening up a terminal and typing:\nip addr show\rNow you can use your SSH client and SSH into the Nano, type in your username and password and you're ready to follow the rest of the tutorial. Dependencies Open up a terminal on your Nano and install the following dependencies:\nsudo apt-get update\rsudo apt-get upgrade\rsudo apt-get install build-essential python3 python3-dev python3-pip libhdf5-serial-dev hdf5-tools nano ntp\rSet up a Virtual Env # Install the venv package\rpip3 install virtualenv\rpython3 -m virtualenv -p python3 env --system-site-packages\r# Activate the venv automatically at boot\recho \u0026#34;source env/bin/activate\u0026#34; \u0026gt;\u0026gt; ~/.bashrc\rsource ~/.bashrc\rCompiling and installing OpenCV First, since OpenCV needs more than 4GB of RAM to be built from source, and our Jetson Nano just doesn't have that much RAM, we have to define some swap space to prevent it from going bonkers while compiling it:\n# Allocates 4G of additional swap space at /var/swapfile\rsudo fallocate -l 4G /var/swapfile\r# Permissions\rsudo chmod 600 /var/swapfile\r# Make swap space\rsudo mkswap /var/swapfile\r# Turn on swap\rsudo swapon /var/swapfile\r# Automount swap space on reboot\rsudo bash -c \u0026#39;echo \u0026#34;/var/swapfile swap swap defaults 0 0\u0026#34; \u0026gt;\u0026gt; /etc/fstab\u0026#39;\r# Reboot\rsudo reboot\rNow, we need to get all the prerequisites needed to build OpenCV from source:\n# Update\rsudo apt-get update\rsudo apt-get upgrade\r# Pre-requisites\rsudo apt-get install build-essential cmake unzip pkg-config\rsudo apt-get install libjpeg-dev libpng-dev libtiff-dev\rsudo apt-get install libavcodec-dev libavformat-dev libswscale-dev libv4l-dev\rsudo apt-get install libxvidcore-dev libx264-dev\rsudo apt-get install libgtk-3-dev\rsudo apt-get install libatlas-base-dev gfortran\rsudo apt-get install python3-dev\rOkay, let's download the source code for OpenCV which we'll be building it from:\n# Create a directory for opencv\rmkdir -p projects/cv2\rcd projects/cv2\r# Download sources\rwget -O opencv.zip https://github.com/opencv/opencv/archive/4.1.0.zip\rwget -O opencv_contrib.zip https://github.com/opencv/opencv_contrib/archive/4.1.0.zip\r# Unzip\runzip opencv.zip\runzip opencv_contrib.zip\r# Rename\rmv opencv-4.1.0 opencv\rmv opencv_contrib-4.1.0 opencv_contrib\rAlso we'll need numpy in our virtual environment for this to work:\n# Install Numpy\rpip install numpy\rWe also need to make sure CMake correctly generates the OpenCV bindings for our virtual environment:\n# Create a build directory\rcd projects/cv2/opencv\rmkdir build\rcd build\r# Setup CMake\rcmake -D CMAKE_BUILD_TYPE=RELEASE \\\r -D CMAKE_INSTALL_PREFIX=/usr/local \\\r -D INSTALL_PYTHON_EXAMPLES=ON \\\r -D INSTALL_C_EXAMPLES=OFF \\\r -D OPENCV_ENABLE_NONFREE=ON \\\r # Contrib path\r-D OPENCV_EXTRA_MODULES_PATH=~/projects/cv2/opencv_contrib/modules \\\r # Your virtual environment\u0026#39;s Python executable\r# You need to specify the result of echo $(which python)\r-D PYTHON_EXECUTABLE=~/env/bin/python \\\r -D BUILD_EXAMPLES=ON ../opencv\rThe cmake command shows a summary of its configuration, and you should make sure that the Interpreter is set to the Python executable of your virtual environment, not the base OS one.\nNow, to compile the code from the build folder, run the following:\nmake -j2\r# Install OpenCV\rsudo make install\rsudo ldconfig\r This will take a while. And by a while, I mean: Go grab a cup of coffee and watch a TV Show or a movie or something while.\n Now we just need to link it to our virtual environment:\n  cd to: /usr/local/lib/python[YOUR.VERSION]/site-packages/cv2/python[YOUR.VERSION] and do ls to find out the exact name of the .so we built.\n  It should look something like: cv2.cpython-[YOURVERSION]m-[***]-linux-gnu.so\n  Rename it to cv2.so: mv cv2.cpython-whatever-the-full-name-is.so cv2.so\n  And finally:\n  # Go to your virtual environments site-packages folder\rcd ~/env/lib/python[YOUR.VERSION]/site-packages/\r# Symlink the native library\rln -s /usr/local/lib/python[YOUR.VERSION]/site-packages/cv2/python-[YOUR.VERSION]/cv2.so cv2.so\r    To make sure everything works as it should, run:\nimport cv2\r# Should print 4.1.0\r print(cv2.__version__)\rInstall DonkeyCar First, go to a directory where you'd like your stuff to be:\n# Probably\rcd ~/projects\rInstall the latest Donkey from GitHub:\n# Clone it from GitHub\rgit clone https://github.com/autorope/donkeycar\rcd donkeycar\r# Checkout the master branch\rgit checkout master\rpip install -e .[nano]\rpip install --extra-index-url https://developer.download.nvidia.com/compute/redist/jp/v42 tensorflow-gpu==1.13.1+nv19.3\r"
},
{
	"uri": "https://ori.codes/hardware/jetson-nano-installation/",
	"title": "Preparing the Jetson Nano",
	"tags": [],
	"description": "",
	"content": "Before we begin assembling our hardware together, we should first prepare our Jetson Nano by installing an OS on it and verifying everything works before it gets buried among all the other hardware on the RC.\nPreparing the microSD First, we'll prepare the microSD by installing an OS on it for the Nano to run.\nThe official Jetson Nano docs are great and you can just follow them until the Next Steps step.\n If you want a TL;DR version:\n  Download the Jetson Nano Developer Kit SD Card Image\n  Format your microSD card and then flash the image on it using whatever tool you’d like.\n You can use Etcher    Insert the microSD into the Jetson and connect your peripherals to it: monitor, keyboard, mouse and an ethernet cable.\n  If you’re using Windows, after the flashing it’ll complain you need to format the microSD before you use it, since it doesn’t recognize the Linux filesystem you just flashed on it. Don’t format it, just ignore the warning.\n First boot The TL;DR version would be:\n Carefully read through the NVIDIA Jetson software EULA and decide if you’ll accept it 🙃 Select the system language, keyboard layout and time zone Create your user and name your Jetson Log in  If all went well, you should see the following screen:\n"
},
{
	"uri": "https://ori.codes/hardware/assembling-the-nano/",
	"title": "Assembling the Jetson Nano",
	"tags": [],
	"description": "",
	"content": "Now we can finish up our Nano by connecting the WLAN card, microSD and the fan to it.\nPlugging in the microSD I actually already did a lot of assembly some time ago, and I'm writing this in retrospect, so don't be worried if my Nano has a lot of stuff already hooked up to it and if it looks a bit different than yours, just focus on the stuff we're going through and disregard the rest.\n First, we have to take out the Nano module from the breakout board we got it with. We begin by unscrewing these two Phillips screws, highlighted in yellow:\nAfter unscrewing them, pull these two levers outwards (highlighted in yellow) and your Nano module will pop out:\nThe Nano should pop out like this:\nAnd after pulling it out, you can plug in your microSD on the bottom side:\nConnecting the Noctua fan to the Nano On the other side of the module, you can find the heatsink with four screw holes into which, if you haven't done so already, you can screw in the fan you bought for your Nano:\nYou also need to plug in the 3-pin (or 4-pin) fan connector to the Jetson board right below the ethernet port:\nPlugging in the WLAN+BT card The only thing left is the wireless card, which is plugged into the slot at the middle of the breakout board. You just need to unscrew one Phillips screw, insert the card, put the screw back in and connect your antennae connectors to the WLAN card. The antennae are the same, so don't worry where to plug which one, it doesn't matter.\nAnd that's it for the Nano board! After putting it all back together, and connecting the antennae, it looks something like this:\n"
},
{
	"uri": "https://ori.codes/hardware/connecting-the-car-to-the-nano/",
	"title": "Connecting the RC to the Nano",
	"tags": [],
	"description": "",
	"content": "Now comes the part that should differ the most, based on the RC you got. But don't worry, it's very much doable no matter the RC you got!\nFinding your ESC/Servo The first thing you should do is find your ESC and your Servo connectors, which should be a three wire connector coming from your RC car.\nIf you've bought a car that came with a wireless receiver, both the connectors should be connected to it. Here's what it looked like on my RC:\nIf you didn't have a wireless receiver, you should be able to see a connector coming out of your ESC, which is hooked to the RC motor, and a connector coming out of your steering servo, found at the front of your RC.\nHere's a closeup of the two connectors connected to the receiver, with the first channel being the steering servo and the second one being the ESC, in my case:\nOkay, we found the connectors, how do we hook them up to our car?\nEnter: the PCA9685 We connect them to the first two channels (0 and 1) of our PCA9685, highlighted in green and blue:\nIt doesn't matter which channel you connect the servo to, and which one the ESC to, just take note which one went where, you'll need it later on. You can see the numbers denoting the channels above the three pins. I've connected mine to the zeroth and first channel:\nOkay, so we hooked up our car to the PCA9685. Now we need to hook it up to our Nano to be able to control the car through it.\nBut first, let's talk a bit about the way the Nano communicates with the RC through the PCA9685, using a protocol called I²C (Inter-Integrated Circuit) (pronounced: I-squared-C).\nFeel free to skip this part if you aren't interested in the alternatives to I²C and why we're using it. You don't need to understand this stuff in order to continue with the tutorial. That being said, it wouldn't hurt to get to know a few other protocols, just to know what's out there and how they differ.\n CLICK HERE TO SKIP I2C What the heck is I²C?\nIt was designed back in the \u0026lsquo;80s by Philips, to enable components on a circuit board easily communicate with each other. So it's a protocol components use to talk to each other.\nIf you want to read a bit more, here's the specification by NXP, since Phillips semiconductors migrated to NXP back in 2006.\nThere's also a pretty neat primer you can read on the i2c-bus.org site.\nWhy I2C? Alternatives? This is basically a TL;DR of the awesome SparkFun I²C tutorial, be sure to check it out if you wanna go into more depth. I'll assume you have some basics of UART and SPI while going through them.\nIf you are interested in this kinda stuff, but haven't really done any embedded/electronics work before, please do check out the Engineering Essentials tutorials on SparkFun, they're great.\n Serial UART:  Serial ports are asynchronous (no CLK), so both devices have to agree on a data rate ahead of time: any excessive clock difference will result in garbled data. Requires hardware overhead: the UART at either end is relatively complex and difficult to accurately implement in software if necessary. At least one start and stop bit is a part of each frame of data, meaning that 10 bits of transmission time are required for each 8 bits of data sent, which eats into the data rate. Two, and only two devices: bus contention (where two devices attempt to drive the same line at the same time) is always an issue and must be dealt with carefully to prevent damage to the devices in question, usually through external hardware. Data rate - only a fixed number of baud rates available, highest of which is usually 230400 bits per second.  SPI:  Number of pins needed: four lines for a single master -\u0026gt; slave connection and each additional slave needs another chip select I/O pin on the master. A larger number of devices rapidly proliferates the connections - routing signals sucks, sometimes impossible in tight PCB situations. It's pretty good for high data rate full-duplex connections (10MHz - bits), scales nicely. The hardware is pretty simple - a shift register, easy software implementation.  I2C  I2C requires a mere two wires, like asynchronous serial, but those two wires can support up to 1008 slave devices. Unlike SPI, I2C can support a multi-master system, allowing more than one master to communicate with all devices on the bus (although the master devices can't talk to each other over the bus and must take turns using the bus lines). Data rates fall between asynchronous serial and SPI; most I2C devices can communicate at 100kHz or 400kHz. There is some overhead with I2C; for every 8 bits of data to be sent, one extra bit of meta data (the \u0026ldquo;ACK/NACK\u0026rdquo; bit) must be transmitted. The hardware required to implement I2C is more complex than SPI, but less than asynchronous serial. It can be fairly trivially implemented in software.  I2C Signal basics Each I2C bus consists of two signals: SCL and SDA.\n SCL is the clock signal, and SDA is the data signal. The clock signal is always generated by the current bus master; some slave devices may force the clock low at times to delay the master sending more data (or to require more time to prepare data before the master attempts to clock it out). This is called \u0026ldquo;clock stretching\u0026rdquo;.  PWM basics PWM is a digital (i.e. square wave) signal that oscillates according to a given frequency and duty cycle.\n The frequency (expressed in Hz) describes how often the output pulse repeats. The period is the time each cycle takes and is the inverse of frequency. The duty cycle (expressed as a percentage) describes the width of the pulse within that frequency window.  You can adjust the duty cycle to increase or decrease the average \u0026ldquo;on\u0026rdquo; time of the signal. The following diagram shows pulse trains at 0%, 25%, and 100% duty:\n Note: Most PWM hardware has to toggle at least once per cycle, so even duty values of 0% and 100% will have a small transition at the beginning of each cycle.\n Example: LED Brightness  20% duty cycle at 100 Hz or above will just look dimmer than fully on.  PCA9865 overview   Adjustable frequency PWM up to about 1.6 KHz.\n  12-bit resolution for each output: for servos, that means about 4us resolution at 60Hz update rate, 4096 levels.\n  Multiple Drivers (up to 62) can be chained to control still more servos. With headers at both ends of the board, the wiring is as simple as connecting a 6-pin parallel cable from one board to the next.\n  Board 0: Address = 0x40 Offset = binary 00000 (no jumpers required)\n  Board 1: Address = 0x41 Offset = binary 00001 (bridge A0)\n  Board 2: Address = 0x42 Offset = binary 00010 (bridge A1)\n  Board 3: Address = 0x43 Offset = binary 00011 (bridge A0 \u0026amp; A1)\n  Board 4: Address = 0x44 Offset = binary 00100 (bridge A2)\n  Board 5: \u0026hellip;\n  PWM Controlled Servo basics  PWM signals go into the signal demodulation circuit through the receiving channel, so to generate a DC bias voltage. It will then be compared with the voltage of the potentiometer, and thus a voltage gap is obtained and input into the motor driver IC to drive the motors to rotate clockwise or anticlockwise. When the speed reaches to a certain number, it will drive the potentiometer R to rotate by the cascaded reduction gear, until the gap is reduced to 0 and the servo stops spinning. A servo is controlled by PWM signals, and the change of duty cycle control that of the position the servo rotates to.  A typical servo motor expects to be updated every 20 ms with a pulse between 1 ms and 2 ms, or in other words, between a 5 and 10% duty cycle on a 50 Hz waveform.\nThe period of 20 ms (50 Hz) comes from the days where the signal was encoded in PPM format to be sent over the air.\nThe PPM period was around 22.5 ms, and the conversion to PWM was trivial: the time of the PWM high state was the time position of the PPM pulse for that servo.\nRC Servo basics   Modern RC servo position is not defined by the PWM duty cycle (ON/OFF time) but only by the width of the pulse.\n  The frequency doesn't matter as long as it is between 40 Hz and 200 Hz.\n  Typically expects around 4.8V to 6V input on the power wire (varies by car) and a PWM control signal on the signal wire.\n  Three wires are colored black-red-white, or brown-red-yellow, where the dark wire (black/brown) is ground, and the center wire (red) is power, and the light wire (white/yellow) is control.\n  RC-style PWM:\n One pulse is sent 60 times a second, and the width of this pulse controls how left/right the servo turns. 1500 microseconds - the servo is centered; 1000 microseconds - the servo is turned all the way left (or right); 2000 microseconds - the servo is turned all the way in the other direction    RC ESC basics  Pulse between 1000 and 2000 microseconds Controls the power to the motor so the motor spins with different amounts of power in forward or reverse. Again, 1500 microseconds typically means \u0026ldquo;center\u0026rdquo; which for the motor means \u0026ldquo;dead stop.\u0026rdquo;  My schematic: Note to self: It is not a good idea to use the Jetson 5V pin to power your PCA9865. Electrical noise and \u0026lsquo;brownouts\u0026rsquo; from excess current draw can cause the Nano to act erratically, reset and/or overheat. The PCA9685 should get its own, separate power supply.\n Connecting the PCA9865 to the Nano We'll be using the SCL, SDA I2C pins from the PCA9865, along with the GND and VCC pins for power:\nTo connect them to the Nano, we first have to find out which of the Nano pins on its J41 header we should use. Luckily, the Jetson Nano J41 header has the same pinout as the Raspberry Pi, which you can see here, or just use the TL;DR below:\n 3V3: labeled red, position 1 and 17, connect PCA VCC pin to either of them GND: labeled black, all over the place, connect PCA GND pin to any of them I2C busses: labeled yellow, there are two of them:  Bus 0: position 27 (SDA) and position 28 (SCL) Bus 1: position 3 (SDA) and position 5 (SCL) Connect the PCA SDA and SCL pins to either bus Note: the SDA/SCL pins come in pairs, you have to choose a pair/bus which you'll use    I've connected mine to the first bus (Bus 0):\n PCA -\u0026gt; Nano:  3V3 -\u0026gt; 1 SDA -\u0026gt; 3 SCL -\u0026gt; 5 GND -\u0026gt; 6    The hardware part is done! "
},
{
	"uri": "https://ori.codes/software/setting-up-donkeycar-on-the-rc/",
	"title": "DonkeyCar configuration: RC car",
	"tags": [],
	"description": "",
	"content": " From now until the end of this chapter, I'll assume you're working on your car via SSH.\n Creating a DonkeyCar application First, we'll run the createcar command, which will create a new directory with all of the files needed to run and train our RC.\nCommand usage from the docs:\ndonkey createcar --path \u0026lt;dir\u0026gt; [--overwrite] [--template \u0026lt;donkey2\u0026gt;]\rRun the following command to create a new donkeycar application:\ndonkey createcar --path ~/mycar\rOpen the newly created directory:\ncd ~/mycar\rConfiguring the DonkeyCar application We can find a bunch of settings for the application we just created in a file called myconfig.py. We'll have to edit this file to make our RC work as intended.\nOpen up the myconfig.py file in a text editor:\nnano myconfig.py\rNote: Some settings are commented out by default (read: have a # at the beginning of the line). Whenever we're filling in a value of some setting, you should uncomment it first/remove the # at the beginning.\nCamera settings: If you're using Nano as your text editor, you can press CTRL+W and type in CAMERA and press enter to jump to the camera settings.\nIf you're using Vim, you probably don't need me to tell you how to search for stuff, but I'll do it anyways: type in /CAMERA to search forward for the camera settings, and use n and N to go back and forth between matches.\nHere are the settings:\n# #CAMERA\rCAMERA_TYPE = \u0026#34;CVCAM\u0026#34; # (PICAM|WEBCAM|CVCAM|CSIC|V4L|MOCK)\rIMAGE_W = 1280\rIMAGE_H = 720\rIMAGE_DEPTH = 3\rThe different types of camera are:\n PICAM: the standard OV5647 Raspberry Pi camera CSIC: the Sony IMX219 Raspberry Pi camera v2+ CVCAM: a USB camera connected to the Nano, I'm using this for my GoPro clone WEBCAM: also for USB cameras, but this requires further setup since it uses pygame V4L: Video4Linux MOCK: A fake camera that outputs a static frame.  If your image is flipped, or you want to mount your camera in a rotated position, use:\n CSIC_CAM_GSTREAMER_FLIP_PARM: for flipping your camera output Example: to flip your image vertically, use CSIC_CAM_GSTREAMER_FLIP_PARM = 3  Also, you can specify your resolution/image depth here:\n IMAGE_W: image width in pixels IMAGE_H: image height in pixels IMAGE_DEPTH: number of channels, 3 for RGB/BGR  PCA9685 settings: We also need to specify which bus we've connected our PCA9685 to.\nSee this to refresh your memory:\n If you used pins 3 and 5 on your Nano, then you're connected to Bus 1. If you used pins 27 and 28, you're connected to Bus 0.  We'll also need the I2C address our PCA is connected to, for which we'll need to:\n# Add our user to the i2c group\rsudo usermod -aG i2c YOUR-USERNAME\r# Reboot so it takes effect\rsudo reboot\rAfter rebooting, type:\n# i2c-tools should come preinstalled with your Nano, but in case they aren\u0026#39;t:\r# sudo apt install i2c-tools\r# For Bus 0: use 0\r# For Bus 1: use 1\rsudo i2cdetect -r -y 1\rIf your PCA is wired/connected correctly to the Nano, you should get something like:\n0 1 2 3 4 5 6 7 8 9 a b c d e f\r00: -- -- -- -- -- -- -- -- -- -- -- -- --\r10: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --\r20: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --\r30: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --\r40: 40 -- -- -- -- -- -- -- -- -- -- -- -- -- -- --\r50: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --\r60: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --\r70: 70 -- -- -- -- -- -- --\rThe results explained:\n  40 and 70: The addresses of our device. We'll be using the first one, 40, to address it.\n  \u0026ndash;: the address was probed, no chip was found\n  UU: probing was skipped because the address is currently in use\n  If you're getting jibberish or all of the addresses are filled, check your connections, you probably swapped SDA and SCL or plugged them into the wrong pins.\nFinally, in the myconfig.py file, find and fill the following:\nPCA9685_I2C_ADDR = 0x40\rPCA9685_I2C_BUSNUM = 1  PCA9685_I2C_ADDR: the I2C address of our PCA, in hex format (0xNumber) PCA9685_I2C_BUSNUM: the I2C bus our PCA is connected to  Steering and throttle channels: We also need to specify to which PCA channels our steering servo and our ESC are connected to.\nSee this to refresh your memory or just look at the numbers on the PCA, above the places you've connected your servo and ESC connectors to, they correspond to their respective channel numbers.\nIn the settings, find and fill out the steering:\n# #STEERING\rSTEERING_CHANNEL = 1 #channel on the 9685 pwm board 0-15\rAnd the throttle channel values:\n# #THROTTLE\rTHROTTLE_CHANNEL = 0 #channel on the 9685 pwm board 0-15\rIn my case, the throttle/ESC is the zeroth channel and the steering/servo is the first channel.\nWe will also need to fill out the following values for steering:\nSTEERING_LEFT_PWM = ??? #pwm value for full left steering\rSTEERING_RIGHT_PWM = ??? #pwm value for full right steering\rAnd throttle:\nTHROTTLE_FORWARD_PWM = ??? #pwm value for max forward throttle\rTHROTTLE_STOPPED_PWM = ??? #pwm value for no movement\rTHROTTLE_REVERSE_PWM = ??? #pwm value for max reverse throttle\rTo do so, we'll need to calibrate our steering and throttle first, and then come back with the values we've found and fill them in.\nFeel free to save the changes you've made to the myconfig.py file so far:\n Nano: CTRL+O and Enter/Return to save, CTRL+X to close the editor Vim: :wq! :)  "
},
{
	"uri": "https://ori.codes/software/calibrating-steering-and-throttle/",
	"title": "Calibrating steering and throttle",
	"tags": [],
	"description": "",
	"content": " Make sure your car wheels are not touching the ground. Prop it up using a shoebox, or in my case, an eGPU dock. We will be calibrating the throttle which means your car will start accelerating very fast, without warning, so you wouldn't want it slamming into a wall at full throttle.\n Calibrating the throttle: \r**First, you'll need to turn on your car**; the actual RC, not the Nano.\rDepending on your RC, it'll probably beep because it's turned on and the controller isn't connected to it, but don't worry, it'll stop once we connect to it via the I2C bus.\nRun the following:\ndonkey calibrate --channel YOUR_ESC\\THROTTLE_CHANNEL --bus=YOUR_I2C_PCA_BUS\rThe output should look like:\nusing donkey v3.1.1 ...\rinit PCA9685 on channel 0 address 0x40 bus 0\rUsing PWM freq: 60\rEnter a PWM setting to test (\u0026#39;q\u0026#39; for quit) (0-1500):\rIf you're using the wrong bus, you'll probably get a OSError: [Errno 121] Remote I/O error\nType in 370 and press enter. The ESC should stop beeping, indicating it is calibrated to the neutral throttle position.\nTo detect your forward values for the throttle:\n Try entering 400 and seeing if your car starts throttling forwards. If it's not, then try entering 330 instead. After finding out which direction forwards is:  Number larger than 370 Number smaller than 370   Start from 370 and enter values +/- 10 and choose a value that you want your maximum forward throttle to be and write it down.  To detect your reverse values for the throttle:\n  In order to go in reverse, the RC needs to get a reverse throttle value, followed by the neutral throttle value, followed by the reverse throttle value again.\n  If your forward values were larger than 370:\n Enter 330, then 370, then 330.    If your forward values were smaller than 370:\n Enter 400, then 370, then 400.    After confirming how to get your car in reverse throttle, once again start from 370 and enter values +/- 10 and choose a value that you'd like your max reverse throttle to be, and write it down.\n  You can now enter q to finish the throttle calibration procedure.\nCalibrating the steering \rOnce again, run the following:\rdonkey calibrate --channel YOUR_STEERING_CHANNEL --bus=YOUR_I2C_PCA_BUS\r Enter 360, and you should see your car slightly steering. If you don't, try 350 or 370. Once again, enter +/- 10 values to find out what values steer your car completely to the left and to the right and write them down.  You can now enter q to finish the steering calibration procedure.\nEntering the values in the config file Open up the myconfig.py in the editor of your choice:\nnano ~/mycar/myconfig.py\rFind the THROTTLE and STEERING sections and enter the values you wrote down during calibration:\n STEERING_RIGHT_PWM: the value which steers your car completely to the right. STEERING_LEFT_PWM: the value which steers your car completely to the right. THROTTLE_STOPPED_PWM: the neutral throttle value. THROTTLE_FORWARD_PWM: the maximum forward throttle value. THROTTLE_REVERSE_PWM: the maximum reverse throttle value.  Save the file and exit the editor and you're done!\n"
},
{
	"uri": "https://ori.codes/software/connecting-a-bluetooth-gamepad/",
	"title": "Using a gamepad",
	"tags": [],
	"description": "",
	"content": "You can, and should, use a gamepad to control your RC. It's much easier to generate good training data using a gamepad, and it's much easier to drive the thing compared to the Web interface that Donkey provides.\nSo how do we connect and use one?\nCompatible controllers First, let's make sure you have one that'll actually work\nThe official Donkey docs list that the following are known to work:\n Logitech Gamepad F710 Sony PS3 Dualshock OEM Sony PS3 Sixaxis OEM (Not compatible with Jetson Nano) Sony PS4 Dualshock OEM WiiU Pro XBox Controller SteelSeries Nimbus (works only on TX2 jetpack 4.2+, may work on the Nano)  Depending on the controller you're going to use, open up the myconfig.py file and find the # JOYSTICK section and uncomment and fill the following (using one of the types suggested inline with the setting):\nCONTROLLER_TYPE=\u0026#39;xbox\u0026#39; #(ps3|ps4|xbox|nimbus|wiiu|F710|rc3)\rIf your gamepad isn't officially supported, try following the rest of the tutorial and connecting it to the Nano. If it shows up as /dev/input/js0, you should be able to use it, since the Donkey platform just uses that OS device mount as the gamepad.\nConnecting your controller Xbox (One) Controller The Xbox controller doesn't like the Enhanced Re-Transmission Mode, so we'll disable it first.\nOpen up the /etc/modprobe.d/xbox_bt.conf file (this may actually create the file if it doesn't exist yet):\nsudo nano /etc/modprobe.d/xbox_bt.conf\r# Add this line and save the file\roptions bluetooth disable_ertm=1\r# Reboot\rsudo reboot\rAfter rebooting, check it's actually disabled by running:\ncat /sys/module/bluetooth/parameters/disable_ertm\rIt should say \u0026lsquo;Y'. If it does, open up bluetoothctl:\nsudo bluetoothctl\r# Register the default agent\ragent on\r# Default agent request\rdefault-agent\r# Scan for devices\rscan on\rTurn on your Xbox controller (by pressing the big Xbox logo button) and start the pairing mode by pressing the sync button on the back of the controller, next to the microUSB port.\nYou should see your controller show up in bluetoothctl similar to:\n[NEW] Device YO:UR:MA:CA:DD:RS XBox One Wireless Controller\rType in:\n# This may take a few tries ...\rconnect YOUR_MAC_ADDRESS\r# If it connects but immediately disconnects, the disable_ertm setting isn\u0026#39;t properly set up, try doing that again\r# When it connects and the big Xbox logo button is solid white:\rtrust YOUR_MAC_ADDRESS\rquit\rOnce connected, it should automatically reconnect anytime the Jetson and it are both powered on. If it doesn't, you'll have to run the steps above again.\nPS4 Controller Install ds4drv:\nsudo pip install ds4drv\rGrant it permissions:\nsudo wget https://raw.githubusercontent.com/chrippa/ds4drv/master/udev/50-ds4drv.rules -O /etc/udev/rules.d/50-ds4drv.rules\rsudo udevadm control --reload-rules\rsudo udevadm trigger\rsudo reboot\rAfter rebooting run ds4drv:\n# --led changes the light bar color, you can modify it or not use it at all\r# --hidraw doesn\u0026#39;t play well with some controllers, if you\u0026#39;re having issues, try not using it\rds4drv --hidraw --led 00ff00\rStart the controller in pairing mode:\n Press and hold Share button Press and hold PS button until the light bar starts blinking If it goes green after a few seconds, pairing is successful.  To run ds4drv on boot, open up /etc/rc.local and paste in the command you used to start ds4drv:\nsudo nano /etc/rc.local\r# Paste in (with or without --hidraw and --led):\r/home/YOUR-USERNAME/env/bin/ds4drv --led 00ff00\rPS3 Controller The Donkey docs say to follow this guide, or you can just run:\nsudo apt-get install bluetooth libbluetooth3 libusb-dev\rsudo systemctl enable bluetooth.service\rsudo usermod -G bluetooth -a pi\rsudo reboot\rAfter rebooting, plug in the controller with an USB cable, press the PS button and:\nwget http://www.pabr.org/sixlinux/sixpair.c\rgcc -o sixpair sixpair.c -lusb\rsudo ./sixpair\rRun bluetoothctl as a super user and pair your controller:\nsudo bluetoothctl\r# Enable the default agent\ragent on\r# List all found devices and get your controller\u0026#39;s MAC address\rdevices\r# Trust your controller\rtrust \u0026lt;MAC ADDRESS\u0026gt;\r# Default agent request\rdefault-agent\r# Quit\rquit\rUnplug the USB cable and press the PS button. Your controller should be mounted at ls /dev/input/js0.\nAny other gamepad/joystick/controller If it's a Bluetooth one, try pairing it via bluetoothctl and see if it mounts at /dev/input/js0. If it does, great, you can move on.\nIf not, try connecting it via USB and see if it mounts at /dev/input/js0, if it does, you're good.\nIf neither goes, try searching how to connect that particular controller to a Linux device online, if there's a way (mostly there is), it should mount at the above mentioned js0 and you should be able to follow along.\nCreating a Button/Axis mapping: After your controller is connected, run:\ndonkey createjs\rThe output should be:\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~##\r## Welcome to Joystick Creator Wizard. ##\r##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~##\rThis will generate code to use your joystick with a Donkey car.\rOverview:\rFirst we name each button, then each axis control.\rNext we map names to actions.\rFinally we output a python file you can use in your project.\rHit Enter to continue\r# After pressing Enter\rPlease plug-in your controller via USB or bluetooth. Make sure status lights are on and device is mapped.\rEnter to continue\r# You can press enter since we\u0026#39;ve already gone through connecting your controller\rWhere can we find the device file for your joystick?\rHit Enter for default: /dev/input/js0 or type alternate path:\r# Should be /dev/input/js0 if it\u0026#39;s a PS3/PS4/Xbox/Standard USB one\r# Hit enter and see if it finds it (it will if its on /dev/input/js0)\rAttempting to open device at that file...\rOpening /dev/input/js0...\rDevice name: The type of Controller you\u0026#39;re using\rFound and accessed input device.\r# After pressing Enter\rNext we are going to look for gyroscope data.\rFor 5 seconds, move controller and rotate on each axis. Hit Enter then start moving:\r# You can skip this, since you most probably won\u0026#39;t be using it\rOk, we didn\u0026#39;t see any events. So perhaps your controller doesn\u0026#39;t emit gyroscope data. No problem.\rFinally, you'll get to this part:\nWe will display the current progress in this set of tables:\rButton Map:\r+-------------+-------------+\r| button code | button name |\r+-------------+-------------+\r+-------------+-------------+\rAxis Map:\r+-----------+-----------+\r| axis code | axis name |\r+-----------+-----------+\r+-----------+-----------+\rControl Map:\r+---------+--------+\r| control | action |\r+---------+--------+\r+---------+--------+\rAs you name buttons and map them to controls this table will be updated.\rWhile in here, you should go through pressing every button on your controller, and giving it a name.\nNote: during this first step, you can only map buttons, the axes, such as triggers or analog sticks will come next.\nFor my Xbox One Controller, I made the following button map:\nButton Map:\r+-------------+-------------+\r| button code | button name |\r+-------------+-------------+\r| 0x130 | A |\r| 0x131 | B |\r| 0x133 | Y |\r| 0x132 | X |\r| 0x137 | Start |\r| 0x136 | Select |\r| 0x135 | R1 |\r| 0x134 | L1 |\r| 0x139 | RS |\r| 0x138 | LS |\r+-------------+-------------+\rAfter mapping all the buttons, wait for 10 seconds and enter Y when the program asks you if you've finished mapping all the buttons.\nThis is what my axis map looks like:\nAxis Map:\r+-----------+-------------------------+\r| axis code | axis name |\r+-----------+-------------------------+\r| 0x5 | RT |\r| 0x2 | LT |\r| 0x0 | Left Stick: Horizontal |\r| 0x1 | Left Stick: Vertical |\r| 0x4 | Right Stick: Vertical |\r| 0x3 | Right Stick: Horizontal |\r+-----------+-------------------------+\rYou can enter D when you're done and move on to the control map, through which we'll map buttons and axes to specific car controls.\nThis is what my control map looks like:\nControl Map:\r+------------------------+--------------------------+\r| control | action |\r+------------------------+--------------------------+\r| Start | toggle_mode |\r| Select | erase_last_N_records |\r| A | emergency_stop |\r| R1 | increase_max_throttle |\r| L1 | decrease_max_throttle |\r| B | toggle_constant_throttle |\r| X | toggle_manual_recording |\r| Left Stick: Horizontal | set_steering |\r| Right Stick: Vertical | set_throttle |\r+------------------------+--------------------------+\rIf you've messed something up, don't worry, at the next menu you can go back to any step you'd like:\nNow we are nearly done! Are you happy with this config or would you like to revisit a topic?\rH)appy, please continue to write out python file.\rB)uttons need renaming.\rA)xes need renaming.\rT)hrottle and steering need remap.\rR)emap buttons to controls.\rIf you're happy with your maps, enter H and it will prompt you for a name under which to save your mapping. The default one is my_joystick.py, but you can enter a custom one, if you're planning to use multiple different controllers, or just for non-generic-naming's sake:\nNow we will write these values to a new python file.\rWhat is the name of python file to create joystick code? [default: my_joystick.py] xbox_one_controller.py\rIt will then ask what to name the custom Python class of the controller you've just created:\nWhat is the name of joystick class? [default: MyJoystick] XboxOneController\rxbox_one_controller.py written.\rCheck your new python file to see the controller implementation. Import this in manage.py and use for control.\rAlmost there, we just need to import our custom mapping in the manage.py script to be able to use it with our RC.\nOpen up manage.py (nano or vim) and at the end of the imports, find the following line:\nfrom donkeycar.parts.controller import get_js_controller\rctr = get_js_controller(cfg)\rAnd replace it with:\n# This assumes you haven\u0026#39;t changed the default names\r from my_joystick import MyJoystick\rctr = MyJoystick(throttle_dir=cfg.JOYSTICK_THROTTLE_DIR,\rthrottle_scale=cfg.JOYSTICK_MAX_THROTTLE,\rsteering_scale=cfg.JOYSTICK_STEERING_SCALE,\rauto_record_on_throttle=cfg.AUTO_RECORD_ON_THROTTLE)\rctr.set_deadzone(cfg.JOYSTICK_DEADZONE)\rOr if you've defined a custom name for the python file containing your mapping, and the class it contains, which I did, then modify the import line so it uses your custom script and class name:\n# In my case\r from xbox_one_controller import XboxOneController\rctr = XboxOneController(throttle_dir=cfg.JOYSTICK_THROTTLE_DIR,\rthrottle_scale=cfg.JOYSTICK_MAX_THROTTLE,\rsteering_scale=cfg.JOYSTICK_STEERING_SCALE,\rauto_record_on_throttle=cfg.AUTO_RECORD_ON_THROTTLE)\rctr.set_deadzone(cfg.JOYSTICK_DEADZONE)\rAnd you're done! Now we can start actually driving the car using our controller!\n"
},
{
	"uri": "https://ori.codes/software/test-driving-the-rc/",
	"title": "Test drive",
	"tags": [],
	"description": "",
	"content": "First of all: congrats on getting this far! Let's spin our RC for a ride.\nBefore continuing:\n Make sure your RC is powered up (not the Jetson Nano, the actual RC). Make sure that the camera is connected and powered up (if you're using a USB camera). Make sure that your RC has enough space around it, depending on what throttle values you've defined.  Test drive using a gamepad Change to the directory you've created with the donkey createcar command:\ncd ~/mycar\rStart the manage.py script with the drive command and the --js flag, to control your RC with your gamepad:\npython manage.py drive --js\rThe output should look like:\nusing donkey v3.1.1 ...\rloading config file: /home/your_username/mycar/config.py\rloading personal config over-rides\rconfig loaded\rcfg.CAMERA_TYPE CVCAM\rcfg.CAMERA_TYPE CVCAM\rAdding part CvCam.\rAdding part XboxOneJoystickController.\rAdding part ThrottleFilter.\rAdding part PilotCondition.\rAdding part RecordTracker.\rAdding part ImgPreProcess.\rAdding part DriveMode.\rAdding part AiLaunch.\rAdding part AiRunCondition.\rInit ESC\rAdding part PWMSteering.\rAdding part PWMThrottle.\rTub does NOT exist. Creating new tub...\rNew tub created at: /home/your_username/mycar/data/tub_15_20-01-02\rAdding part TubWriter.\rYou can now move your joystick to drive your car.\rIt'll also show you your controller mapping and the controller name:\nJoystick Controls:\r+------------------+---------------------------+\r| control | action |\r+------------------+---------------------------+\r| a_button | toggle_mode |\r| b_button | toggle_manual_recording |\r| x_button | erase_last_N_records |\r| y_button | emergency_stop |\r| right_shoulder | increase_max_throttle |\r| left_shoulder | decrease_max_throttle |\r| options | toggle_constant_throttle |\r| circle | show_record_acount_status |\r| R2 | enable_ai_launch |\r| left_stick_horz | set_steering |\r| right_stick_vert | set_throttle |\r| right_trigger | set_magnitude |\r| left_trigger | set_magnitude |\r+------------------+---------------------------+\rOpening /dev/input/js0...\rStarting vehicle...\rDevice name: Xbox Wireless Controller\r\r\rYou can also see it mentioning something called a tub, and if you drive your RC around, it'll say that it's making records:\nStarting vehicle...\rrecorded 10 records\rrecorded 20 records\rrecorded 30 records\rrecorded 40 records\rrecorded 50 records\rrecorded 60 records\rrecorded 70 records\rrecorded 80 records\rrecorded 90 records\rrecorded 100 records\r...\rA tub is a folder containing data from a drive. It's located in the data directory inside the mycar directory. The data is made up of camera images (JPEGs), along with steering and throttle values (JSONs).\nWe'll use this data later on to train our models.\nIf you make a mistake while driving, you can delete the last 100 records by using the button you've mapped to the erase_last_N_records control.\nTest drive using the Web interface The Web interface can sometimes be a bit simpler to use, since it allows you to see the camera output and has a nice GUI for all of the autopilot settings. Let's try it out.\nFirst, change to the directory you've created with the donkey createcar command:\ncd ~/mycar\rStart the manage.py script with the drive command:\npython manage.py drive\rIf all went well, the end of the output should say:\nYou can now go to \u0026lt;your pis hostname.local\u0026gt;:8887 to drive your car.\rStarting vehicle...\r8887\rYou can now open up your browser, and enter your Jetson Nano's IP with the port 8887 to control your car:\n192.168.x.xxx:8887\r Before starting up your RC, set the Max Throttle value to 50% or less. By default it will allow your RC to go as fast as it can, regardless of the values you've set in myconfig.py. A lot of RC's can easily go over 50MPH/80KPH, so this could cause a world of pain.\n You should be seeing something like this:\nLet's break it down:\n The Control Mode has three options:  Joystick: controls the RC via the blue touch/click area on the screen. You can also use I/K/J/L as forward/reverse/left/right controls. Gamepad: uses a gamepad connected to the device that's browsing the page. Haven't tried this one yet, it seems it only works on Linux with some controllers. Device Tilt: intended for smartphones browsing the site, you tilt the phone to control the car.   Max Throttle: defines the maximum throttle the vehicle can achieve when the user presses full forward throttle via the controls. Throttle Mode has two options:  User: where the user gives the throttle manually via the controls. Constant (Selected Max) which constantly keeps the throttle at the value defined in the Max Throttle dropdown.   Angle \u0026amp; Throttle show the current values of throttle and steering. Useful when looking at how your autopilot drives. Mode \u0026amp; Pilot has three modes:  User: where the user manually controls both the steering and throttle. Local Pilot: where the autopilot/model controls both the steering and throttle. Local Angle: where the autopilot controls the steering, but the user controls the throttle.   Start Recording: toggles data recording. Start Vehicle starts the vehicle if it's in autopilot mode.  A quick note if you're using an USB camera and it's showing a blue-ish picture. The default color space for the CVCAM is BGR, since it's an OpenCV camera. Don't worry about it, it makes no practical difference for your model.\nIf, however, you insist on seeing the CVCAM output in RGB, open up the cv.py script, found in your projects folder, inside the donkeycar/donkeycar/parts directory:\nnano ~/projects/donkeycar/donkeycar/parts/cv.py\rFind the CvCam class, and:\n Add an instance of the ImgBGR2RGB class to a class variable Call the run() method of the ImgBGR2RGB instance any time the class returns an image frame  The resulting CvCam class code should look like this:\nclass CvCam(object):\rdef __init__(self, image_w=1280, image_h=720, image_d=3, iCam=0):\r# Added an instance of the converter class\r self.imgConverter = ImgBGR2RGB()\rself.frame = None\rself.cap = cv2.VideoCapture(iCam)\rself.running = True\rself.cap.set(3, image_w)\rself.cap.set(4, image_h)\rdef poll(self):\rif self.cap.isOpened():\rret, self.frame = self.cap.read()\rdef update(self):\rwhile(self.running):\rself.poll() # poll the camera for a frame\r def run_threaded(self):\rreturn self.imgConverter.run(self.frame) # Convert the image to RGB\r def run(self):\rself.poll()\rreturn self.imgConverter.run(self.frame) # Convert the image to RGB\r def shutdown(self):\rself.running = False\rtime.sleep(0.2)\rself.cap.release()\rNow you should be seeing your camera output in RGB. But as I said, it doesn't really matter to the model.\n"
},
{
	"uri": "https://ori.codes/software/sanity-check-first-autopilot/",
	"title": "First Autopilot: sanity check",
	"tags": [],
	"description": "",
	"content": "After doing a bunch of work like we just did, it's always important to periodically check that everything works as intended, before moving on to even more complex stuff.\nSo that's what we'll be doing.\nBuilding a test track First, you need to build a test track. For this sanity checking, I wouldn't do anything over the top. Just take some duct-tape and make a circular track that's easy to drive around.\nThis is what mine looked like:\nYou can also make something more fancy, like this, but it isn't neccessary:\nCollecting the training data After your RC is on the track, connect your gamepad to it and from the ~/mycar directory run:\npython manage.py drive --js\rNow, try driving one lap around the track. It should be collecting records. If it is, you're good to go.\n\rYou should now learn how to drive around the test track without making any errors. After you're confident in your driving abilities, you should stop the manage.py script, position your car on the track and start it up again.\nTo get your training data:\n  Drive the RC for at least 10 laps, without any errors.\n If you make a mistake while driving, press the Y/Triangle button to erase the last 5 seconds of the drive, put your car back on the track and continue driving.    Stop the manage.py script.\n  Scroll up to see in which tub the data was saved:\n# The line you\u0026#39;re looking for is:\rNew tub created at: /home/your_username/mycar/data/tub_**_**-**-**\r  Copy the name of the tub\n  Transferring the data to your host PC Now we just need to transfer our training data to our host PC:\n  Mac OS X / Linux: (use the tub name you copied)\n  rsync -r username@your_nano_IP_address:~/mycar/data/tub_**_**-**-** ~/mycar/data/\r    Windows (MobaXTerm):\n MobaXTerm has a built in SFTP viewer to the left of your terminal To copy your data, go to: /home/your_username/mycar/data/ using the SFTP viewer Find the tub with the same name you've copied Right click on it, and click Download Save the files to the mycar directory on your host PC (should be at C:\\Users\\your_username\\mycar\\)     Training your first model \rAs mentioned previously, Donkey comes with a number of different neural network architectures you can use to train your autopilot. You can find them at the directory where you've cloned the donkey repository, inside the `parts\\keras.py` script.\rThe usual location, if you've followed the tutorials and used the suggested directory, would be:\n# Linux/Mac OS X\r~/projects/donkeycar/donkeycar/parts/keras.py\r# Windows\rC:\\Users\\your_username\\projects\\donkeycar\\donkeycar\\parts\\keras.py\rWe'll be using one of the simplest ones, the linear model.\nYou can see a list of all the architectures explained here.\nTo train your first autopilot model, go to the mycar directory on your host PC, and run:\npython ~/mycar/manage.py train --model ~/mycar/models/firstAutopilot.h5 --type=linear\rThis will use all of the tubs it can find in your data folder.\nIf you'd like to specify specific tubs Donkey should use to train your model, you can run:\npython ~/mycar/manage.py train --tub /path/to/the/tub,/path/to/other/tub --model ~/mycar/models/theNameOfTheModel.h5 --type=architectureName\rNotice that you can either separate specific tub paths by using a comma, or you can use a mask, e.g.: /somePath/tubs/tub-20* to train all tubs starting with the name tub-20.\nFeel free to train a number of models using different architectures to see how they work. I'd recommend trying the linear, rnn and 3d architectures.\n Moving the trained model back to your RC: After the training script has finished, we just need to move our model(s) back to our RC in order to use it.\n  Linux/Mac OS X:\n  rsync -r ~/mycar/models/ username@your_nano_IP_address:~/mycar/models/\r    Windows (MobaXTerm):\n Navigate to the/home/mycar/models/ on the Nano, using the built-in SFTP viewer and just drag and drop all of the models (.h5 files) you've trained from your host PC to the Nano.    Let the autopilot drive by itself After training and moving your models to your RC, let the model control the RC by running:\npython manage.py drive --model ~/mycar/models/yourModel.h5\rGo to the Web interface by opening up a browser and entering yourNanoIP:8887 into the address bar.\nI'd recommend setting the Mode \u0026amp; Pilot to Local Angle, starting the vehicle and either giving the throttle by yourself, using the blue touch/click area on the right, or by giving it a constant throttle by setting the Max Throttle to 50% and setting the Throttle Mode to Constant.\nAfter making sure your model doesn't just ram your RC into a wall, you can use the Local Pilot mode and it'll control the throttle by itself.\n\r## What's next?\rThe autopilot works, but it's far from perfect. But it's still pretty impressive considering we started out with a regular RC, and now we have a thing that can drive around a track on its own. :)\nThe more important thing is, we now have everything we need in order to make a complex self-driving model. We can automatically collect data by just driving our RC around a track, we have a pipeline established that can take that data, run it through an architecture we've defined and spit out a model we can just plug our car into and it'll control it via it's output. That's pretty sweet.\nNow we can go onto the most interesting part of this project, the machine (and deep) learning part.\n"
},
{
	"uri": "https://ori.codes/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://ori.codes/",
	"title": "Home Page",
	"tags": [],
	"description": "",
	"content": "Hi there! My name is Ivan, and I’m currently working on a self-driving RC car for my Master’s thesis. This website is meant to be a guide/tutorial for all those interested in making their own self-driving RC, and will be updated regularly to document my progress working on it.\nFeel free to ping me at  Twitter  or at my  E-mail  if you want to get in touch or know more/ask anything.\n"
},
{
	"uri": "https://ori.codes/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]